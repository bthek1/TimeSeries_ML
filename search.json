[
  {
    "objectID": "xclassifier.html",
    "href": "xclassifier.html",
    "title": "Xclassifier",
    "section": "",
    "text": "import xplainable as xp\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom fastai.imports import *\nfrom IPython.display import HTML\n\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nfolder = Path('titanic')\nlocation = Path('Data')\npath = Path(f'{location}/{folder}')\n\nimport zipfile,kaggle\n\nkaggle.api.competition_download_cli(f'{folder}', path = str(location))\nzipfile.ZipFile(f'{location}/{folder}.zip').extractall(f'{location}/{folder}')\n\ntitanic.zip: Skipping, found more recently modified local copy (use --force to force download)\n\n\n\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\npp = xp.Preprocessor()\npp.preprocess(df)\n\n\n\n\n\ndf = pd.read_csv('preprocessed_1695571534.csv')\ndf.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n1\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n2\n7\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n3\n11\n1\n3\nSandstrom, Miss. Marguerite Rut\nfemale\n4.0\n1\n1\nPP 9549\n16.7000\nG6\nS\n\n\n4\n12\n1\n1\nBonnell, Miss. Elizabeth\nfemale\n58.0\n0\n0\n113783\n26.5500\nC103\nS\n\n\n\n\n\n\n\n\n# Train a model\nmodel = xp.classifier(df)\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/xplainable/_dependencies.py:39: UserWarning: Your version of Tornado is greater than 6.1, which is known to crash the Jupyter kernel when training models. Please consider downgrading to Tornado 6.1\n  warnings.warn(\"Your version of Tornado is greater than 6.1, which\"\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Xclassifier"
    ]
  },
  {
    "objectID": "prophet.html",
    "href": "prophet.html",
    "title": "Hourly Time Series Forecasting using Facebook’s Prophet",
    "section": "",
    "text": "Background on the Types of Time Series Data\n\n\n\nimg\n\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom prophet import Prophet\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nfrom pathlib import Path\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use('ggplot')\nplt.style.use('fivethirtyeight')\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    \"\"\"Calculates MAPE given y_true and y_pred\"\"\"\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\n\n\nData\nThe data we will be using is hourly power consumption data from PJM. Energy consumption has some unique characteristics. It will be interesting to see how prophet picks them up.\nPulling the PJM East which has data from 2002-2018 for the entire east region.\n\nname = 'hourly-energy-consumption'\npath = Path(f'Data/{name}')\npjme = pd.read_csv(f'{path}/PJME_hourly.csv',\n                   index_col=[0],\n                  parse_dates=[0])\npjme.head()\n\n\n\n\n\n\n\n\nPJME_MW\n\n\nDatetime\n\n\n\n\n\n2002-12-31 01:00:00\n26498.0\n\n\n2002-12-31 02:00:00\n25147.0\n\n\n2002-12-31 03:00:00\n24574.0\n\n\n2002-12-31 04:00:00\n24393.0\n\n\n2002-12-31 05:00:00\n24860.0\n\n\n\n\n\n\n\n\ncolor_pal = sns.color_palette()\npjme.plot(style='.',\n          figsize=(10, 5),\n          ms=1,\n          color=color_pal[0],\n          title='PJME MW')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTime Series Features\n\nfrom pandas.api.types import CategoricalDtype\n\ncat_type = CategoricalDtype(categories=['Monday','Tuesday',\n                                        'Wednesday',\n                                        'Thursday','Friday',\n                                        'Saturday','Sunday'],\n                            ordered=True)\n\ndef create_features(df, label=None):\n    \"\"\"\n    Creates time series features from datetime index.\n    \"\"\"\n    df = df.copy()\n    df['date'] = df.index\n    df['hour'] = df['date'].dt.hour\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['weekday'] = df['date'].dt.day_name()\n    df['weekday'] = df['weekday'].astype(cat_type)\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    df['weekofyear'] = df['date'].dt.isocalendar().week\n    df['date_offset'] = (df.date.dt.month*100 + df.date.dt.day - 320)%1300\n\n    df['season'] = pd.cut(df['date_offset'], [0, 300, 602, 900, 1300], \n                          labels=['Spring', 'Summer', 'Fall', 'Winter']\n                   )\n    X = df[['hour','dayofweek','quarter','month','year',\n           'dayofyear','dayofmonth','weekofyear','weekday',\n           'season']]\n    if label:\n        y = df[label]\n        return X, y\n    return X\n\nX, y = create_features(pjme, label='PJME_MW')\nfeatures_and_target = pd.concat([X, y], axis=1)\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.boxplot(data=features_and_target.dropna(),\n            x='weekday',\n            y='PJME_MW',\n            hue='season',\n            ax=ax,\n            linewidth=1)\nax.set_title('Power Use MW by Day of Week')\nax.set_xlabel('Day of Week')\nax.set_ylabel('Energy (MW)')\nax.legend(bbox_to_anchor=(1, 1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTrain / Test Split\n\nsplit_date = '1-Jan-2015'\npjme_train = pjme.loc[pjme.index &lt;= split_date].copy()\npjme_test = pjme.loc[pjme.index &gt; split_date].copy()\n\n# Plot train and test so you can see where we have split\npjme_test \\\n    .rename(columns={'PJME_MW': 'TEST SET'}) \\\n    .join(pjme_train.rename(columns={'PJME_MW': 'TRAINING SET'}),\n          how='outer') \\\n    .plot(figsize=(10, 5), title='PJM East', style='.', ms=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSimple Prophet Model\n\nProphet model expects the dataset to be named a specific way. We will rename our dataframe columns before feeding it into the model.\n\nDatetime column named: ds\ntarget : y\n\n\n\n# Format data for prophet model using ds and y\npjme_train_prophet = pjme_train.reset_index() \\\n    .rename(columns={'Datetime':'ds',\n                     'PJME_MW':'y'})\n\n\nmodel = Prophet()\nmodel.fit(pjme_train_prophet)\n\n18:04:09 - cmdstanpy - INFO - Chain [1] start processing\n18:04:59 - cmdstanpy - INFO - Chain [1] done processing\n\n\n&lt;prophet.forecaster.Prophet&gt;\n\n\n\n# Predict on test set with model\npjme_test_prophet = pjme_test.reset_index() \\\n    .rename(columns={'Datetime':'ds',\n                     'PJME_MW':'y'})\n\npjme_test_fcst = model.predict(pjme_test_prophet)\n\n\npjme_test_fcst.head()\n\n\n\n\n\n\n\n\nds\ntrend\nyhat_lower\nyhat_upper\ntrend_lower\ntrend_upper\nadditive_terms\nadditive_terms_lower\nadditive_terms_upper\ndaily\n...\nweekly\nweekly_lower\nweekly_upper\nyearly\nyearly_lower\nyearly_upper\nmultiplicative_terms\nmultiplicative_terms_lower\nmultiplicative_terms_upper\nyhat\n\n\n\n\n0\n2015-01-01 01:00:00\n31210.530967\n23912.189071\n32701.384160\n31210.530967\n31210.530967\n-2893.742472\n-2893.742472\n-2893.742472\n-4430.272423\n...\n1281.328732\n1281.328732\n1281.328732\n255.201219\n255.201219\n255.201219\n0.0\n0.0\n0.0\n28316.788495\n\n\n1\n2015-01-01 02:00:00\n31210.494154\n22326.307825\n31018.012373\n31210.494154\n31210.494154\n-4398.239425\n-4398.239425\n-4398.239425\n-5927.272577\n...\n1272.574102\n1272.574102\n1272.574102\n256.459050\n256.459050\n256.459050\n0.0\n0.0\n0.0\n26812.254729\n\n\n2\n2015-01-01 03:00:00\n31210.457342\n21462.567951\n30599.849415\n31210.457342\n31210.457342\n-5269.974485\n-5269.974485\n-5269.974485\n-6790.346308\n...\n1262.613389\n1262.613389\n1262.613389\n257.758434\n257.758434\n257.758434\n0.0\n0.0\n0.0\n25940.482857\n\n\n3\n2015-01-01 04:00:00\n31210.420529\n21734.347259\n30591.022280\n31210.420529\n31210.420529\n-5411.456410\n-5411.456410\n-5411.456410\n-6922.126021\n...\n1251.570211\n1251.570211\n1251.570211\n259.099400\n259.099400\n259.099400\n0.0\n0.0\n0.0\n25798.964119\n\n\n4\n2015-01-01 05:00:00\n31210.383716\n22005.992294\n31027.473128\n31210.383716\n31210.383716\n-4737.018106\n-4737.018106\n-4737.018106\n-6237.080479\n...\n1239.580401\n1239.580401\n1239.580401\n260.481971\n260.481971\n260.481971\n0.0\n0.0\n0.0\n26473.365610\n\n\n\n\n5 rows × 22 columns\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nfig = model.plot(pjme_test_fcst, ax=ax)\nax.set_title('Prophet Forecast')\nplt.show()\n\n\n\n\n\n\n\n\n\nfig = model.plot_components(pjme_test_fcst)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCompare Forecast to Actuals\n\n# Plot the forecast with the actuals\nf, ax = plt.subplots(figsize=(15, 5))\nax.scatter(pjme_test.index, pjme_test['PJME_MW'], color='r')\nfig = model.plot(pjme_test_fcst, ax=ax)\n\n\n\n\n\n\n\n\n\npjme_test['PJME_MW'].head()\n\nDatetime\n2015-12-31 01:00:00    24305.0\n2015-12-31 02:00:00    23156.0\n2015-12-31 03:00:00    22514.0\n2015-12-31 04:00:00    22330.0\n2015-12-31 05:00:00    22773.0\nName: PJME_MW, dtype: float64\n\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.scatter(pjme_test.index, pjme_test['PJME_MW'], color='r')\n\nfig = model.plot(pjme_test_fcst, ax=ax)\nax.set_xbound(lower=pd.to_datetime('12-01-2014'),\n              upper=pd.to_datetime('02-01-2015'))\nax.set_ylim(0, 60000)\nax.legend()\nplot = plt.suptitle('January 2015 Forecast vs Actuals')\n\n\n\n\n\n\n\n\n\n# Plot the forecast with the actuals\nf, ax = plt.subplots(figsize=(15, 5))\nax.scatter(pjme_test.index, pjme_test['PJME_MW'], color='r')\nfig = model.plot(pjme_test_fcst, ax=ax)\nax.set_xbound(lower= pd.to_datetime('2015-01-05'), upper=pd.to_datetime('2015-01-08'))\nax.set_ylim(0, 60000)\nax.set_title('First Week of January Forecast vs Actuals')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEvaluate the model with Error Metrics\n\nnp.sqrt(mean_squared_error(y_true=pjme_test['PJME_MW'],\n                   y_pred=pjme_test_fcst['yhat']))\n\n6616.966074225221\n\n\n\nmean_absolute_error(y_true=pjme_test['PJME_MW'],\n                   y_pred=pjme_test_fcst['yhat'])\n\n5181.911537928106\n\n\n\nmean_absolute_percentage_error(y_true=pjme_test['PJME_MW'],\n                   y_pred=pjme_test_fcst['yhat'])\n\n16.512003880182647\n\n\n\n\nAdding Holidays\nNext we will see if adding holiday indicators will help the accuracy of the model. Prophet comes with a Holiday Effects parameter that can be provided to the model prior to training.\nWe will use the built in pandas USFederalHolidayCalendar to pull the list of holidays\n\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n\ncal = calendar()\n\n\nholidays = cal.holidays(start=pjme.index.min(),\n                        end=pjme.index.max(),\n                        return_name=True)\nholiday_df = pd.DataFrame(data=holidays,\n                          columns=['holiday'])\nholiday_df = holiday_df.reset_index().rename(columns={'index':'ds'})\n\n\nmodel_with_holidays = Prophet(holidays=holiday_df)\nmodel_with_holidays.fit(pjme_train_prophet)\n\n18:05:15 - cmdstanpy - INFO - Chain [1] start processing\n18:06:36 - cmdstanpy - INFO - Chain [1] done processing\n\n\n&lt;prophet.forecaster.Prophet&gt;\n\n\n\n# Predict on training set with model\npjme_test_fcst_with_hols = \\\n    model_with_holidays.predict(df=pjme_test_prophet)\n\n\nfig = model_with_holidays.plot_components(\n    pjme_test_fcst_with_hols)\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.scatter(pjme_test.index, pjme_test['PJME_MW'], color='r')\nfig = model.plot(pjme_test_fcst_with_hols, ax=ax)\nax.set_xbound(lower=pd.to_datetime('07-01-2015'),\n              upper=pd.to_datetime('07-07-2015'))\nax.set_ylim(0, 60000)\nplot = plt.suptitle('July 4 Predictions vs Actual')\n\n\n\n\n\n\n\n\n\nnp.sqrt(mean_squared_error(y_true=pjme_test['PJME_MW'],\n                   y_pred=pjme_test_fcst_with_hols['yhat']))\n\n6639.587205626055\n\n\n\nmean_absolute_error(y_true=pjme_test['PJME_MW'],\n                   y_pred=pjme_test_fcst_with_hols['yhat'])\n\n5201.46462763833\n\n\n\nmean_absolute_percentage_error(y_true=pjme_test['PJME_MW'],\n                   y_pred=pjme_test_fcst_with_hols['yhat'])\n\n16.558807523531467\n\n\n\n\nPredict into the Future\nWe can use the built in make_future_dataframe method to build our future dataframe and make predictions.\n\nfuture = model.make_future_dataframe(periods=365*24, freq='h', include_history=False)\nforecast = model_with_holidays.predict(future)\n\n\nforecast[['ds','yhat']].head()\n\n\n\n\n\n\n\n\nds\nyhat\n\n\n\n\n0\n2015-01-01 01:00:00\n25567.271675\n\n\n1\n2015-01-01 02:00:00\n24065.351481\n\n\n2\n2015-01-01 03:00:00\n23195.828972\n\n\n3\n2015-01-01 04:00:00\n23056.212620\n\n\n4\n2015-01-01 05:00:00\n23732.192245\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Hourly Time Series Forecasting using Facebook's Prophet"
    ]
  },
  {
    "objectID": "index.html#future-prediction-using-nixtla-library",
    "href": "index.html#future-prediction-using-nixtla-library",
    "title": "Time Series Analysis",
    "section": "Future Prediction using Nixtla Library",
    "text": "Future Prediction using Nixtla Library",
    "crumbs": [
      "Blog",
      "[Time Series Analysis](https://bthek1.github.io/TimeSeriesML/)"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Phisaver Project",
    "section": "",
    "text": "Develop ML model to\n\nPredict consumption\nDetect anomalies\ndetect devices from using only the total power usage\n\nsolar = func(future clouds, future sunlight, past three days)\nconsumption = func(individual components, future weather, future humidity, past three days, past week, past month)\naircon = func(future weather, future humidity, past three days)",
    "crumbs": [
      "Blog",
      "Phisaver Project"
    ]
  },
  {
    "objectID": "core.html#project-specifications",
    "href": "core.html#project-specifications",
    "title": "Phisaver Project",
    "section": "",
    "text": "Develop ML model to\n\nPredict consumption\nDetect anomalies\ndetect devices from using only the total power usage\n\nsolar = func(future clouds, future sunlight, past three days)\nconsumption = func(individual components, future weather, future humidity, past three days, past week, past month)\naircon = func(future weather, future humidity, past three days)",
    "crumbs": [
      "Blog",
      "Phisaver Project"
    ]
  },
  {
    "objectID": "core.html#data-strucuture",
    "href": "core.html#data-strucuture",
    "title": "Phisaver Project",
    "section": "Data Strucuture",
    "text": "Data Strucuture\n\nEnergy\n\n1 hour integral of power in kWh\n2021/04/01 - 2023/11/09\n\nPower\n\n5 mins average of Iotawatt\n2022/07/21 - 2023/11/09\n\nIotawatt\n\n1 min average of sensor readings",
    "crumbs": [
      "Blog",
      "Phisaver Project"
    ]
  },
  {
    "objectID": "core.html#frameworks",
    "href": "core.html#frameworks",
    "title": "Phisaver Project",
    "section": "Frameworks",
    "text": "Frameworks\n\nDarts models : https://unit8co.github.io/darts/generated_api/darts.models.forecasting.html\nNixtla models : https://nixtla.github.io/statsforecast/src/core/models.html\nsktime",
    "crumbs": [
      "Blog",
      "Phisaver Project"
    ]
  },
  {
    "objectID": "core.html#models",
    "href": "core.html#models",
    "title": "Phisaver Project",
    "section": "Models",
    "text": "Models\nAuto Forecast: Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series. Includes automatic versions of: Arima, ETS, Theta, CES.\nExponential Smoothing: Uses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Examples: SES, Holt’s Winters, SSO.\nBenchmark models: classical models for establishing baselines. Examples: Mean, Naive, Random Walk\nIntermittent or Sparse models: suited for series with very few non-zero observations. Examples: CROSTON, ADIDA, IMAPA\nMultiple Seasonalities: suited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs. Examples: MSTL.\nTheta Models: fit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts. Examples: Theta, DynamicTheta\n\nStatical\n\nAutoARIMA:\n\nAutomatically selects the best ARIMA (AutoRegressive Integrated Moving Average) model using an information criterion.\n\nHoltWinters:\n\nTriple exponential smoothing, Holt-Winters’ method is an extension of exponential smoothing for series that contain both trend and seasonality.\n\nCrostonClassic as Croston:\nHistoricAverage: arthimetic mean\nDynamicOptimizedTheta as DOT:\n\nThe theta family of models has been shown to perform well in various datasets such as M3. Models the deseasonalized time series.\n\nSeasonalNaive:\n\nMemory Efficient Seasonal Naive predictions\n\nMSTL\nFFT\n\n\n\nML\n\nXGBRegressor\nLGBMRegressor\nLinearRegression\n\ntarget_transforms: These are transformations applied to the target variable before model training and after model prediction. This can be useful when working with data that may benefit from transformations, such as log-transforms for highly skewed data.\nlags: This parameter accepts specific lag values to be used as regressors. Lags represent how many steps back in time you want to look when creating features for your model. For example, if you want to use the previous day’s data as a feature for predicting today’s value, you would specify a lag of 1.\nlags_transforms: These are specific transformations for each lag. This allows you to apply transformations to your lagged features.\n\nexpanding_mean\nrolling_mean\n\ndate_features: This parameter specifies date-related features to be used as regressors. For instance, you might want to include the day of the week or the month as a feature in your model.\nnum_threads: This parameter controls the number of threads to use for parallelizing feature creation, helping to speed up this process when working with large datasets.",
    "crumbs": [
      "Blog",
      "Phisaver Project"
    ]
  },
  {
    "objectID": "core.html#neural",
    "href": "core.html#neural",
    "title": "Phisaver Project",
    "section": "Neural",
    "text": "Neural\n\nNBEATS\nNHITS\nMLP",
    "crumbs": [
      "Blog",
      "Phisaver Project"
    ]
  },
  {
    "objectID": "FFT/fft-examples.html",
    "href": "FFT/fft-examples.html",
    "title": "Darts - Fast Fourier Transform Forecasting Model (FFT)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom darts import TimeSeries\nfrom darts.models import FFT, AutoARIMA, ExponentialSmoothing, Theta\nfrom darts.metrics import mae\nfrom darts.utils.missing_values import fill_missing_values\nfrom darts.datasets import TemperatureDataset, AirPassengersDataset, EnergyDataset\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nimport logging\n\nlogging.disable(logging.CRITICAL)\n\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/neptune/internal/backends/hosted_client.py:51: NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n  from neptune.version import version as neptune_client_version\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/pytorch_lightning/loggers/neptune.py:51: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n  import neptune.new as neptune",
    "crumbs": [
      "Blog",
      "FFT",
      "Darts - Fast Fourier Transform Forecasting Model (FFT)"
    ]
  },
  {
    "objectID": "FFT/fft-examples.html#read-and-format",
    "href": "FFT/fft-examples.html#read-and-format",
    "title": "Darts - Fast Fourier Transform Forecasting Model (FFT)",
    "section": "Read and format",
    "text": "Read and format\nHere we simply read the CSV file containing daily temperatures, and transform the values into the desired format.\n\nts = TemperatureDataset().load()",
    "crumbs": [
      "Blog",
      "FFT",
      "Darts - Fast Fourier Transform Forecasting Model (FFT)"
    ]
  },
  {
    "objectID": "FFT/fft-examples.html#constructing-timeseries-instances-for-training-and-validation",
    "href": "FFT/fft-examples.html#constructing-timeseries-instances-for-training-and-validation",
    "title": "Darts - Fast Fourier Transform Forecasting Model (FFT)",
    "section": "Constructing TimeSeries instances for training and validation",
    "text": "Constructing TimeSeries instances for training and validation\n\ntrain, val = ts.split_after(pd.Timestamp(\"19850701\"))\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")",
    "crumbs": [
      "Blog",
      "FFT",
      "Darts - Fast Fourier Transform Forecasting Model (FFT)"
    ]
  },
  {
    "objectID": "FFT/fft-examples.html#basic-fft-model",
    "href": "FFT/fft-examples.html#basic-fft-model",
    "title": "Darts - Fast Fourier Transform Forecasting Model (FFT)",
    "section": "Basic FFT model",
    "text": "Basic FFT model\n\nmodel = FFT(required_matches=set(), nr_freqs_to_keep=None)\nmodel.fit(train)\npred_val = model.predict(len(val))\n\nThe plot below shows us that a simple DFT with a random train-test split will most likely lead to bad results. Upon closer inspection we can see that the prediction (in purple) simply repeats the training set (blue). This is the standard behavior of the DFT, and by itself it is quite useless, since repeating our training set could be done much more efficiently. Three improvements were made to this approach.\n\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\npred_val.plot(label=\"prediction\")\nprint(\"MAE:\", mae(pred_val, val))\n\nMAE: 5.424526892430279",
    "crumbs": [
      "Blog",
      "FFT",
      "Darts - Fast Fourier Transform Forecasting Model (FFT)"
    ]
  },
  {
    "objectID": "FFT/fft-examples.html#improvement-1-crop-the-training-set",
    "href": "FFT/fft-examples.html#improvement-1-crop-the-training-set",
    "title": "Darts - Fast Fourier Transform Forecasting Model (FFT)",
    "section": "Improvement 1: Crop the training set",
    "text": "Improvement 1: Crop the training set\nThe first improvement consists of cropping the training set before feeding it to the FFT algorithm such that the first timestamp in the cropped series matches the first timestamp to be predicted in terms of seasonality, i.e. it has the same month, day, weekday, time of day, etc. We could achieve this by passing the optional argument required_matches to the FFT constructor that explicitly tells our model which timestamp attributes are relevant. But actually, if we don’t set it manually, the model will attempt to automatically find the pd.Timestamp attributes that are relevant and crop the training set accordingly (which we will do here).\n\nmodel = FFT(nr_freqs_to_keep=None)\nmodel.fit(train)\npred_val = model.predict(len(val))\n\nWe can see that the results look like the seasonality of the predictions nicely aligns with the seasonality of the validation set. However, we are still just repeating the training set, including all of the noise. Looking at the error we can see that this is still a pretty bad forecast.\n\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\npred_val.plot(label=\"predict\")\nprint(\"MAE:\", mae(pred_val, val))\n\nMAE: 3.0995766932270916",
    "crumbs": [
      "Blog",
      "FFT",
      "Darts - Fast Fourier Transform Forecasting Model (FFT)"
    ]
  },
  {
    "objectID": "FFT/fft-examples.html#improvement-2-filtering-out-low-amplitude-waves",
    "href": "FFT/fft-examples.html#improvement-2-filtering-out-low-amplitude-waves",
    "title": "Darts - Fast Fourier Transform Forecasting Model (FFT)",
    "section": "Improvement 2: Filtering out low-amplitude waves",
    "text": "Improvement 2: Filtering out low-amplitude waves\nThe decomposition of the DFT into the frequency domain allows us to selectively filter out waves with low amplitudes. This allows us to keep strong seasonal trends while discarding some noise. This is achieved in the FFT model by passing the optional argument nr_freqs_to_keep. This argument represents the total number of frequencies that will be kept. For instance, if a value of 20 is passed, only the 20 frequencies with the highest amplitudes will be utilized. The default value is set to 10.\n\nmodel = FFT(nr_freqs_to_keep=20)\nmodel.fit(train)\npred_val = model.predict(len(val))\n\nWe get a signal that is less noisy. Depending on the data set, this might be a better forecast. Looking at the error metric, we can see that this model performs significantly better than the previous models.\n\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\npred_val.plot(label=\"predict\")\nprint(\"MAE:\", mae(pred_val, val))\n\nMAE: 2.2941917142812893",
    "crumbs": [
      "Blog",
      "FFT",
      "Darts - Fast Fourier Transform Forecasting Model (FFT)"
    ]
  },
  {
    "objectID": "FFT/fft-examples.html#improvement-3-detrending",
    "href": "FFT/fft-examples.html#improvement-3-detrending",
    "title": "Darts - Fast Fourier Transform Forecasting Model (FFT)",
    "section": "Improvement 3: Detrending",
    "text": "Improvement 3: Detrending\nLet’s try out a different data set that has a global upward trend\n\nts_2 = AirPassengersDataset().load()\ntrain, val = ts_2.split_after(pd.Timestamp(\"19551201\"))\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\n\n\n\n\n\n\n\n\n\nmodel = FFT()\nmodel.fit(train)\npred_val = model.predict(len(val))\n\nClearly, our model fails completely at incorporating the upward trend. Due to the trend, our model also fails to recognize the monthly seasonality.\n\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\npred_val.plot(label=\"prediction\")\n\n\n\n\n\n\n\n\nThis problem can be solved by setting the optional trend argument to either ‘poly’ or ‘exp’, which fits a polynomial or exponential funtion to the data and subtracts it before moving on to DFT. When predicting, the trend is added again.\n\nmodel = FFT(trend=\"poly\")\nmodel.fit(train)\npred_val = model.predict(len(val))\n\nWe have a much better prediction now.\n\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\npred_val.plot(label=\"prediction\")",
    "crumbs": [
      "Blog",
      "FFT",
      "Darts - Fast Fourier Transform Forecasting Model (FFT)"
    ]
  },
  {
    "objectID": "FFT/fft-examples.html#new-data-hourly-nuclear-energy-generation",
    "href": "FFT/fft-examples.html#new-data-hourly-nuclear-energy-generation",
    "title": "Darts - Fast Fourier Transform Forecasting Model (FFT)",
    "section": "New Data: Hourly Nuclear Energy Generation",
    "text": "New Data: Hourly Nuclear Energy Generation\n\nts_3 = EnergyDataset().load()\nts_3 = fill_missing_values(ts_3, \"auto\")\nts_3 = ts_3[\"generation nuclear\"]\ntrain, val = ts_3.split_after(pd.Timestamp(\"2017-07-01\"))\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\n\n\n\n\n\n\n\n\nInstead of simply looking at the performance of the FFT model, we can also look at how a bunch of other forecasting models performs on this new data set in terms of MAE. Surprisingly, on this dataset, the FFT model outperforms all of the others (at least with their default parameters). Granted, this dataset was specifically chosen because of its highly seasonal nature. However, this shows us that there are use cases for FFT. Furthermore, the FFT model has a much shorter running time than the other models!\n\nmodels = [AutoARIMA(), Theta(), FFT()]\n\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\n\nfor model in models:\n    model.fit(train)\n    pred_val = model.predict(len(val))\n    pred_val.plot(label=f'{model}')\n    print(str(model) + \" MAE: \" + str(mae(pred_val, val)))\n\nAutoARIMA() MAE: 956.1953089418552\nTheta() MAE: 944.0018921218331\nFFT() MAE: 643.337489093281",
    "crumbs": [
      "Blog",
      "FFT",
      "Darts - Fast Fourier Transform Forecasting Model (FFT)"
    ]
  },
  {
    "objectID": "xgboost2.html",
    "href": "xgboost2.html",
    "title": "XGBoost: Forecast Energy Consumption",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nimport zipfile,kaggle\nfrom fastai.imports import *\n\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\n\nfrom nbdevAuto.functions import * \nimport nbdevAuto.functions\ncolor_pal = sns.color_palette()\nplt.style.use('fivethirtyeight')\n\nname = 'hourly-energy-consumption'\npath = Path(f'Data/{name}')\nuser = 'robikscube'\nkaggle_dataset_download(user = user, \n                         name = name)\n\nfile exists\ndf = pd.read_csv(f'{path}/PJME_hourly.csv')\ndf = df.set_index('Datetime')\ndf.index = pd.to_datetime(df.index)\ndf.plot(style='.',\n        figsize=(15, 5),\n        color=color_pal[0],\n        title='PJME Energy Use in MW')\nplt.show()",
    "crumbs": [
      "Blog",
      "XGBoost: Forecast Energy Consumption"
    ]
  },
  {
    "objectID": "xgboost2.html#train-using-cross-validation",
    "href": "xgboost2.html#train-using-cross-validation",
    "title": "XGBoost: Forecast Energy Consumption",
    "section": "Train Using Cross Validation",
    "text": "Train Using Cross Validation\n\ntss = TimeSeriesSplit(n_splits=5, test_size=24*365*1, gap=24)\ndf = df.sort_index()\n\n\nfold = 0\npreds = []\nscores = []\nfor train_idx, val_idx in tss.split(df):\n    train = df.iloc[train_idx]\n    test = df.iloc[val_idx]\n\n    train = create_features(train)\n    test = create_features(test)\n\n    FEATURES = ['dayofyear', 'hour', 'dayofweek', 'quarter', 'month','year',\n                'lag1','lag2','lag3']\n    TARGET = 'PJME_MW'\n\n    X_train = train[FEATURES]\n    y_train = train[TARGET]\n\n    X_test = test[FEATURES]\n    y_test = test[TARGET]\n\n    reg = xgb.XGBRegressor(base_score=0.5, booster='gbtree',    \n                           n_estimators=1000,\n                           early_stopping_rounds=50,\n                           objective='reg:linear',\n                           max_depth=3,\n                           learning_rate=0.01)\n    reg.fit(X_train, y_train,\n            eval_set=[(X_train, y_train), (X_test, y_test)],\n            verbose=100)\n\n    y_pred = reg.predict(X_test)\n    preds.append(y_pred)\n    score = np.sqrt(mean_squared_error(y_test, y_pred))\n    scores.append(score)\n\n[0] validation_0-rmse:32732.49608   validation_1-rmse:31956.60163\n\n\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [17:58:44] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n\n\n[100]   validation_0-rmse:12532.64369   validation_1-rmse:11906.14134\n[200]   validation_0-rmse:5747.92495    validation_1-rmse:5359.26490\n[300]   validation_0-rmse:3872.48134    validation_1-rmse:3900.86965\n[400]   validation_0-rmse:3434.23853    validation_1-rmse:3762.33705\n[442]   validation_0-rmse:3369.34730    validation_1-rmse:3764.82810\n[0] validation_0-rmse:32672.16678   validation_1-rmse:32138.89241\n\n\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [17:58:47] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n\n\n[100]   validation_0-rmse:12513.65574   validation_1-rmse:12224.93373\n[200]   validation_0-rmse:5753.34937    validation_1-rmse:5662.07107\n[300]   validation_0-rmse:3902.71304    validation_1-rmse:3933.73076\n[400]   validation_0-rmse:3476.90515    validation_1-rmse:3590.55005\n[500]   validation_0-rmse:3353.72424    validation_1-rmse:3516.39915\n[600]   validation_0-rmse:3297.94766    validation_1-rmse:3481.94003\n[700]   validation_0-rmse:3258.48267    validation_1-rmse:3461.37383\n[800]   validation_0-rmse:3221.51553    validation_1-rmse:3436.49603\n[900]   validation_0-rmse:3190.11480    validation_1-rmse:3428.88699\n[999]   validation_0-rmse:3166.16314    validation_1-rmse:3420.31309\n[0] validation_0-rmse:32631.20370   validation_1-rmse:31073.29733\n\n\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [17:58:52] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n\n\n[100]   validation_0-rmse:12499.28425   validation_1-rmse:11136.70202\n[200]   validation_0-rmse:5750.81453    validation_1-rmse:4813.22087\n[300]   validation_0-rmse:3917.04200    validation_1-rmse:3553.46419\n[400]   validation_0-rmse:3494.55924    validation_1-rmse:3495.32356\n[410]   validation_0-rmse:3476.66883    validation_1-rmse:3502.25535\n[0] validation_0-rmse:32528.44438   validation_1-rmse:31475.39670\n\n\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [17:58:55] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n\n\n[100]   validation_0-rmse:12462.36581   validation_1-rmse:12020.28283\n[200]   validation_0-rmse:5738.57925    validation_1-rmse:5796.45874\n[300]   validation_0-rmse:3918.53218    validation_1-rmse:4388.39477\n[400]   validation_0-rmse:3501.24270    validation_1-rmse:4173.36380\n[500]   validation_0-rmse:3384.02490    validation_1-rmse:4119.56538\n[600]   validation_0-rmse:3325.50024    validation_1-rmse:4105.01446\n[700]   validation_0-rmse:3282.73755    validation_1-rmse:4091.23557\n[800]   validation_0-rmse:3250.37610    validation_1-rmse:4083.12690\n[900]   validation_0-rmse:3223.87814    validation_1-rmse:4081.46154\n[999]   validation_0-rmse:3199.82843    validation_1-rmse:4052.57120\n[0] validation_0-rmse:32462.05557   validation_1-rmse:31463.90500\n\n\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [17:59:02] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n\n\n[100]   validation_0-rmse:12445.87740   validation_1-rmse:11963.42706\n[200]   validation_0-rmse:5752.44568    validation_1-rmse:5611.92884\n[300]   validation_0-rmse:3951.51709    validation_1-rmse:4156.41403\n[400]   validation_0-rmse:3539.25569    validation_1-rmse:4006.58873\n[440]   validation_0-rmse:3479.88018    validation_1-rmse:4011.05271\n\n\n\nprint(f'Score across folds {np.mean(scores):0.4f}')\nprint(f'Fold scores:{scores}')\n\nScore across folds 3742.5833\nFold scores:[3760.8277187583353, 3420.313091887879, 3478.018038580526, 4052.5712055405547, 4001.186553933809]",
    "crumbs": [
      "Blog",
      "XGBoost: Forecast Energy Consumption"
    ]
  },
  {
    "objectID": "xgboost2.html#predict-the-future",
    "href": "xgboost2.html#predict-the-future",
    "title": "XGBoost: Forecast Energy Consumption",
    "section": "Predict the future",
    "text": "Predict the future\n\nfuture_w_features['pred'] = reg.predict(future_w_features[FEATURES])\n\n\nfuture_w_features['pred'].plot(figsize=(10, 5),\n                               color=color_pal[4],\n                               ms=1,\n                               lw=1,\n                               title='Future Predictions')\nplt.show()",
    "crumbs": [
      "Blog",
      "XGBoost: Forecast Energy Consumption"
    ]
  },
  {
    "objectID": "Nixtla/electricityloadforecasting.html",
    "href": "Nixtla/electricityloadforecasting.html",
    "title": "Nixtla - Electricity Load Forecast",
    "section": "",
    "text": "Some time series are generated from very low frequency data. These data generally exhibit multiple seasonalities. For example, hourly data may exhibit repeated patterns every hour (every 24 observations) or every day (every 24 * 7, hours per day, observations). This is the case for electricity load. Electricity load may vary hourly, e.g., during the evenings electricity consumption may be expected to increase. But also, the electricity load varies by week. Perhaps on weekends there is an increase in electrical activity.\nIn this example we will show how to model the two seasonalities of the time series to generate accurate forecasts in a short time. We will use hourly PJM electricity load data. The original data can be found here.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Electricity Load Forecast"
    ]
  },
  {
    "objectID": "Nixtla/electricityloadforecasting.html#introduction",
    "href": "Nixtla/electricityloadforecasting.html#introduction",
    "title": "Nixtla - Electricity Load Forecast",
    "section": "",
    "text": "Some time series are generated from very low frequency data. These data generally exhibit multiple seasonalities. For example, hourly data may exhibit repeated patterns every hour (every 24 observations) or every day (every 24 * 7, hours per day, observations). This is the case for electricity load. Electricity load may vary hourly, e.g., during the evenings electricity consumption may be expected to increase. But also, the electricity load varies by week. Perhaps on weekends there is an increase in electrical activity.\nIn this example we will show how to model the two seasonalities of the time series to generate accurate forecasts in a short time. We will use hourly PJM electricity load data. The original data can be found here.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Electricity Load Forecast"
    ]
  },
  {
    "objectID": "Nixtla/electricityloadforecasting.html#libraries",
    "href": "Nixtla/electricityloadforecasting.html#libraries",
    "title": "Nixtla - Electricity Load Forecast",
    "section": "Libraries",
    "text": "Libraries\nIn this example we will use the following libraries:\n\nStatsForecast. Lightning ⚡️ fast forecasting with statistical and econometric models. Includes the MSTL model for multiple seasonalities.\nDatasetsForecast. Used to evaluate the performance of the forecasts.\nProphet. Benchmark model developed by Facebook.\nNeuralProphet. Deep Learning version of Prophet. Used as benchark.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Electricity Load Forecast"
    ]
  },
  {
    "objectID": "Nixtla/electricityloadforecasting.html#forecast-using-multiple-seasonalities",
    "href": "Nixtla/electricityloadforecasting.html#forecast-using-multiple-seasonalities",
    "title": "Nixtla - Electricity Load Forecast",
    "section": "Forecast using Multiple Seasonalities",
    "text": "Forecast using Multiple Seasonalities\n\nElectricity Load Data\nAccording to the dataset’s page,\n\nPJM Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system serving all or parts of Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, West Virginia, and the District of Columbia. The hourly power consumption data comes from PJM’s website and are in megawatts (MW).\n\nLet’s take a look to the data.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/panambY/Hourly_Energy_Consumption/master/data/PJM_Load_hourly.csv')\ndf.head()\n\n\n\n\n\n\n\n\nDatetime\nPJM_Load_MW\n\n\n\n\n0\n1998-12-31 01:00:00\n29309.0\n\n\n1\n1998-12-31 02:00:00\n28236.0\n\n\n2\n1998-12-31 03:00:00\n27692.0\n\n\n3\n1998-12-31 04:00:00\n27596.0\n\n\n4\n1998-12-31 05:00:00\n27888.0\n\n\n\n\n\n\n\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\n\ndf.columns = ['ds', 'y']\ndf.insert(0, 'unique_id', 'PJM_Load_hourly')\ndf['ds'] = pd.to_datetime(df['ds'])\ndf = df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\ndf.tail()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n32891\nPJM_Load_hourly\n2001-12-31 20:00:00\n36392.0\n\n\n32892\nPJM_Load_hourly\n2001-12-31 21:00:00\n35082.0\n\n\n32893\nPJM_Load_hourly\n2001-12-31 22:00:00\n33890.0\n\n\n32894\nPJM_Load_hourly\n2001-12-31 23:00:00\n32590.0\n\n\n32895\nPJM_Load_hourly\n2002-01-01 00:00:00\n31569.0\n\n\n\n\n\n\n\n\ntype(df['ds'])\n\npandas.core.series.Series\n\n\n\ndf.plot(x='ds', y='y')\n\n\n\n\n\n\n\n\nWe clearly observe that the time series exhibits seasonal patterns. Moreover, the time series contains 32,896 observations, so it is necessary to use very computationally efficient methods to display them in production.\n\n\nMSTL model\nThe MSTL (Multiple Seasonal-Trend decomposition using LOESS) model, originally developed by Kasun Bandara, Rob J Hyndman and Christoph Bergmeir, decomposes the time series in multiple seasonalities using a Local Polynomial Regression (LOESS). Then it forecasts the trend using a custom non-seasonal model and each seasonality using a SeasonalNaive model.\nStatsForecast contains a fast implementation of the MSTL model. Also, the decomposition of the time series can be calculated.\nFirst we must define the model parameters. As mentioned before, the electricity load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] as the seasonalities that the MSTL model receives. We must also specify the manner in which the trend will be forecasted. In this case we will use the AutoARIMA model.\n\nintervals = ConformalIntervals()\n\nmstl = MSTL(\n    season_length=[24, 24 * 7], # seasonalities of the time series \n    trend_forecaster=AutoARIMA(), # model used to forecast trend\n    prediction_intervals = intervals\n)\n\nOnce the model is instantiated, we have to instantiate the StatsForecast class to create forecasts.\n\nsf = StatsForecast(\n    models=[mstl], # model used to fit each time series \n    freq='H', # frequency of the data\n    fallback_model = SeasonalNaive(season_length=7)\n)\n\n\nFit the model\nAfer that, we just have to use the fit method to fit each model to each time series.\n\nsf = sf.fit(df=df)\n\n\n\nDecompose the time series in multiple seasonalities\nOnce the model is fitted, we can access the decomposition using the fitted_ attribute of StatsForecast. This attribute stores all relevant information of the fitted models for each of the time series.\nIn this case we are fitting a single model for a single time series, so by accessing the fitted_ location [0, 0] we will find the relevant information of our model. The MSTL class generates a model_ attribute that contains the way the series was decomposed.\n\nsf.fitted_[0, 0].model_\n\n\n\n\n\n\n\n\ndata\ntrend\nseasonal24\nseasonal168\nremainder\n\n\n\n\n0\n22259.0\n26183.898892\n-5215.124554\n609.000432\n681.225229\n\n\n1\n21244.0\n26181.599305\n-6255.673234\n603.823918\n714.250011\n\n\n2\n20651.0\n26179.294886\n-6905.329895\n636.820423\n740.214587\n\n\n3\n20421.0\n26176.985472\n-7073.420118\n615.825999\n701.608647\n\n\n4\n20713.0\n26174.670877\n-7062.395760\n991.521912\n609.202971\n\n\n...\n...\n...\n...\n...\n...\n\n\n32891\n36392.0\n33123.552727\n4387.149171\n-488.177882\n-630.524015\n\n\n32892\n35082.0\n33148.242575\n3479.852929\n-682.928737\n-863.166767\n\n\n32893\n33890.0\n33172.926165\n2307.808829\n-650.566775\n-940.168219\n\n\n32894\n32590.0\n33197.603322\n748.587723\n-555.177849\n-801.013195\n\n\n32895\n31569.0\n33222.273902\n-967.124123\n-265.895357\n-420.254422\n\n\n\n\n32896 rows × 5 columns\n\n\n\nLet’s look graphically at the different components of the time series.\n\nsf.fitted_[0, 0].model_.tail(24 * 28).plot(subplots=True, grid=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe observe that there is a clear trend towards the high (orange line). This component would be predicted with the AutoARIMA model. We can also observe that every 24 hours and every 24 * 7 hours there is a very well defined pattern. These two components will be forecast separately using a SeasonalNaive model.\n\n\nProduce forecasts\nTo generate forecasts we only have to use the predict method specifying the forecast horizon (h). In addition, to calculate prediction intervals associated to the forecasts, we can include the parameter level that receives a list of levels of the prediction intervals we want to build. In this case we will only calculate the 90% forecast interval (level=[90]).\n\nforecasts = sf.predict(h=24, level=[90])\nforecasts.head()\n\n\n\n\n\n\n\n\nds\nMSTL\nMSTL-lo-90\nMSTL-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\nPJM_Load_hourly\n2002-01-01 01:00:00\n29956.744141\n27476.636719\n32436.849609\n\n\nPJM_Load_hourly\n2002-01-01 02:00:00\n29057.691406\n26577.585938\n31537.796875\n\n\nPJM_Load_hourly\n2002-01-01 03:00:00\n28654.699219\n26174.593750\n31134.806641\n\n\nPJM_Load_hourly\n2002-01-01 04:00:00\n28499.009766\n26018.904297\n30979.115234\n\n\nPJM_Load_hourly\n2002-01-01 05:00:00\n28821.716797\n26341.611328\n31301.824219\n\n\n\n\n\n\n\nLet’s look at our forecasts graphically.\n\n_, ax = plt.subplots(1, 1, figsize = (20, 7))\ndf_plot = pd.concat([df, forecasts]).set_index('ds').tail(24 * 7)\ndf_plot[['y', 'MSTL']].plot(ax=ax, linewidth=2)\nax.fill_between(df_plot.index, \n                df_plot['MSTL-lo-90'], \n                df_plot['MSTL-hi-90'],\n                alpha=.35,\n                color='orange',\n                label='MSTL-level-90')\nax.set_title('PJM Load Hourly', fontsize=22)\nax.set_ylabel('Electricity Load', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n\n\n\n\n\n\n\n\nIn the next section we will plot different models so it is convenient to reuse the previous code with the following function.\n\n\n\nPerformance of the MSTL model\n\nSplit Train/Test sets\nTo validate the accuracy of the MSTL model, we will show its performance on unseen data. We will use a classical time series technique that consists of dividing the data into a training set and a test set. We will leave the last 24 observations (the last day) as the test set. So the model will train on 32,872 observations.\n\ndf_test = df.tail(24)\ndf_train = df.drop(df_test.index)\n\n\n\nMSTL model\nIn addition to the MSTL model, we will include the SeasonalNaive model as a benchmark to validate the added value of the MSTL model. Including StatsForecast models is as simple as adding them to the list of models to be fitted.\n\nsf = StatsForecast(\n    models=[mstl, SeasonalNaive(season_length=24)], # add SeasonalNaive model to the list\n    freq='H'\n)\n\nTo measure the fitting time we will use the time module.\n\nfrom time import time\n\nTo retrieve the forecasts of the test set we only have to do fit and predict as before.\n\ninit = time()\nsf = sf.fit(df=df_train)\nforecasts_test = sf.forecast(h=len(df_test), level=[90],  prediction_intervals = intervals)\nend = time()\nforecasts_test.head()\n\n\n\n\n\n\n\n\nds\nMSTL\nMSTL-lo-90\nMSTL-hi-90\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\nPJM_Load_hourly\n2001-12-31 01:00:00\n28345.212891\n23706.974609\n32983.449219\n28326.0\n23668.550781\n32983.449219\n\n\nPJM_Load_hourly\n2001-12-31 02:00:00\n27567.455078\n22929.216797\n32205.693359\n27362.0\n22704.550781\n32019.449219\n\n\nPJM_Load_hourly\n2001-12-31 03:00:00\n27260.001953\n22621.763672\n31898.240234\n27108.0\n22450.550781\n31765.449219\n\n\nPJM_Load_hourly\n2001-12-31 04:00:00\n27328.125000\n22689.886719\n31966.363281\n26865.0\n22207.550781\n31522.449219\n\n\nPJM_Load_hourly\n2001-12-31 05:00:00\n27640.673828\n23002.435547\n32278.910156\n26808.0\n22150.550781\n31465.449219\n\n\n\n\n\n\n\n\ntime_mstl = (end - init) / 60\nprint(f'MSTL Time: {time_mstl:.2f} minutes')\n\nMSTL Time: 0.98 minutes\n\n\nThen we were able to generate forecasts for the next 24 hours. Now let’s look at the graphical comparison of the forecasts with the actual values.\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive'])\n\n\n\n\n\n\n\n\nLet’s look at those produced only by MSTL.\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL'])\n\n\n\n\n\n\n\n\nWe note that MSTL produces very accurate forecasts that follow the behavior of the time series. Now let us calculate numerically the accuracy of the model. We will use the following metrics: MAE, MAPE, MASE, RMSE, SMAPE.\n\nfrom datasetsforecast.losses import (\n    mae, mape, mase, rmse, smape\n)\n\n\nevaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive'])\n\n\n\n\n\n\n\n\nmase\nmae\nmape\nrmse\nsmape\n\n\n\n\nMSTL\n0.341926\n709.932048\n2.182804\n892.888012\n2.162832\n\n\nSeasonalNaive\n0.894653\n1857.541667\n5.648190\n2201.384101\n5.868604\n\n\n\n\n\n\n\nWe observe that MSTL has an improvement of about 60% over the SeasonalNaive method in the test set measured in MASE.\n\n\nComparison with Prophet\nOne of the most widely used models for time series forecasting is Prophet. This model is known for its ability to model different seasonalities (weekly, daily yearly). We will use this model as a benchmark to see if the MSTL adds value for this time series.\n\nfrom prophet import Prophet\n\n# create prophet model\nprophet = Prophet(interval_width=0.9)\ninit = time()\nprophet.fit(df_train)\n# produce forecasts\nfuture = prophet.make_future_dataframe(periods=len(df_test), freq='H', include_history=False)\nforecast_prophet = prophet.predict(future)\nend = time()\n# data wrangling\nforecast_prophet = forecast_prophet[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\nforecast_prophet.columns = ['ds', 'Prophet', 'Prophet-lo-90', 'Prophet-hi-90']\nforecast_prophet.insert(0, 'unique_id', 'PJM_Load_hourly')\nforecast_prophet.head()\n\n22:54:15 - cmdstanpy - INFO - Chain [1] start processing\n22:54:47 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\n\n\nunique_id\nds\nProphet\nProphet-lo-90\nProphet-hi-90\n\n\n\n\n0\nPJM_Load_hourly\n2001-12-31 01:00:00\n25294.246960\n20311.733404\n29908.815190\n\n\n1\nPJM_Load_hourly\n2001-12-31 02:00:00\n24000.725423\n18852.571714\n28911.111485\n\n\n2\nPJM_Load_hourly\n2001-12-31 03:00:00\n23324.771966\n18382.021271\n28217.281060\n\n\n3\nPJM_Load_hourly\n2001-12-31 04:00:00\n23332.519871\n18219.344766\n28193.514933\n\n\n4\nPJM_Load_hourly\n2001-12-31 05:00:00\n24107.126827\n19242.526107\n29079.004972\n\n\n\n\n\n\n\n\ntime_prophet = (end - init) / 60\nprint(f'Prophet Time: {time_prophet:.2f} minutes')\n\nProphet Time: 0.58 minutes\n\n\n\ntimes = pd.DataFrame({'model': ['MSTL', 'Prophet'], 'time (mins)': [time_mstl, time_prophet]})\ntimes\n\n\n\n\n\n\n\n\nmodel\ntime (mins)\n\n\n\n\n0\nMSTL\n0.982553\n\n\n1\nProphet\n0.576759\n\n\n\n\n\n\n\nWe observe that the time required for Prophet to perform the fit and predict pipeline is greater than MSTL. Let’s look at the forecasts produced by Prophet.\n\nforecasts_test = forecasts_test.merge(forecast_prophet, how='left', on=['unique_id', 'ds'])\n\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive', 'Prophet'])\n\n\n\n\n\n\n\n\nWe note that Prophet is able to capture the overall behavior of the time series. However, in some cases it produces forecasts well below the actual value. It also does not correctly adjust the valleys.\n\nevaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'Prophet', 'SeasonalNaive'])\n\n\n\n\n\n\n\n\nmase\nmae\nmape\nrmse\nsmape\n\n\n\n\nMSTL\n0.341926\n709.932048\n2.182804\n892.888012\n2.162832\n\n\nProphet\n1.099551\n2282.966977\n7.374979\n2721.817203\n7.726533\n\n\nSeasonalNaive\n0.894653\n1857.541667\n5.648190\n2201.384101\n5.868604\n\n\n\n\n\n\n\nIn terms of accuracy, Prophet is not able to produce better forecasts than the SeasonalNaive model, however, the MSTL model improves Prophet’s forecasts by 69% (MASE).\nWith respect to numerical evaluation, NeuralProphet improves the results of Prophet, as expected, however, MSTL improves over NeuralProphet’s foreacasts by 68% (MASE).\n\n\n\n\n\n\nImportant\n\n\n\nThe performance of NeuralProphet can be improved using hyperparameter optimization, which can increase the fitting time significantly. In this example we show its performance with the default version.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Electricity Load Forecast"
    ]
  },
  {
    "objectID": "Nixtla/electricityloadforecasting.html#conclusion",
    "href": "Nixtla/electricityloadforecasting.html#conclusion",
    "title": "Nixtla - Electricity Load Forecast",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we introduced MSTL, a model originally developed by Kasun Bandara, Rob Hyndman and Christoph Bergmeir capable of handling time series with multiple seasonalities. We also showed that for the PJM electricity load time series offers better performance in time and accuracy than the Prophet and NeuralProphet models.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Electricity Load Forecast"
    ]
  },
  {
    "objectID": "Nixtla/electricityloadforecasting.html#references",
    "href": "Nixtla/electricityloadforecasting.html#references",
    "title": "Nixtla - Electricity Load Forecast",
    "section": "References",
    "text": "References\n\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). “MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns”.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Electricity Load Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - exogenous_variables.html",
    "href": "Nixtla/nixtla - exogenous_variables.html",
    "title": "Exogenous Variables",
    "section": "",
    "text": "Exogenous variables can provide additional information to greatly improve forecasting accuracy. Some examples include price or future promotions variables for demand forecasting, and weather data for electricity load forecast. In this notebook we show an example on how to add different types of exogenous variables to NeuralForecast models for making day-ahead hourly electricity price forecasts (EPF) for France and Belgium markets.\nAll NeuralForecast models are capable of incorporating exogenous variables to model the following conditional predictive distribution: \\[\\mathbb{P}(\\mathbf{y}_{t+1:t+H} \\;|\\; \\mathbf{y}_{[:t]},\\; \\mathbf{x}^{(h)}_{[:t]},\\; \\mathbf{x}^{(f)}_{[:t+H]},\\; \\mathbf{x}^{(s)} )\\]\nwhere the regressors are static exogenous \\(\\mathbf{x}^{(s)}\\), historic exogenous \\(\\mathbf{x}^{(h)}_{[:t]}\\), exogenous available at the time of the prediction \\(\\mathbf{x}^{(f)}_{[:t+H]}\\) and autorregresive features \\(\\mathbf{y}_{[:t]}\\). Depending on the train loss, the model outputs can be point forecasts (location estimators) or uncertainty intervals (quantiles).\nWe will show you how to include exogenous variables in the data, specify variables to a model, and produce forecasts using future exogenous variables.\nYou can run these experiments using GPU with Google Colab.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Exogenous Variables"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - exogenous_variables.html#libraries",
    "href": "Nixtla/nixtla - exogenous_variables.html#libraries",
    "title": "Exogenous Variables",
    "section": "1. Libraries",
    "text": "1. Libraries\n%%capture !pip install neuralforecast",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Exogenous Variables"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - exogenous_variables.html#load-data",
    "href": "Nixtla/nixtla - exogenous_variables.html#load-data",
    "title": "Exogenous Variables",
    "section": "2. Load data",
    "text": "2. Load data\nThe df dataframe contains the target and exogenous variables past information to train the model. The unique_id column identifies the markets, ds contains the datestamps, and y the electricity price.\nInclude both historic and future temporal variables as columns. In this example, we are adding the system load (system_load) as historic data. For future variables, we include a forecast of how much electricity will be produced (gen_forecast) and day of week (week_day). Both the electricity system demand and offer impact the price significantly, including these variables to the model greatly improve performance, as we demonstrate in Olivares et al. (2022).\nThe distinction between historic and future variables will be made later as parameters of the model.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE.csv')\ndf['ds'] = pd.to_datetime(df['ds'])\ndf.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\ngen_forecast\nsystem_load\nweek_day\n\n\n\n\n0\nFR\n2015-01-01 00:00:00\n53.48\n76905.0\n74812.0\n3\n\n\n1\nFR\n2015-01-01 01:00:00\n51.93\n75492.0\n71469.0\n3\n\n\n2\nFR\n2015-01-01 02:00:00\n48.76\n74394.0\n69642.0\n3\n\n\n3\nFR\n2015-01-01 03:00:00\n42.27\n72639.0\n66704.0\n3\n\n\n4\nFR\n2015-01-01 04:00:00\n38.41\n69347.0\n65051.0\n3\n\n\n\n\n\n\n\n\ndf.shape\n\n(32160, 6)\n\n\n\n\n\n\n\n\nTip\n\n\n\nCalendar variables such as day of week, month, and year are very useful to capture long seasonalities.\n\n\n\nplt.figure(figsize=(15,5))\nplt.plot(df[df['unique_id']=='FR']['ds'], df[df['unique_id']=='FR']['y'])\nplt.xlabel('Date')\nplt.ylabel('Price [EUR/MWh]')\nplt.grid()\n\n\n\n\n\n\n\n\nAdd the static variables in a separate static_df dataframe. In this example, we are using one-hot encoding of the electricity market. The static_df must include one observation (row) for each unique_id of the df dataframe, with the different statics variables as columns.\n\nstatic_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE_static.csv')\nstatic_df.head()\n\n\n\n\n\n\n\n\nunique_id\nmarket_0\nmarket_1\n\n\n\n\n0\nFR\n1\n0\n\n\n1\nBR\n0\n1",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Exogenous Variables"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - exogenous_variables.html#training-with-exogenous-variables",
    "href": "Nixtla/nixtla - exogenous_variables.html#training-with-exogenous-variables",
    "title": "Exogenous Variables",
    "section": "3. Training with exogenous variables",
    "text": "3. Training with exogenous variables\nWe distinguish the exogenous variables by whether they reflect static or time-dependent aspects of the modeled data.\n\nStatic exogenous variables: The static exogenous variables carry time-invariant information for each time series. When the model is built with global parameters to forecast multiple time series, these variables allow sharing information within groups of time series with similar static variable levels. Examples of static variables include designators such as identifiers of regions, groups of products, etc.\nHistoric exogenous variables: This time-dependent exogenous variable is restricted to past observed values. Its predictive power depends on Granger-causality, as its past values can provide significant information about future values of the target variable \\(\\mathbf{y}\\).\nFuture exogenous variables: In contrast with historic exogenous variables, future values are available at the time of the prediction. Examples include calendar variables, weather forecasts, and known events that can cause large spikes and dips such as scheduled promotions.\n\nTo add exogenous variables to the model, first specify the name of each variable from the previous dataframes to the corresponding model hyperparameter during initialization: futr_exog_list, hist_exog_list, and stat_exog_list. We also set horizon as 24 to produce the next day hourly forecasts, and set input_size to use the last 5 days of data as input.\n\nfrom neuralforecast.auto import NHITS\nfrom neuralforecast.core import NeuralForecast\n\nimport logging\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n\n\nhorizon = 24 # day-ahead daily forecast\nmodels = [NHITS(h = horizon,\n                input_size = 5*horizon,\n                max_steps=10,\n                futr_exog_list = ['gen_forecast', 'week_day'], # &lt;- Future exogenous variables\n                hist_exog_list = ['system_load'], # &lt;- Historical exogenous variables\n                stat_exog_list = ['market_0', 'market_1'], # &lt;- Static exogenous variables\n                scaler_type = 'robust')]\n\nGlobal seed set to 1\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen including exogenous variables always use a scaler by setting the scaler_type hyperparameter. The scaler will scale all the temporal features: the target variable y, historic and future variables.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMake sure future and historic variables are correctly placed. Defining historic variables as future variables will lead to data leakage.\n\n\nNext, pass the datasets to the df and static_df inputs of the fit method.\n\nnf = NeuralForecast(models=models, freq='H')\nnf.fit(df=df,\n       static_df=static_df,\n      )",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Exogenous Variables"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - exogenous_variables.html#forecasting-with-exogenous-variables",
    "href": "Nixtla/nixtla - exogenous_variables.html#forecasting-with-exogenous-variables",
    "title": "Exogenous Variables",
    "section": "4. Forecasting with exogenous variables",
    "text": "4. Forecasting with exogenous variables\nBefore predicting the prices, we need to gather the future exogenous variables for the day we want to forecast. Define a new dataframe (futr_df) with the unique_id, ds, and future exogenous variables. There is no need to add the target variable y and historic variables as they won’t be used by the model.\n\nfutr_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE_futr.csv')\nfutr_df['ds'] = pd.to_datetime(futr_df['ds'])\nfutr_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ngen_forecast\nweek_day\n\n\n\n\n0\nFR\n2016-11-01 00:00:00\n49118.0\n1\n\n\n1\nFR\n2016-11-01 01:00:00\n47890.0\n1\n\n\n2\nFR\n2016-11-01 02:00:00\n47158.0\n1\n\n\n3\nFR\n2016-11-01 03:00:00\n45991.0\n1\n\n\n4\nFR\n2016-11-01 04:00:00\n45378.0\n1\n\n\n\n\n\n\n\n\nfutr_df.shape\n\n(48, 4)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMake sure futr_df has informations for the entire forecast horizon. In this example, we are forecasting 24 hours ahead, so futr_df must have 24 rows for each time series.\n\n\nFinally, use the predict method to forecast the day-ahead prices.\n\nY_hat_df = nf.predict(futr_df=futr_df)\nY_hat_df.head()\n\nPredicting DataLoader 0: 100%|██████████| 1/1 [00:00&lt;00:00, 95.56it/s] \n\n\n\n\n\n\n\n\n\nds\nNHITS\n\n\nunique_id\n\n\n\n\n\n\nBE\n2016-11-01 00:00:00\n36.936493\n\n\nBE\n2016-11-01 01:00:00\n33.701057\n\n\nBE\n2016-11-01 02:00:00\n30.956253\n\n\nBE\n2016-11-01 03:00:00\n28.285088\n\n\nBE\n2016-11-01 04:00:00\n27.118006\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplot_df = df[df['unique_id']=='FR'].tail(24*5).reset_index(drop=True)\nY_hat_df = Y_hat_df.reset_index(drop=False)\nY_hat_df = Y_hat_df[Y_hat_df['unique_id']=='FR']\n\nplot_df = pd.concat([plot_df, Y_hat_df ]).set_index('ds') # Concatenate the train and forecast dataframes\n\nplot_df[['y', 'NHITS']].plot(linewidth=2)\nplt.axvline('2016-11-01', color='red')\nplt.ylabel('Price [EUR/MWh]', fontsize=12)\nplt.xlabel('Date', fontsize=12)\nplt.grid()\n\n\n\n\n\n\n\n\nIn summary, to add exogenous variables to a model make sure to follow the next steps:\n\nAdd temporal exogenous variables as columns to the main dataframe (df).\nAdd static exogenous variables with the static_df dataframe.\nSpecify the name for each variable in the corresponding model hyperparameter.\nIf the model uses future exogenous variables, pass the future dataframe (futr_df) to the predict method.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Exogenous Variables"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - exogenous_variables.html#references",
    "href": "Nixtla/nixtla - exogenous_variables.html#references",
    "title": "Exogenous Variables",
    "section": "References",
    "text": "References\n\nKin G. Olivares, Cristian Challu, Grzegorz Marcjasz, Rafał Weron, Artur Dubrawski, Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with NBEATSx, International Journal of Forecasting\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). NHITS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Exogenous Variables"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - all_methods.html",
    "href": "Nixtla/nixtla - all_methods.html",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "",
    "text": "Statistical, Machine Learning, and Neural Forecasting Methods In this tutorial, we will explore the process of forecasting on the M5 dataset by utilizing the most suitable model for each time series. We’ll accomplish this through an essential technique known as cross-validation. This approach helps us in estimating the predictive performance of our models, and in selecting the model that yields the best performance for each time series.\nThe M5 dataset comprises of hierarchical sales data, spanning five years, from Walmart. The aim is to forecast daily sales for the next 28 days. The dataset is broken down into the 50 states of America, with 10 stores in each state.\nIn the realm of time series forecasting and analysis, one of the more complex tasks is identifying the model that is optimally suited for a specific group of series. Quite often, this selection process leans heavily on intuition, which may not necessarily align with the empirical reality of our dataset.\nIn this tutorial, we aim to provide a more structured, data-driven approach to model selection for different groups of series within the M5 benchmark dataset. This dataset, well-known in the field of forecasting, allows us to showcase the versatility and power of our methodology.\nWe will train an assortment of models from various forecasting paradigms:\nStatsForecast\nMLForecast\nMachine Learning: Leveraging ML models like LightGBM, XGBoost, and LinearRegression can be advantageous due to their capacity to uncover intricate patterns in data. We’ll use the MLForecast library for this purpose.\nNeuralForecast\nDeep Learning: DL models, such as Transformers (AutoTFT) and Neural Networks (AutoNHITS), allow us to handle complex non-linear dependencies in time series data. We’ll utilize the NeuralForecast library for these models.\nUsing the Nixtla suite of libraries, we’ll be able to drive our model selection process with data, ensuring we utilize the most suitable models for specific groups of series in our dataset.\nOutline:",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Statistical, Machine Learning and Neural Forecasting methods"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - all_methods.html#installing-libraries",
    "href": "Nixtla/nixtla - all_methods.html#installing-libraries",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Installing Libraries",
    "text": "Installing Libraries\n!pip install statsforecast mlforecast neuralforecast datasetforecast s3fs pyarrow",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Statistical, Machine Learning and Neural Forecasting methods"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - all_methods.html#download-and-prepare-data",
    "href": "Nixtla/nixtla - all_methods.html#download-and-prepare-data",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Download and prepare data",
    "text": "Download and prepare data\nThe example uses the M5 dataset. It consists of 30,490 bottom time series.\n\nimport pandas as pd\n\n\n# Load the training target dataset from the provided URL\nY_df = pd.read_parquet('../Data/target.parquet')\n#Y_df = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet')\n\n# Rename columns to match the Nixtlaverse's expectations\n# The 'item_id' becomes 'unique_id' representing the unique identifier of the time series\n# The 'timestamp' becomes 'ds' representing the time stamp of the data points\n# The 'demand' becomes 'y' representing the target variable we want to forecast\nY_df = Y_df.rename(columns={\n    'item_id': 'unique_id', \n    'timestamp': 'ds', \n    'demand': 'y'\n})\n\n# Convert the 'ds' column to datetime format to ensure proper handling of date-related operations in subsequent steps\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n\nY_df\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nFOODS_1_001_CA_1\n2011-01-29\n3.0\n\n\n1\nFOODS_1_001_CA_1\n2011-01-30\n0.0\n\n\n2\nFOODS_1_001_CA_1\n2011-01-31\n0.0\n\n\n3\nFOODS_1_001_CA_1\n2011-02-01\n1.0\n\n\n4\nFOODS_1_001_CA_1\n2011-02-02\n4.0\n\n\n...\n...\n...\n...\n\n\n46796215\nHOUSEHOLD_2_516_WI_3\n2016-05-18\n0.0\n\n\n46796216\nHOUSEHOLD_2_516_WI_3\n2016-05-19\n0.0\n\n\n46796217\nHOUSEHOLD_2_516_WI_3\n2016-05-20\n0.0\n\n\n46796218\nHOUSEHOLD_2_516_WI_3\n2016-05-21\n0.0\n\n\n46796219\nHOUSEHOLD_2_516_WI_3\n2016-05-22\n0.0\n\n\n\n\n46796220 rows × 3 columns\n\n\n\nFor simplicity sake we will keep just one category\n\nY_df = Y_df.query('unique_id.str.startswith(\"FOODS_3_001\")').reset_index(drop=True)\n\nY_df['unique_id'] = Y_df['unique_id'].astype(str)\n\n\nY_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 18758 entries, 0 to 18757\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   unique_id  18758 non-null  object        \n 1   ds         18758 non-null  datetime64[ns]\n 2   y          18758 non-null  float32       \ndtypes: datetime64[ns](1), float32(1), object(1)\nmemory usage: 366.5+ KB\n\n\n\ndf = pd.DataFrame()\ntest = pd.DataFrame()\nhorizon = 28\n\nfor a in Y_df.unique_id.unique():\n    length = len(Y_df.query(f'unique_id.str.startswith(\"{a}\")'))\n    df_sub = Y_df.query(f'unique_id.str.startswith(\"{a}\")')[:length - horizon]\n    test_sub = Y_df.query(f'unique_id.str.startswith(\"{a}\")')[length - 2 * horizon:]\n    df = pd.concat([df, df_sub], ignore_index=True)\n    test = pd.concat([test, test_sub], ignore_index=True)\n    \n    newlength = len(df.query(f'unique_id.str.startswith(\"{a}\")'))\n    print(f'{a} : {length} and {newlength}')\n\nFOODS_3_001_CA_1 : 1941 and 1913\nFOODS_3_001_CA_2 : 1936 and 1908\nFOODS_3_001_CA_3 : 1941 and 1913\nFOODS_3_001_CA_4 : 1936 and 1908\nFOODS_3_001_TX_1 : 1940 and 1912\nFOODS_3_001_TX_2 : 1938 and 1910\nFOODS_3_001_TX_3 : 1940 and 1912\nFOODS_3_001_WI_1 : 1305 and 1277\nFOODS_3_001_WI_2 : 1940 and 1912\nFOODS_3_001_WI_3 : 1941 and 1913",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Statistical, Machine Learning and Neural Forecasting methods"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - all_methods.html#statsforecast",
    "href": "Nixtla/nixtla - all_methods.html#statsforecast",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "StatsForecast",
    "text": "StatsForecast\nStatsForecast is a comprehensive library providing a suite of popular univariate time series forecasting models, all designed with a focus on high performance and scalability.\nHere’s what makes StatsForecast a powerful tool for time series forecasting:\n\nCollection of Local Models: StatsForecast provides a diverse collection of local models that can be applied to each time series individually, allowing us to capture unique patterns within each series.\nSimplicity: With StatsForecast, training, forecasting, and backtesting multiple models become a straightforward process, requiring only a few lines of code. This simplicity makes it a convenient tool for both beginners and experienced practitioners.\nOptimized for Speed: The implementation of the models in StatsForecast is optimized for speed, ensuring that large-scale computations are performed efficiently, thereby reducing the overall time for model training and prediction.\nHorizontal Scalability: One of the distinguishing features of StatsForecast is its ability to scale horizontally. It is compatible with distributed computing frameworks such as Spark, Dask, and Ray. This feature allows it to handle large datasets by distributing the computations across multiple nodes in a cluster, making it a go-to solution for large-scale time series forecasting tasks.\n\nStatsForecast receives a list of models to fit each time series. Since we are dealing with Daily data, it would be benefitial to use 7 as seasonality.\n\n# Import necessary models from the statsforecast library\nfrom statsforecast.models import (\n    # SeasonalNaive: A model that uses the previous season's data as the forecast\n    SeasonalNaive,\n    # Naive: A simple model that uses the last observed value as the forecast\n    Naive,\n    # HistoricAverage: This model uses the average of all historical data as the forecast\n    HistoricAverage,\n    # CrostonOptimized: A model specifically designed for intermittent demand forecasting\n    CrostonOptimized,\n    # ADIDA: Adaptive combination of Intermittent Demand Approaches, a model designed for intermittent demand\n    ADIDA,\n    # IMAPA: Intermittent Multiplicative AutoRegressive Average, a model for intermittent series that incorporates autocorrelation\n    IMAPA,\n    # AutoETS: Automated Exponential Smoothing model that automatically selects the best Exponential Smoothing model based on AIC\n    AutoETS\n)\nfrom statsforecast.utils import ConformalIntervals\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/core.py:25: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\nn_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails. Any settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n\nintervals = ConformalIntervals(h=horizon, n_windows=2)\nmodels = [\n    SeasonalNaive(season_length=7),\n    Naive(),\n    HistoricAverage(),\n    CrostonOptimized(),\n    ADIDA(),\n    IMAPA(),\n    AutoETS(season_length=7)\n]\n\n\n# Instantiate the StatsForecast class\nsf = StatsForecast(\n    models=models,  # A list of models to be used for forecasting\n    freq='D',  # The frequency of the time series data (in this case, 'D' stands for daily frequency)\n    n_jobs=10,  # The number of CPU cores to use for parallel execution (-1 means use all available cores)\n    verbose = True\n)\n\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis block of code times how long it takes to run the forecasting function of the StatsForecast class, which predicts the next 28 days (h=28). The level is set to [90], meaning it will compute the 90% prediction interval. The time is calculated in minutes and printed out at the end.\n\nfrom time import time\n\n# Get the current time before forecasting starts, this will be used to measure the execution time\ninit = time()\n\n# Call the forecast method of the StatsForecast instance to predict the next 28 days (h=28) \n# Level is set to [90], which means that it will compute the 90% prediction interval\nfcst_df = sf.forecast(df=df, \n                      h=horizon,\n                      level=[90],\n                      prediction_intervals=intervals\n                     )\n\n# Get the current time after the forecasting ends\nend = time()\n\n# Calculate and print the total time taken for the forecasting in minutes\nprint(f'Forecast Minutes: {(end - init) / 60}')\n#Forecast Minutes: 0.7311509331067403\n\nForecast Minutes: 0.8084765871365865\n\n\n\nfcst_df.head()\n\n\n\n\n\n\n\n\nds\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\nNaive\nNaive-lo-90\nNaive-hi-90\nHistoricAverage\nHistoricAverage-lo-90\nHistoricAverage-hi-90\n...\nCrostonOptimized-hi-90\nADIDA\nADIDA-lo-90\nADIDA-hi-90\nIMAPA\nIMAPA-lo-90\nIMAPA-hi-90\nAutoETS\nAutoETS-lo-90\nAutoETS-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFOODS_3_001_CA_1\n2016-04-25\n2.0\n0.00\n4.00\n1.0\n0.00\n2.00\n0.452692\n0.000000\n0.905384\n...\n1.107222\n0.56904\n0.000000\n1.138079\n0.555747\n0.000000\n1.111494\n0.611209\n-1.015629e-08\n1.222419\n\n\nFOODS_3_001_CA_1\n2016-04-26\n0.0\n-0.85\n0.85\n1.0\n0.15\n1.85\n0.452692\n-0.080423\n0.985808\n...\n1.091138\n0.56904\n0.020712\n1.117367\n0.555747\n0.016724\n1.094770\n0.519098\n5.729341e-03\n1.032466\n\n\nFOODS_3_001_CA_1\n2016-04-27\n0.0\n-0.85\n0.85\n1.0\n0.15\n1.85\n0.452692\n-0.080423\n0.985808\n...\n1.091138\n0.56904\n0.020712\n1.117367\n0.555747\n0.016724\n1.094770\n0.567324\n2.019713e-02\n1.114451\n\n\nFOODS_3_001_CA_1\n2016-04-28\n1.0\n0.15\n1.85\n1.0\n0.15\n1.85\n0.452692\n-0.080423\n0.985808\n...\n1.091138\n0.56904\n0.020712\n1.117367\n0.555747\n0.016724\n1.094770\n0.410421\n-1.522845e-01\n0.973126\n\n\nFOODS_3_001_CA_1\n2016-04-29\n0.0\n0.00\n0.00\n1.0\n0.00\n2.00\n0.452692\n0.000000\n0.905384\n...\n1.107222\n0.56904\n0.000000\n1.138079\n0.555747\n0.000000\n1.111494\n0.540213\n-3.345982e-09\n1.080427\n\n\n\n\n5 rows × 22 columns\n\n\n\n\nStatsForecast.plot(test, fcst_df, engine = 'plotly')",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Statistical, Machine Learning and Neural Forecasting methods"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - all_methods.html#mlforecast",
    "href": "Nixtla/nixtla - all_methods.html#mlforecast",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "MLForecast",
    "text": "MLForecast\nMLForecast is a powerful library that provides automated feature creation for time series forecasting, facilitating the use of global machine learning models. It is designed for high performance and scalability.\nKey features of MLForecast include:\n\nSupport for sklearn models: MLForecast is compatible with models that follow the scikit-learn API. This makes it highly flexible and allows it to seamlessly integrate with a wide variety of machine learning algorithms.\nSimplicity: With MLForecast, the tasks of training, forecasting, and backtesting models can be accomplished in just a few lines of code. This streamlined simplicity makes it user-friendly for practitioners at all levels of expertise.\nOptimized for speed: MLForecast is engineered to execute tasks rapidly, which is crucial when handling large datasets and complex models.\nHorizontal Scalability: MLForecast is capable of horizontal scaling using distributed computing frameworks such as Spark, Dask, and Ray. This feature enables it to efficiently process massive datasets by distributing the computations across multiple nodes in a cluster, making it ideal for large-scale time series forecasting tasks.\n\n\nfrom mlforecast import MLForecast\nfrom mlforecast.target_transforms import Differences\nfrom mlforecast.utils import PredictionIntervals\nfrom window_ops.expanding import expanding_mean\n\n!pip install lightgbm xgboost\n\n# Import the necessary models from various libraries\n\n# LGBMRegressor: A gradient boosting framework that uses tree-based learning algorithms from the LightGBM library\nfrom lightgbm import LGBMRegressor\n\n# XGBRegressor: A gradient boosting regressor model from the XGBoost library\nfrom xgboost import XGBRegressor\n\n# LinearRegression: A simple linear regression model from the scikit-learn library\nfrom sklearn.linear_model import LinearRegression\n\n\n# Instantiate the MLForecast object\nmlf = MLForecast(\n    models=[LGBMRegressor(), XGBRegressor(), LinearRegression()],  # List of models for forecasting: LightGBM, XGBoost and Linear Regression\n    freq='D',  # Frequency of the data - 'D' for daily frequency\n    lags=list(range(1, 7)),  # Specific lags to use as regressors: 1 to 6 days\n    lag_transforms = {\n        1:  [expanding_mean],  # Apply expanding mean transformation to the lag of 1 day\n    },\n    date_features=['year', 'month', 'day', 'dayofweek', 'quarter', 'week'],  # Date features to use as regressors\n)\n\nJust call the fit models to train the select models. In this case we are generating conformal prediction intervals.\n\n# Start the timer to calculate the time taken for fitting the models\ninit = time()\n\n# Fit the MLForecast models to the data, with prediction intervals set using a window size of 28 days\nmlf.fit(df, prediction_intervals=PredictionIntervals(h=horizon, n_windows=2))\n\n# Calculate the end time after fitting the models\nend = time()\n\n# Print the time taken to fit the MLForecast models, in minutes\nprint(f'MLForecast Minutes: {(end - init) / 60}')\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001585 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 469\n[LightGBM] [Info] Number of data points in the train set: 17858, number of used features: 13\n[LightGBM] [Info] Start training from score 0.526375\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000635 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 469\n[LightGBM] [Info] Number of data points in the train set: 18418, number of used features: 13\n[LightGBM] [Info] Start training from score 0.531328\nMLForecast Minutes: 0.06777082284291586\n\n\nAfter that, just call predict to generate forecasts.\n\nfcst_mlf_df = mlf.predict(horizon,\n                          #level=[90]\n                         )\n\n\nfcst_mlf_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nLGBMRegressor\nXGBRegressor\nLinearRegression\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-04-25\n0.810128\n0.099661\n0.122907\n\n\n1\nFOODS_3_001_CA_1\n2016-04-26\n0.881765\n0.062783\n0.140931\n\n\n2\nFOODS_3_001_CA_1\n2016-04-27\n0.462391\n-0.064804\n0.167513\n\n\n3\nFOODS_3_001_CA_1\n2016-04-28\n0.453431\n-0.084136\n0.199248\n\n\n4\nFOODS_3_001_CA_1\n2016-04-29\n0.376807\n0.018470\n0.223721\n\n\n\n\n\n\n\n\nStatsForecast.plot(test, fcst_mlf_df, engine = 'plotly')",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Statistical, Machine Learning and Neural Forecasting methods"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - all_methods.html#neuralforecast",
    "href": "Nixtla/nixtla - all_methods.html#neuralforecast",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "NeuralForecast",
    "text": "NeuralForecast\nNeuralForecast is a robust collection of neural forecasting models that focuses on usability and performance. It includes a variety of model architectures, from classic networks such as Multilayer Perceptrons (MLP) and Recurrent Neural Networks (RNN) to novel contributions like N-BEATS, N-HITS, Temporal Fusion Transformers (TFT), and more.\nKey features of NeuralForecast include:\n\nA broad collection of global models. Out of the box implementation of MLP, LSTM, RNN, TCN, DilatedRNN, NBEATS, NHITS, ESRNN, TFT, Informer, PatchTST and HINT.\nA simple and intuitive interface that allows training, forecasting, and backtesting of various models in a few lines of code.\nSupport for GPU acceleration to improve computational speed.\n\nThis machine doesn’t have GPU, but Google Colabs offers some for free.\nUsing Colab’s GPU to train NeuralForecast.\n\n# Read the results from Colab\nfcst_nf_df = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/forecast-nf.parquet')\n#Y_df = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet')\n\n\nfcst_nf_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nAutoNHITS\nAutoNHITS-lo-90\nAutoNHITS-hi-90\nAutoTFT\nAutoTFT-lo-90\nAutoTFT-hi-90\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-05-23\n0.0\n0.0\n2.0\n0.0\n0.0\n2.0\n\n\n1\nFOODS_3_001_CA_1\n2016-05-24\n0.0\n0.0\n2.0\n0.0\n0.0\n2.0\n\n\n2\nFOODS_3_001_CA_1\n2016-05-25\n0.0\n0.0\n2.0\n0.0\n0.0\n1.0\n\n\n3\nFOODS_3_001_CA_1\n2016-05-26\n0.0\n0.0\n2.0\n0.0\n0.0\n2.0\n\n\n4\nFOODS_3_001_CA_1\n2016-05-27\n0.0\n0.0\n2.0\n0.0\n0.0\n2.0\n\n\n\n\n\n\n\n\n# Merge the forecasts from StatsForecast and NeuralForecast\nfcst_df = fcst_df.merge(fcst_nf_df, how='left', on=['unique_id', 'ds'])\n\n# Merge the forecasts from MLForecast into the combined forecast dataframe\nfcst_df = fcst_df.merge(fcst_mlf_df, how='left', on=['unique_id', 'ds'])\n\n\nfcst_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\nNaive\nNaive-lo-90\nNaive-hi-90\nHistoricAverage\nHistoricAverage-lo-90\n...\nAutoETS-hi-90\nAutoNHITS\nAutoNHITS-lo-90\nAutoNHITS-hi-90\nAutoTFT\nAutoTFT-lo-90\nAutoTFT-hi-90\nLGBMRegressor\nXGBRegressor\nLinearRegression\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-04-25\n2.0\n0.00\n4.00\n1.0\n0.00\n2.00\n0.452692\n0.000000\n...\n1.222419\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.810128\n0.099661\n0.122907\n\n\n1\nFOODS_3_001_CA_1\n2016-04-26\n0.0\n-0.85\n0.85\n1.0\n0.15\n1.85\n0.452692\n-0.080423\n...\n1.032466\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.881765\n0.062783\n0.140931\n\n\n2\nFOODS_3_001_CA_1\n2016-04-27\n0.0\n-0.85\n0.85\n1.0\n0.15\n1.85\n0.452692\n-0.080423\n...\n1.114451\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.462391\n-0.064804\n0.167513\n\n\n3\nFOODS_3_001_CA_1\n2016-04-28\n1.0\n0.15\n1.85\n1.0\n0.15\n1.85\n0.452692\n-0.080423\n...\n0.973126\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.453431\n-0.084136\n0.199248\n\n\n4\nFOODS_3_001_CA_1\n2016-04-29\n0.0\n0.00\n0.00\n1.0\n0.00\n2.00\n0.452692\n0.000000\n...\n1.080427\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.376807\n0.018470\n0.223721\n\n\n\n\n5 rows × 32 columns",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Statistical, Machine Learning and Neural Forecasting methods"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - all_methods.html#forecast-plots",
    "href": "Nixtla/nixtla - all_methods.html#forecast-plots",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Forecast plots",
    "text": "Forecast plots\n\nsf.plot(Y_df, fcst_df, max_insample_length=28 * 3)\n\n\n\n\n\n\n\n\nUse the plot function to explore models and ID’s\n\nsf.plot(Y_df, fcst_df, max_insample_length=28 * 3, \n        models=['CrostonOptimized', 'AutoNHITS', 'SeasonalNaive', 'LGBMRegressor'])",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Statistical, Machine Learning and Neural Forecasting methods"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - all_methods.html#cross-validation-in-statsforecast",
    "href": "Nixtla/nixtla - all_methods.html#cross-validation-in-statsforecast",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Cross Validation in StatsForecast",
    "text": "Cross Validation in StatsForecast\nThe cross_validation method from the StatsForecast class accepts the following arguments:\n\ndf: A DataFrame representing the training data.\nh (int): The forecast horizon, represented as the number of steps into the future that we wish to predict. For example, if we’re forecasting hourly data, h=24 would represent a 24-hour forecast.\nstep_size (int): The step size between each cross-validation window. This parameter determines how often we want to run the forecasting process.\nn_windows (int): The number of windows used for cross validation. This parameter defines how many past forecasting processes we want to evaluate.\n\nThese parameters allow us to control the extent and granularity of our cross-validation process. By tuning these settings, we can balance between computational cost and the thoroughness of the cross-validation.\n\ninit = time()\ncv_df = sf.cross_validation(df=Y_df, h=horizon, n_windows=3, step_size=horizon, level=[90])\nend = time()\nprint(f'CV Minutes: {(end - init) / 60}')\n\nCV Minutes: 1.091424548625946\n\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncv_df.head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\nNaive\nNaive-lo-90\nNaive-hi-90\nHistoricAverage\n...\nCrostonOptimized-hi-90\nADIDA\nADIDA-lo-90\nADIDA-hi-90\nIMAPA\nIMAPA-lo-90\nIMAPA-hi-90\nAutoETS\nAutoETS-lo-90\nAutoETS-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFOODS_3_001_CA_1\n2016-02-29\n2016-02-28\n0.0\n2.0\n0.15\n3.85\n0.0\n-0.85\n0.85\n0.449111\n...\n1.201402\n0.618375\n0.035513\n1.201238\n0.617998\n0.035399\n1.200596\n0.655286\n4.658565e-02\n1.263985\n\n\nFOODS_3_001_CA_1\n2016-03-01\n2016-02-28\n1.0\n0.0\n-1.70\n1.70\n0.0\n-1.70\n1.70\n0.449111\n...\n1.885541\n0.618375\n-0.648762\n1.885513\n0.617998\n-0.649404\n1.885399\n0.568595\n-7.333890e-01\n1.870578\n\n\nFOODS_3_001_CA_1\n2016-03-02\n2016-02-28\n1.0\n0.0\n-1.85\n1.85\n0.0\n-1.85\n1.85\n0.449111\n...\n1.850000\n0.618375\n-0.613249\n1.850000\n0.617998\n-0.614005\n1.850000\n0.618805\n-6.123903e-01\n1.850000\n\n\nFOODS_3_001_CA_1\n2016-03-03\n2016-02-28\n0.0\n1.0\n0.00\n2.00\n0.0\n0.00\n0.00\n0.449111\n...\n1.236943\n0.618375\n0.000000\n1.236751\n0.617998\n0.000000\n1.235995\n0.455891\n4.113969e-09\n0.911781\n\n\nFOODS_3_001_CA_1\n2016-03-04\n2016-02-28\n0.0\n1.0\n0.00\n2.00\n0.0\n-1.70\n1.70\n0.449111\n...\n1.885541\n0.618375\n-0.648762\n1.885513\n0.617998\n-0.649404\n1.885399\n0.591197\n-6.949658e-01\n1.877359\n\n\n\n\n5 rows × 24 columns",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Statistical, Machine Learning and Neural Forecasting methods"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - all_methods.html#mlforecast-1",
    "href": "Nixtla/nixtla - all_methods.html#mlforecast-1",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "MLForecast",
    "text": "MLForecast\nThe cross_validation method from the MLForecast class takes the following arguments.\n\ndata: training data frame\nwindow_size (int): represents h steps into the future that are being forecasted. In this case, 24 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows (int): number of windows used for cross-validation. In other words: what number of forecasting processes in the past do you want to evaluate.\nprediction_intervals: class to compute conformal intervals.\n\n\ninit = time()\ncv_mlf_df = mlf.cross_validation(\n    df=Y_df, \n    h=horizon, \n    n_windows=3, \n    step_size=horizon, \n    level=[90],\n)\nend = time()\nprint(f'CV Minutes: {(end - init) / 60}')\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000634 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 469\n[LightGBM] [Info] Number of data points in the train set: 17858, number of used features: 13\n[LightGBM] [Info] Start training from score 0.526375\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000567 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 469\n[LightGBM] [Info] Number of data points in the train set: 18138, number of used features: 13\n[LightGBM] [Info] Start training from score 0.530047\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000540 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 469\n[LightGBM] [Info] Number of data points in the train set: 18418, number of used features: 13\n[LightGBM] [Info] Start training from score 0.531328\nCV Minutes: 0.02442166805267334\n\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncv_mlf_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\ny\nLGBMRegressor\nXGBRegressor\nLinearRegression\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-02-29\n2016-02-28\n0.0\n0.476516\n0.073750\n0.103426\n\n\n1\nFOODS_3_001_CA_1\n2016-03-01\n2016-02-28\n1.0\n0.739830\n0.035399\n0.260099\n\n\n2\nFOODS_3_001_CA_1\n2016-03-02\n2016-02-28\n1.0\n0.586140\n0.007563\n0.296629\n\n\n3\nFOODS_3_001_CA_1\n2016-03-03\n2016-02-28\n0.0\n0.536099\n-0.004784\n0.328724\n\n\n4\nFOODS_3_001_CA_1\n2016-03-04\n2016-02-28\n0.0\n0.601544\n-0.025531\n0.371325",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Statistical, Machine Learning and Neural Forecasting methods"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - all_methods.html#neuralforecast-1",
    "href": "Nixtla/nixtla - all_methods.html#neuralforecast-1",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "NeuralForecast",
    "text": "NeuralForecast\nThis machine doesn’t have GPU, but Google Colabs offers some for free.\nUsing Colab’s GPU to train NeuralForecast.\n\ncv_nf_df = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/cross-validation-nf.parquet')\n\n\ncv_nf_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\nAutoNHITS\nAutoNHITS-lo-90\nAutoNHITS-hi-90\nAutoTFT\nAutoTFT-lo-90\nAutoTFT-hi-90\ny\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-02-29\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n0.0\n\n\n1\nFOODS_3_001_CA_1\n2016-03-01\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n1.0\n\n\n2\nFOODS_3_001_CA_1\n2016-03-02\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n1.0\n\n\n3\nFOODS_3_001_CA_1\n2016-03-03\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n0.0\n\n\n4\nFOODS_3_001_CA_1\n2016-03-04\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n0.0",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Statistical, Machine Learning and Neural Forecasting methods"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - all_methods.html#merge-cross-validation-forecasts",
    "href": "Nixtla/nixtla - all_methods.html#merge-cross-validation-forecasts",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Merge cross validation forecasts",
    "text": "Merge cross validation forecasts\n\ncv_df = cv_df.merge(cv_nf_df.drop(columns=['y']), how='left', on=['unique_id', 'ds', 'cutoff'])\ncv_df = cv_df.merge(cv_mlf_df.drop(columns=['y']), how='left', on=['unique_id', 'ds', 'cutoff'])",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Statistical, Machine Learning and Neural Forecasting methods"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - all_methods.html#plots-cv",
    "href": "Nixtla/nixtla - all_methods.html#plots-cv",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Plots CV",
    "text": "Plots CV\n\ncutoffs = cv_df['cutoff'].unique()\n\n\nfor cutoff in cutoffs:\n    img = sf.plot(\n        Y_df, \n        cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']), \n        max_insample_length=28 * 5, \n        unique_ids=['FOODS_3_001_CA_1'],\n    )\n    img.show()\n\n\nAggregate Demand\n\nagg_cv_df = cv_df.loc[:,~cv_df.columns.str.contains('hi|lo')].groupby(['ds', 'cutoff']).sum(numeric_only=True).reset_index()\nagg_cv_df.insert(0, 'unique_id', 'agg_demand')\n\n\nagg_Y_df = Y_df.groupby(['ds']).sum(numeric_only=True).reset_index()\nagg_Y_df.insert(0, 'unique_id', 'agg_demand')\n\n\nfor cutoff in cutoffs:\n    img = sf.plot(\n        agg_Y_df, \n        agg_cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']),\n        max_insample_length=28 * 5,\n    )\n    img.show()",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Statistical, Machine Learning and Neural Forecasting methods"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - all_methods.html#evaluation-per-series-and-cv-window",
    "href": "Nixtla/nixtla - all_methods.html#evaluation-per-series-and-cv-window",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Evaluation per series and CV window",
    "text": "Evaluation per series and CV window\nIn this section, we will evaluate the performance of each model for each time series and each cross validation window. Since we have many combinations, we will use dask to parallelize the evaluation. The parallelization will be done using fugue.\n\nfrom typing import List, Callable\n\nfrom distributed import Client\nfrom fugue import transform\nfrom fugue_dask import DaskExecutionEngine\nfrom datasetsforecast.losses import mse, mae, smape\n\nThe evaluate function receives a unique combination of a time series and a window, and calculates different metrics for each model in df.\n\ndef evaluate(df: pd.DataFrame, metrics: List[Callable]) -&gt; pd.DataFrame:\n    eval_ = {}\n    models = df.loc[:, ~df.columns.str.contains('unique_id|y|ds|cutoff|lo|hi')].columns\n    for model in models:\n        eval_[model] = {}\n        for metric in metrics:\n            eval_[model][metric.__name__] = metric(df['y'], df[model])\n    eval_df = pd.DataFrame(eval_).rename_axis('metric').reset_index()\n    eval_df.insert(0, 'cutoff', df['cutoff'].iloc[0])\n    eval_df.insert(0, 'unique_id', df['unique_id'].iloc[0])\n    return eval_df\n\n\nstr_models = cv_df.loc[:, ~cv_df.columns.str.contains('unique_id|y|ds|cutoff|lo|hi')].columns\nstr_models = ','.join([f\"{model}:float\" for model in str_models])\ncv_df['cutoff'] = cv_df['cutoff'].astype(str)\ncv_df['unique_id'] = cv_df['unique_id'].astype(str)\n\nLet’s cleate a dask client.\n\nclient = Client() # without this, dask is not in distributed mode\n# fugue.dask.dataframe.default.partitions determines the default partitions for a new DaskDataFrame\nengine = DaskExecutionEngine({\"fugue.dask.dataframe.default.partitions\": 96})\n\nThe transform function takes the evaluate functions and applies it to each combination of time series (unique_id) and cross validation window (cutoff) using the dask client we created before.\n\nevaluation_df = transform(\n    cv_df.loc[:, ~cv_df.columns.str.contains('lo|hi')], \n    evaluate, \n    engine=\"dask\",\n    params={'metrics': [mse, mae, smape]}, \n    schema=f\"unique_id:str,cutoff:str,metric:str, {str_models}\", \n    as_local=True,\n    partition={'by': ['unique_id', 'cutoff']}\n)\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 12.0.1 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow&gt;=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 12.0.1 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow&gt;=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 12.0.1 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow&gt;=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 12.0.1 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow&gt;=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n\n\n\nevaluation_df.head()\n\n\n\n\n\n\n\n\nunique_id\ncutoff\nmetric\nSeasonalNaive\nNaive\nHistoricAverage\nCrostonOptimized\nADIDA\nIMAPA\nAutoETS\nAutoNHITS\nAutoTFT\nLGBMRegressor\nXGBRegressor\nLinearRegression\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-03-27\nmse\n2.0\n0.857143\n0.609448\n0.649203\n0.641414\n0.674665\n0.615575\n0.857143\n0.857143\n0.611648\n0.663199\n0.598533\n\n\n1\nFOODS_3_001_CA_1\n2016-03-27\nmae\n1.0\n0.785714\n0.62914\n0.701453\n0.69575\n0.7171\n0.682166\n0.785714\n0.785714\n0.674352\n0.517375\n0.59254\n\n\n2\nFOODS_3_001_CA_1\n2016-03-27\nsmape\n120.238098\n136.90477\n161.733765\n148.482315\n149.396652\n146.06955\n147.722168\n136.90477\n136.90477\n143.217346\n179.527222\n166.670731\n\n\n3\nFOODS_3_001_CA_2\n2016-02-28\nmse\n2.571429\n2.357143\n2.358604\n2.285869\n2.280574\n2.286965\n2.281743\n2.678571\n2.785714\n2.396476\n2.250315\n2.479802\n\n\n4\nFOODS_3_001_CA_2\n2016-02-28\nmae\n0.642857\n1.142857\n0.896868\n0.976789\n0.98991\n0.97454\n0.986573\n0.821429\n0.714286\n1.093605\n0.905789\n0.883407\n\n\n\n\n\n\n\n\n# Calculate the mean metric for each cross validation window\nevaluation_df.groupby(['cutoff', 'metric']).mean(numeric_only=True)\n\n\n\n\n\n\n\n\n\nSeasonalNaive\nNaive\nHistoricAverage\nCrostonOptimized\nADIDA\nIMAPA\nAutoETS\nAutoNHITS\nAutoTFT\nLGBMRegressor\nXGBRegressor\nLinearRegression\n\n\ncutoff\nmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-02-28\nmae\n0.835714\n0.721429\n0.807745\n0.797376\n0.756887\n0.757553\n0.76293\n0.678571\n0.685714\n0.840334\n0.787216\n0.798487\n\n\nmse\n1.985714\n1.4\n1.568773\n1.381082\n1.363623\n1.387873\n1.391179\n1.428571\n1.55\n1.312784\n1.631657\n1.666233\n\n\nsmape\n85.984131\n83.268143\n159.98056\n150.941254\n150.603729\n151.213348\n152.517731\n75.515877\n74.916672\n144.99794\n166.576538\n166.23584\n\n\n2016-03-27\nmae\n0.942857\n0.685714\n0.722005\n0.80301\n0.731083\n0.726217\n0.69309\n0.728571\n0.730357\n0.871179\n0.707606\n0.699428\n\n\nmse\n2.564286\n1.378571\n1.202432\n1.193963\n1.13829\n1.130871\n1.077629\n1.385714\n1.375893\n1.314976\n1.135834\n1.196972\n\n\nsmape\n84.425171\n89.159866\n163.598099\n156.133636\n156.127029\n155.551025\n155.68808\n82.819733\n89.659866\n154.666199\n165.329605\n165.570541\n\n\n2016-04-24\nmae\n0.835714\n0.678571\n0.765896\n0.756284\n0.717147\n0.719683\n0.725428\n0.591071\n0.617857\n0.847164\n0.687518\n0.751087\n\n\nmse\n2.6\n1.835714\n1.852598\n1.657853\n1.646621\n1.65177\n1.647888\n1.76875\n1.658929\n1.847793\n2.000684\n1.938778\n\n\nsmape\n77.323128\n80.833336\n161.604263\n153.30835\n153.582413\n154.115997\n155.562042\n62.976189\n70.452377\n150.188019\n176.644547\n166.390579\n\n\n\n\n\n\n\nResults showed in previous experiments.\n\n\n\nmodel\nMSE\n\n\n\n\nMQCNN\n10.09\n\n\nDeepAR-student_t\n10.11\n\n\nDeepAR-lognormal\n30.20\n\n\nDeepAR\n9.13\n\n\nNPTS\n11.53\n\n\n\nTop 3 models: DeepAR, AutoNHITS, AutoETS.\n\nDistribution of errors\n\n!pip install seaborn\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nevaluation_df_melted = pd.melt(evaluation_df, id_vars=['unique_id', 'cutoff', 'metric'], var_name='model', value_name='error')\n\n\nSMAPE\n\nsns.violinplot(evaluation_df_melted.query('metric==\"smape\"'), x='error', y='model')\n\n\n\n\n\n\n\n\n\n\n\nChoose models for groups of series\nFeature:\n\nA unified dataframe with forecasts for all different models\nEasy Ensamble\nE.g. Average predictions\nOr MinMax (Choosing is ensembling)\n\n\n# Choose the best model for each time series, metric, and cross validation window\nevaluation_df['best_model'] = evaluation_df.idxmin(axis=1, numeric_only=True)\n# count how many times a model wins per metric and cross validation window\ncount_best_model = evaluation_df.groupby(['cutoff', 'metric', 'best_model']).size().rename('n').to_frame().reset_index()\n# plot results\nsns.barplot(count_best_model, x='n', y='best_model', hue='metric')\n\n\n\n\n\n\n\n\n\n\nEt pluribus unum: an inclusive forecasting Pie.\n\n# For the mse, calculate how many times a model wins\neval_series_df = evaluation_df.query('metric == \"mse\"').groupby(['unique_id']).mean(numeric_only=True)\neval_series_df['best_model'] = eval_series_df.idxmin(axis=1)\ncounts_series = eval_series_df.value_counts('best_model')\nplt.pie(counts_series, labels=counts_series.index, autopct='%.0f%%')\nplt.show()\n\n\n\n\n\n\n\n\n\nsf.plot(Y_df, cv_df.drop(columns=['cutoff', 'y']), \n        max_insample_length=28 * 6, \n        models=['AutoNHITS'],\n        unique_ids=eval_series_df.query('best_model == \"AutoNHITS\"').index[:8])",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Statistical, Machine Learning and Neural Forecasting methods"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - hierarchicalforecast.html",
    "href": "Nixtla/nixtla - hierarchicalforecast.html",
    "title": "Nixtla - Hierarchical Forecast",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Hierarchical Forecast"
    ]
  },
  {
    "objectID": "Nixtla/electricitypeakforecasting.html",
    "href": "Nixtla/electricitypeakforecasting.html",
    "title": "Nixtla - Detect Demand Peaks",
    "section": "",
    "text": "Predicting peaks in different markets is useful. In the electricity market, consuming electricity at peak demand is penalized with higher tarifs. When an individual or company consumes electricity when its most demanded, regulators calls that a coincident peak (CP).\nIn the Texas electricity market (ERCOT), the peak is the monthly 15-minute interval when the ERCOT Grid is at a point of highest capacity. The peak is caused by all consumers’ combined demand on the electrical grid. The coincident peak demand is an important factor used by ERCOT to determine final electricity consumption bills. ERCOT registers the CP demand of each client for 4 months, between June and September, and uses this to adjust electricity prices. Clients can therefore save on electricity bills by reducing the coincident peak demand.\nIn this example we will train an MSTL (Multiple Seasonal-Trend decomposition using LOESS) model on historic load data to forecast day-ahead peaks on September 2022. Multiple seasonality is traditionally present in low sampled electricity data. Demand exhibits daily and weekly seasonality, with clear patterns for specific hours of the day such as 6:00pm vs 3:00am or for specific days such as Sunday vs Friday.\nFirst, we will load ERCOT historic demand, then we will use the StatsForecast.cross_validation method to fit the MSTL model and forecast daily load during September. Finally, we show how to use the forecasts to detect the coincident peak.\nOutline\n\nInstall libraries\nLoad and explore the data\nFit MSTL model and forecast\nPeak detection",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Detect Demand Peaks"
    ]
  },
  {
    "objectID": "Nixtla/electricitypeakforecasting.html#introduction",
    "href": "Nixtla/electricitypeakforecasting.html#introduction",
    "title": "Nixtla - Detect Demand Peaks",
    "section": "",
    "text": "Predicting peaks in different markets is useful. In the electricity market, consuming electricity at peak demand is penalized with higher tarifs. When an individual or company consumes electricity when its most demanded, regulators calls that a coincident peak (CP).\nIn the Texas electricity market (ERCOT), the peak is the monthly 15-minute interval when the ERCOT Grid is at a point of highest capacity. The peak is caused by all consumers’ combined demand on the electrical grid. The coincident peak demand is an important factor used by ERCOT to determine final electricity consumption bills. ERCOT registers the CP demand of each client for 4 months, between June and September, and uses this to adjust electricity prices. Clients can therefore save on electricity bills by reducing the coincident peak demand.\nIn this example we will train an MSTL (Multiple Seasonal-Trend decomposition using LOESS) model on historic load data to forecast day-ahead peaks on September 2022. Multiple seasonality is traditionally present in low sampled electricity data. Demand exhibits daily and weekly seasonality, with clear patterns for specific hours of the day such as 6:00pm vs 3:00am or for specific days such as Sunday vs Friday.\nFirst, we will load ERCOT historic demand, then we will use the StatsForecast.cross_validation method to fit the MSTL model and forecast daily load during September. Finally, we show how to use the forecasts to detect the coincident peak.\nOutline\n\nInstall libraries\nLoad and explore the data\nFit MSTL model and forecast\nPeak detection",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Detect Demand Peaks"
    ]
  },
  {
    "objectID": "Nixtla/electricitypeakforecasting.html#libraries",
    "href": "Nixtla/electricitypeakforecasting.html#libraries",
    "title": "Nixtla - Detect Demand Peaks",
    "section": "Libraries",
    "text": "Libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nInstall the necessary packages using pip install statsforecast",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Detect Demand Peaks"
    ]
  },
  {
    "objectID": "Nixtla/electricitypeakforecasting.html#load-data",
    "href": "Nixtla/electricitypeakforecasting.html#load-data",
    "title": "Nixtla - Detect Demand Peaks",
    "section": "Load Data",
    "text": "Load Data\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nFirst, download and read the 2022 historic total demand of the ERCOT market, available here. The data processing includes adding the missing hour due to daylight saving time, parsing the date to datetime format, and filtering columns of interest. This step should take around 2s.\n\nimport numpy as np\nimport pandas as pd\n\n\n# Load data\nhistoric = pd.read_csv('./Native_Load_2022.csv')\nhistoric\n\n\n\n\n\n\n\n\nHour Ending\nCOAST\nEAST\nFWEST\nNORTH\nNCENT\nSOUTH\nSCENT\nWEST\nERCOT\n\n\n\n\n0\n01/01/2022 01:00\n12,054.94\n1,302.30\n4,161.19\n757.84\n9,676.30\n3,172.88\n5,908.03\n973.46\n38,006.94\n\n\n1\n01/01/2022 02:00\n11,793.29\n1,259.36\n4,147.91\n737.24\n9,307.13\n3,123.32\n5,708.51\n959.78\n37,036.52\n\n\n2\n01/01/2022 03:00\n11,460.84\n1,210.29\n4,156.41\n725.61\n8,920.42\n3,003.40\n5,463.52\n941.11\n35,881.61\n\n\n3\n01/01/2022 04:00\n11,244.98\n1,179.31\n4,149.81\n717.42\n8,678.81\n2,898.10\n5,255.25\n920.37\n35,044.06\n\n\n4\n01/01/2022 05:00\n11,073.09\n1,171.84\n4,140.62\n719.18\n8,573.37\n2,825.10\n5,164.17\n918.20\n34,585.57\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n12/31/2022 20:00\n10,247.56\n1,407.68\n5,219.00\n938.90\n11,656.96\n3,452.87\n6,833.56\n1,125.34\n40,881.88\n\n\n8756\n12/31/2022 21:00\n9,887.68\n1,362.64\n5,217.50\n963.91\n11,242.91\n3,228.13\n6,629.00\n1,103.92\n39,635.70\n\n\n8757\n12/31/2022 22:00\n9,572.38\n1,327.30\n5,235.31\n949.80\n10,944.37\n3,078.54\n6,413.23\n1,084.12\n38,605.05\n\n\n8758\n12/31/2022 23:00\n9,258.59\n1,237.91\n5,226.63\n928.93\n10,508.05\n3,020.18\n6,161.41\n1,058.60\n37,400.30\n\n\n8759\n12/31/2022 24:00\n8,999.62\n1,216.12\n5,193.65\n898.51\n10,139.20\n2,951.50\n5,957.23\n1,043.48\n36,399.31\n\n\n\n\n8760 rows × 10 columns\n\n\n\n\nhistoric = historic.drop(8016)\n\n\nhistoric['Hour Ending'][8015:8020]\n\n8015    11/30/2022 24:00\n8017    12/01/2022 02:00\n8018    12/01/2022 03:00\n8019    12/01/2022 04:00\n8020    12/01/2022 05:00\nName: Hour Ending, dtype: object\n\n\n\n# Add missing hour due to daylight saving time\nhistoric = pd.concat([historic, pd.DataFrame({'Hour Ending':['03/13/2022 03:00'], 'ERCOT':['43980.57']})])\nhistoric = historic.sort_values('Hour Ending').reset_index(drop=True)\n# Convert to datetime\nhistoric['ERCOT'] = historic['ERCOT'].str.replace(',','').astype(float)\nhistoric = historic[~pd.isna(historic['ERCOT'])]\nhistoric['ds'] = pd.to_datetime(historic['Hour Ending'].str[:10]) + pd.to_timedelta(np.tile(range(24), len(historic)//24),'h')\nhistoric['unique_id'] = 'ERCOT'\nhistoric['y'] = historic['ERCOT']\n# Select relevant columns and dates\nY_df = historic[['unique_id', 'ds', 'y']]\nY_df = Y_df[Y_df['ds']&lt;='2022-10-01']\n\nPlot the series using the plot method from the StatsForecast class. This method prints up to 8 random series from the dataset and is useful for basic EDA.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a default engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(Y_df)\n\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/core.py:25: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\n\n\n\n\n\n\n\nWe observe that the time series exhibits seasonal patterns. Moreover, the time series contains 6,552 observations, so it is necessary to use computationally efficient methods to deploy them in production.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Detect Demand Peaks"
    ]
  },
  {
    "objectID": "Nixtla/electricitypeakforecasting.html#fit-and-forecast-mstl-model",
    "href": "Nixtla/electricitypeakforecasting.html#fit-and-forecast-mstl-model",
    "title": "Nixtla - Detect Demand Peaks",
    "section": "Fit and Forecast MSTL model",
    "text": "Fit and Forecast MSTL model\nThe MSTL (Multiple Seasonal-Trend decomposition using LOESS) model decomposes the time series in multiple seasonalities using a Local Polynomial Regression (LOESS). Then it forecasts the trend using a custom non-seasonal model and each seasonality using a SeasonalNaive model.\n\n\n\n\n\n\nTip\n\n\n\nCheck our detailed explanation and tutorial on MSTL here\n\n\nImport the StatsForecast class and the models you need.\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import MSTL, AutoARIMA\n\nFirst, instantiate the model and define the parameters. The electricity load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] as the seasonalities. See this link for a detailed explanation on how to set seasonal lengths. In this example we use the AutoARIMA model for the trend component, however, any StatsForecast model can be used. The complete list of models is available here.\n\nmodels = [MSTL(\n            season_length=[24, 24 * 7], # seasonalities of the time series \n            trend_forecaster=AutoARIMA(nmodels=10) # model used to forecast trend\n            )\n          ]\n\n\n\n\n\n\n\nTip\n\n\n\nThe parameter nmodels of the AutoARIMA controls the number of models considered in stepwise search. The default is 94, reduce it to decrease training times!\n\n\nWe fit the model by instantiating a StatsForecast object with the following required parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\n\n\n# Instantiate StatsForecast class as sf\nsf = StatsForecast(\n    df=Y_df, \n    models=models,\n    freq='H', \n)\n\n\n\n\n\n\n\nTip\n\n\n\nStatsForecast also supports this optional parameter.\n\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores. (Default: 1)\nfallback_model: a model to be used if a model fails. (Default: none)\n\n\n\nThe cross_validation method allows the user to simulate multiple historic forecasts, greatly simplifying pipelines by replacing for loops with fit and predict methods. This method re-trains the model and forecast each window. See this tutorial for an animation of how the windows are defined.\nUse the cross_validation method to produce all the daily forecasts for September. To produce daily forecasts set the forecasting horizon h as 24. In this example we are simulating deploying the pipeline during September, so set the number of windows as 30 (one for each day). Finally, set the step size between windows as 24, to only produce one forecast per day.\n\ncrossvalidation_df = sf.cross_validation(\n    df=Y_df,\n    h=24,\n    step_size=24,\n    n_windows=30\n  )\n\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/arima.py:2270: UserWarning: Stepwise search was stopped early due to reaching the model number limit: nmodels=10\n  warnings.warn(\n\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nMSTL\n\n\nunique_id\n\n\n\n\n\n\n\n\nERCOT\n2022-09-01 01:00:00\n2022-09-01\n43602.660156\n44841.687500\n\n\nERCOT\n2022-09-01 02:00:00\n2022-09-01\n42284.820312\n43591.015625\n\n\nERCOT\n2022-09-01 03:00:00\n2022-09-01\n41663.160156\n42850.105469\n\n\nERCOT\n2022-09-01 04:00:00\n2022-09-01\n41710.621094\n43010.621094\n\n\nERCOT\n2022-09-01 05:00:00\n2022-09-01\n42847.570312\n44453.636719\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using cross_validation make sure the forecasts are produced at the desired timestamps. Check the cutoff column which specifices the last timestamp before the forecasting window.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Detect Demand Peaks"
    ]
  },
  {
    "objectID": "Nixtla/electricitypeakforecasting.html#peak-detection",
    "href": "Nixtla/electricitypeakforecasting.html#peak-detection",
    "title": "Nixtla - Detect Demand Peaks",
    "section": "Peak Detection",
    "text": "Peak Detection\nFinally, we use the forecasts in crossvaldation_df to detect the daily hourly demand peaks. For each day, we set the detected peaks as the highest forecasts. In this case, we want to predict one peak (npeaks); depending on your setting and goals, this parameter might change. For example, the number of peaks can correspond to how many hours a battery can be discharged to reduce demand.\n\nnpeaks = 1 # Number of peaks\n\nFor the ERCOT 4CP detection task we are interested in correctly predicting the highest monthly load. Next, we filter the day in September with the highest hourly demand and predict the peak.\n\ncrossvalidation_df = crossvalidation_df.reset_index()[['ds','y','MSTL']]\nmax_day = crossvalidation_df.iloc[crossvalidation_df['y'].argmax()].ds.day # Day with maximum load\ncv_df_day = crossvalidation_df.query('ds.dt.day == @max_day')\nmax_hour = cv_df_day['y'].argmax()\npeaks = cv_df_day['MSTL'].argsort().iloc[-npeaks:].values # Predicted peaks\n\nIn the following plot we see how the MSTL model is able to correctly detect the coincident peak for September 2022.\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(10, 5))\nplt.axvline(cv_df_day.iloc[max_hour]['ds'], color='black', label='True Peak')\nplt.scatter(cv_df_day.iloc[peaks]['ds'], cv_df_day.iloc[peaks]['MSTL'], color='green', label=f'Predicted Top-{npeaks}')\nplt.plot(cv_df_day['ds'], cv_df_day['y'], label='y', color='blue')\nplt.plot(cv_df_day['ds'], cv_df_day['MSTL'], label='Forecast', color='red')\nplt.xlabel('Time')\nplt.ylabel('Load (MW)')\nplt.grid()\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this example we only include September. However, MSTL can correctly predict the peaks for the 4 months of 2022. You can try this by increasing the nwindows parameter of cross_validation or filtering the Y_df dataset. The complete run for all months take only 10 minutes.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Detect Demand Peaks"
    ]
  },
  {
    "objectID": "Nixtla/electricitypeakforecasting.html#next-steps",
    "href": "Nixtla/electricitypeakforecasting.html#next-steps",
    "title": "Nixtla - Detect Demand Peaks",
    "section": "Next steps",
    "text": "Next steps\nStatsForecast and MSTL in particular are good benchmarking models for peak detection. However, it might be useful to explore further and newer forecasting algorithms. We have seen particularly good results with the N-HiTS, a deep-learning model from Nixtla’s NeuralForecast library.\nLearn how to predict ERCOT demand peaks with our deep-learning N-HiTS model and the NeuralForecast library in this tutorial.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Detect Demand Peaks"
    ]
  },
  {
    "objectID": "Nixtla/electricitypeakforecasting.html#references",
    "href": "Nixtla/electricitypeakforecasting.html#references",
    "title": "Nixtla - Detect Demand Peaks",
    "section": "References",
    "text": "References\n\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). “MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns”.\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). “N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting”. Accepted at AAAI 2023.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Detect Demand Peaks"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - statsforecasting.html",
    "href": "Nixtla/nixtla - statsforecasting.html",
    "title": "Nixtla - StatsForecasting",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis Guide assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start.\nFollow this article for a step to step guide on building a production-ready forecasting pipeline for multiple time series.\nDuring this guide you will gain familiary with the core StatsForecastclass and some relevant methods like StatsForecast.plot, StatsForecast.forecast and StatsForecast.cross_validation.\nWe will use a classical benchmarking dataset from the M4 competition. The dataset includes time series from different domains like finance, economy and sales. In this example, we will use a subset of the Hourly dataset.\nWe will model each time series individually. Forecasting at this level is also known as local forecasting. Therefore, you will train a series of models for every unique series and then select the best one. StatsForecast focuses on speed, simplicity, and scalability, which makes it ideal for this task.\nOutline:",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - StatsForecasting"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - statsforecasting.html#install-libraries",
    "href": "Nixtla/nixtla - statsforecasting.html#install-libraries",
    "title": "Nixtla - StatsForecasting",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - StatsForecasting"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - statsforecasting.html#read-the-data",
    "href": "Nixtla/nixtla - statsforecasting.html#read-the-data",
    "title": "Nixtla - StatsForecasting",
    "section": "Read the data",
    "text": "Read the data\nWe will use pandas to read the M4 Hourly data set stored in a parquet file for efficiency. You can use ordinary pandas operations to read your data in other formats likes .csv.\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestampe ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. The target column needs to be renamed to y if it has a different column name.\n\nThis data set already satisfies the requirements.\nDepending on your internet connection, this step should take around 10 seconds.\n\nimport pandas as pd\n\nY_df = pd.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet')\n\nY_df\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH1\n1\n605.0\n\n\n1\nH1\n2\n586.0\n\n\n2\nH1\n3\n586.0\n\n\n3\nH1\n4\n559.0\n\n\n4\nH1\n5\n511.0\n\n\n...\n...\n...\n...\n\n\n373367\nH99\n744\n24039.0\n\n\n373368\nH99\n745\n22946.0\n\n\n373369\nH99\n746\n22217.0\n\n\n373370\nH99\n747\n21416.0\n\n\n373371\nH99\n748\n19531.0\n\n\n\n\n373372 rows × 3 columns\n\n\n\nThis dataset contains 414 unique series with 900 observations on average. For this example and reproducibility’s sake, we will select only 10 unique IDs and keep only the last week. Depending on your processing infrastructure feel free to select more or less series.\n\n\n\n\n\n\nNote\n\n\n\nProcessing time is dependent on the available computing resources. Running this example with the complete dataset takes around 10 minutes in a c5d.24xlarge (96 cores) instance from AWS.\n\n\n\nuids = Y_df['unique_id'].unique()[:10] # Select 10 ids to make the example faster\n\nY_df = Y_df.query('unique_id in @uids') \n\nY_df = Y_df.groupby('unique_id').tail(7 * 24) #Select last 7 days of data to make example faster\nY_df\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n580\nH1\n581\n587.0\n\n\n581\nH1\n582\n537.0\n\n\n582\nH1\n583\n492.0\n\n\n583\nH1\n584\n464.0\n\n\n584\nH1\n585\n443.0\n\n\n...\n...\n...\n...\n\n\n7475\nH107\n744\n4316.0\n\n\n7476\nH107\n745\n4159.0\n\n\n7477\nH107\n746\n4058.0\n\n\n7478\nH107\n747\n3971.0\n\n\n7479\nH107\n748\n3770.0\n\n\n\n\n1680 rows × 3 columns",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - StatsForecasting"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - statsforecasting.html#explore-data-with-the-plot-method",
    "href": "Nixtla/nixtla - statsforecasting.html#explore-data-with-the-plot-method",
    "title": "Nixtla - StatsForecasting",
    "section": "Explore Data with the plot method",
    "text": "Explore Data with the plot method\nPlot some series using the plot method from the StatsForecast class. This method prints 8 random series from the dataset and is useful for basic EDA.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a defaul engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\n\nStatsForecast.plot(Y_df)",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - StatsForecasting"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - statsforecasting.html#train-multiple-models-for-many-series",
    "href": "Nixtla/nixtla - statsforecasting.html#train-multiple-models-for-many-series",
    "title": "Nixtla - StatsForecasting",
    "section": "Train multiple models for many series",
    "text": "Train multiple models for many series\nStatsForecast can train many models on many time series efficiently.\nStart by importing and instantiating the desired models. StatsForecast offers a wide variety of models grouped in the following categories:\n\nAuto Forecast: Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series. Includes automatic versions of: Arima, ETS, Theta, CES.\nExponential Smoothing: Uses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Examples: SES, Holt’s Winters, SSO.\nBenchmark models: classical models for establishing baselines. Examples: Mean, Naive, Random Walk\nIntermittent or Sparse models: suited for series with very few non-zero observations. Examples: CROSTON, ADIDA, IMAPA\nMultiple Seasonalities: suited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs. Examples: MSTL.\nTheta Models: fit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts. Examples: Theta, DynamicTheta\n\nHere you can check the complete list of models .\nFor this example we will use:\n\nAutoARIMA: Automatically selects the best ARIMA (AutoRegressive Integrated Moving Average) model using an information criterion. Ref: AutoARIMA.\nHoltWinters: triple exponential smoothing, Holt-Winters’ method is an extension of exponential smoothing for series that contain both trend and seasonality. Ref: HoltWinters\nSeasonalNaive: Memory Efficient Seasonal Naive predictions. Ref: SeasonalNaive\nHistoricAverage: arthimetic mean. Ref: HistoricAverage.\nDynamicOptimizedTheta: The theta family of models has been shown to perform well in various datasets such as M3. Models the deseasonalized time series. Ref: DynamicOptimizedTheta.\n\nImport and instantiate the models. Setting the season_length argument is sometimes tricky. This article on Seasonal periods) by the master, Rob Hyndmann, can be useful.\n\nfrom statsforecast.models import (\n    AutoARIMA,\n    HoltWinters,\n    CrostonClassic as Croston, \n    HistoricAverage,\n    DynamicOptimizedTheta as DOT,\n    SeasonalNaive,\n    MSTL\n)\n\n\n# Create a list of models and instantiation parameters\nmodels = [\n    AutoARIMA(season_length=24),\n    HoltWinters(),\n    Croston(),\n    SeasonalNaive(season_length=24),\n    HistoricAverage(),\n    DOT(season_length=24),\n    MSTL\n]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See pandas available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n# Instantiate StatsForecast class as sf\nsf = StatsForecast(\n    df=Y_df, \n    models=models,\n    freq='H', \n    n_jobs=-1,\n    fallback_model = SeasonalNaive(season_length=7)\n)\n\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s.\n\n\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\n\n\n\n\n\nNote\n\n\n\nThe forecast method is compatible with distributed clusters, so it does not store any model parameters. If you want to store parameters for every model you can use the fit and predict methods. However, those methods are not defined for distrubed engines like Spark, Ray or Dask.\n\n\n\nforecasts_df = sf.forecast(h=48, level=[90])\n\nforecasts_df.head()\n\n\n\n\n\n\n\n\nds\nAutoARIMA\nAutoARIMA-lo-90\nAutoARIMA-hi-90\nHoltWinters\nHoltWinters-lo-90\nHoltWinters-hi-90\nCrostonClassic\nCrostonClassic-lo-90\nCrostonClassic-hi-90\n...\nSeasonalNaive-hi-90\nHistoricAverage\nHistoricAverage-lo-90\nHistoricAverage-hi-90\nDynamicOptimizedTheta\nDynamicOptimizedTheta-lo-90\nDynamicOptimizedTheta-hi-90\n&lt;class 'statsforecast.models.MSTL'&gt;\n&lt;class 'statsforecast.models.MSTL'&gt;-lo-90\n&lt;class 'statsforecast.models.MSTL'&gt;-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nH1\n749\n592.461792\n572.325623\n612.597961\n829.0\n-246.367554\n1904.367554\n829.0\n-246.367554\n1904.367554\n...\n732.528809\n660.982117\n398.03775\n923.926514\n592.701843\n577.677307\n611.652649\n829.0\n-246.367554\n1904.367554\n\n\nH1\n750\n527.174316\n495.321777\n559.026855\n807.0\n-268.367554\n1882.367554\n807.0\n-268.367554\n1882.367554\n...\n669.528809\n660.982117\n398.03775\n923.926514\n525.589111\n505.449738\n546.621826\n807.0\n-268.367554\n1882.367554\n\n\nH1\n751\n488.418549\n445.535583\n531.301514\n785.0\n-290.367554\n1860.367554\n785.0\n-290.367554\n1860.367554\n...\n629.528809\n660.982117\n398.03775\n923.926514\n489.251801\n462.072876\n512.424133\n785.0\n-290.367554\n1860.367554\n\n\nH1\n752\n452.284454\n400.677155\n503.891785\n756.0\n-319.367554\n1831.367554\n756.0\n-319.367554\n1831.367554\n...\n590.528809\n660.982117\n398.03775\n923.926514\n456.195038\n430.554291\n478.260956\n756.0\n-319.367554\n1831.367554\n\n\nH1\n753\n433.127563\n374.070984\n492.184143\n719.0\n-356.367554\n1794.367554\n719.0\n-356.367554\n1794.367554\n...\n574.528809\n660.982117\n398.03775\n923.926514\n436.290527\n411.051239\n461.815948\n719.0\n-356.367554\n1794.367554\n\n\n\n\n5 rows × 22 columns\n\n\n\nPlot the results of 8 random series using the StatsForecast.plot method.\n\nsf.plot(Y_df,forecasts_df)\n\n\n\n\n\n\n\n\nThe StatsForecast.plot allows for further customization. For example, plot the results of the different models and unique ids.\n\n# Plot to unique_ids and some selected models\nsf.plot(Y_df, forecasts_df, models=[\"HoltWinters\",\"DynamicOptimizedTheta\"], unique_ids=[\"H10\", \"H105\"], level=[90])\n\n\n\n\n\n\n\n\n\n# Explore other models \nsf.plot(Y_df, forecasts_df, models=[\"AutoARIMA\"], unique_ids=[\"H10\", \"H105\"], level=[90])",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - StatsForecasting"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - statsforecasting.html#evaluate-the-models-performance",
    "href": "Nixtla/nixtla - statsforecasting.html#evaluate-the-models-performance",
    "title": "Nixtla - StatsForecasting",
    "section": "Evaluate the model’s performance",
    "text": "Evaluate the model’s performance\nIn previous steps, we’ve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model’s predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 2 days (n_windows=2), forecasting every second day (step_size=48). Depending on your computer, this step should take around 1 min.\n\n\n\n\n\n\nTip\n\n\n\nSetting n_windows=1 mirrors a traditional train-test split with our historical data serving as the training set and the last 48 hours serving as the testing set.\n\n\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 24 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvaldation_df = sf.cross_validation(\n    df=Y_df,\n    h=24,\n    step_size=24,\n    n_windows=2\n  )\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncrossvaldation_df.head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nAutoARIMA\nHoltWinters\nCrostonClassic\nSeasonalNaive\nHistoricAverage\nDynamicOptimizedTheta\n&lt;class 'statsforecast.models.MSTL'&gt;\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nH1\n701\n700\n619.0\n603.925415\n847.0\n742.668762\n691.0\n661.674988\n612.767517\n847.0\n\n\nH1\n702\n700\n565.0\n507.591736\n820.0\n742.668762\n618.0\n661.674988\n536.846252\n820.0\n\n\nH1\n703\n700\n532.0\n481.281677\n790.0\n742.668762\n563.0\n661.674988\n497.824280\n790.0\n\n\nH1\n704\n700\n495.0\n444.410248\n784.0\n742.668762\n529.0\n661.674988\n464.723236\n784.0\n\n\nH1\n705\n700\n481.0\n421.168762\n752.0\n742.668762\n504.0\n661.674988\n440.972351\n752.0\n\n\n\n\n\n\n\nNext, we will evaluate the performance of every model for every series using common error metrics like Mean Absolute Error (MAE) or Mean Square Error (MSE) Define a utility function to evaluate different error metrics for the cross validation data frame.\nFirst import the desired error metrics from mlforecast.losses. Then define a utility function that takes a cross-validation data frame as a metric and returns an evaluation data frame with the average of the error metric for every unique id and fitted model and all cutoffs.\n\nfrom utilsforecast.losses import mse\nfrom utilsforecast.evaluation import evaluate\n\n\ndef evaluate_cross_validation(df, metric):\n    models = df.drop(columns=['unique_id', 'ds', 'cutoff', 'y']).columns.tolist()\n    evals = []\n    # Calculate loss for every unique_id and cutoff.    \n    for cutoff in df['cutoff'].unique():\n        eval_ = evaluate(df[df['cutoff'] == cutoff], metrics=[metric], models=models)\n        evals.append(eval_)\n    evals = pd.concat(evals)\n    evals = evals.groupby('unique_id').mean(numeric_only=True) # Averages the error metrics for all cutoffs for every combination of model and unique_id\n    evals['best_model'] = evals.idxmin(axis=1)\n    return evals\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can also use Mean Average Percentage Error (MAPE), however for granular forecasts, MAPE values are extremely hard to judge and not useful to assess forecasting quality.\n\n\nCreate the data frame with the results of the evaluation of your cross-validation data frame using a Mean Squared Error metric.\n\nevaluation_df = evaluate_cross_validation(crossvaldation_df.reset_index(), mse)\n\nevaluation_df.head()\n\n\n\n\n\n\n\n\nAutoARIMA\nHoltWinters\nCrostonClassic\nSeasonalNaive\nHistoricAverage\nDynamicOptimizedTheta\n&lt;class 'statsforecast.models.MSTL'&gt;\nbest_model\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\nH1\n1979.302124\n44888.019531\n28038.736328\n1422.666748\n20927.664062\n1296.333984\n44888.019531\nDynamicOptimizedTheta\n\n\nH10\n458.892700\n2812.916504\n1483.484131\n96.895828\n1980.367432\n379.621124\n2812.916504\nSeasonalNaive\n\n\nH100\n8629.948242\n121625.375000\n91945.140625\n12019.000000\n78491.187500\n21699.648438\n121625.375000\nAutoARIMA\n\n\nH101\n6818.348633\n28453.394531\n16183.634766\n10944.458008\n18208.404297\n63698.082031\n28453.394531\nAutoARIMA\n\n\nH102\n65489.964844\n232924.843750\n132655.296875\n12699.896484\n309110.468750\n31393.519531\n232924.843750\nSeasonalNaive\n\n\n\n\n\n\n\nCreate a summary table with a model column and the number of series where that model performs best. In this case, the Arima and Seasonal Naive are the best models for 10 series and the Theta model should be used for two.\n\nsummary_df = evaluation_df.groupby('best_model').size().sort_values().to_frame()\n\nsummary_df.reset_index().columns = [\"Model\", \"Nr. of unique_ids\"]\n\nYou can further explore your results by plotting the unique_ids where a specific model wins.\n\nseasonal_ids = evaluation_df.query('best_model == \"SeasonalNaive\"').index\n\nsf.plot(Y_df,forecasts_df, unique_ids=seasonal_ids, models=[\"SeasonalNaive\",\"DynamicOptimizedTheta\"])",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - StatsForecasting"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - statsforecasting.html#select-the-best-model-for-every-unique-series",
    "href": "Nixtla/nixtla - statsforecasting.html#select-the-best-model-for-every-unique-series",
    "title": "Nixtla - StatsForecasting",
    "section": "Select the best model for every unique series",
    "text": "Select the best model for every unique series\nDefine a utility function that takes your forecast’s data frame with the predictions and the evaluation data frame and returns a data frame with the best possible forecast for every unique_id.\n\ndef get_best_model_forecast(forecasts_df, evaluation_df):\n    df = forecasts_df.set_index('ds', append=True).stack().to_frame().reset_index(level=2) # Wide to long \n    df.columns = ['model', 'best_model_forecast'] \n    df = df.join(evaluation_df[['best_model']])\n    df = df.query('model.str.replace(\"-lo-90|-hi-90\", \"\", regex=True) == best_model').copy()\n    df.loc[:, 'model'] = [model.replace(bm, 'best_model') for model, bm in zip(df['model'], df['best_model'])]\n    df = df.drop(columns='best_model').set_index('model', append=True).unstack()\n    df.columns = df.columns.droplevel()\n    df = df.reset_index(level=1)\n    return df\n\nCreate your production-ready data frame with the best forecast for every unique_id.\n\nprod_forecasts_df = get_best_model_forecast(forecasts_df, evaluation_df)\n\nprod_forecasts_df.head()\n\n\n\n\n\n\n\nmodel\nds\nbest_model\nbest_model-hi-90\nbest_model-lo-90\n\n\nunique_id\n\n\n\n\n\n\n\n\nH1\n749\n592.701843\n611.652649\n577.677307\n\n\nH1\n750\n525.589111\n546.621826\n505.449738\n\n\nH1\n751\n489.251801\n512.424133\n462.072876\n\n\nH1\n752\n456.195038\n478.260956\n430.554291\n\n\nH1\n753\n436.290527\n461.815948\n411.051239\n\n\n\n\n\n\n\nPlot the results.\n\nsf.plot(Y_df, prod_forecasts_df, level=[90])",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - StatsForecasting"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neuralforecast.html",
    "href": "Nixtla/nixtla - neuralforecast.html",
    "title": "Neural Forecast",
    "section": "",
    "text": "This notebook provides an example on how to start using the main functionalities of the NeuralForecast library. The NeuralForecast class allows users to easily interact with NeuralForecast.models PyTorch models. In this example we will forecast AirPassengers data with a classic LSTM and the recent NHITS models. The full list of available models is available here.\nYou can run these experiments using GPU with Google Colab.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Neural Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neuralforecast.html#installing-neuralforecast",
    "href": "Nixtla/nixtla - neuralforecast.html#installing-neuralforecast",
    "title": "Neural Forecast",
    "section": "1. Installing NeuralForecast",
    "text": "1. Installing NeuralForecast\n%%capture !pip install neuralforecast",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Neural Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neuralforecast.html#loading-airpassengers-data",
    "href": "Nixtla/nixtla - neuralforecast.html#loading-airpassengers-data",
    "title": "Neural Forecast",
    "section": "2. Loading AirPassengers Data",
    "text": "2. Loading AirPassengers Data\nThe core.NeuralForecast class contains shared, fit, predict and other methods that take as inputs pandas DataFrames with columns ['unique_id', 'ds', 'y'], where unique_id identifies individual time series from the dataset, ds is the date, and y is the target variable.\nIn this example dataset consists of a set of a single series, but you can easily fit your model to larger datasets in long format.\n\nfrom neuralforecast.utils import AirPassengersDF\n\nY_df = AirPassengersDF # Defined in neuralforecast.utils\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n1.0\n1949-01-31\n112.0\n\n\n1\n1.0\n1949-02-28\n118.0\n\n\n2\n1.0\n1949-03-31\n132.0\n\n\n3\n1.0\n1949-04-30\n129.0\n\n\n4\n1.0\n1949-05-31\n121.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDataFrames must include all ['unique_id', 'ds', 'y'] columns. Make sure y column does not have missing or non-numeric values.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Neural Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neuralforecast.html#model-training",
    "href": "Nixtla/nixtla - neuralforecast.html#model-training",
    "title": "Neural Forecast",
    "section": "3. Model Training",
    "text": "3. Model Training\n\nFit the models\nUsing the NeuralForecast.fit method you can train a set of models to your dataset. You can define the forecasting horizon (12 in this example), and modify the hyperparameters of the model. For example, for the LSTM we changed the default hidden size for both encoder and decoders.\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import LSTM, NHITS, RNN\n\n\nhorizon = 12\n\n# Try different hyperparmeters to improve accuracy.\nmodels = [LSTM(h=horizon,                    # Forecast horizon\n               max_steps=500,                # Number of steps to train\n               scaler_type='standard',       # Type of scaler to normalize data\n               encoder_hidden_size=64,       # Defines the size of the hidden state of the LSTM\n               decoder_hidden_size=64,),     # Defines the number of hidden units of each layer of the MLP decoder\n          NHITS(h=horizon,                   # Forecast horizon\n                input_size=2 * horizon,      # Length of input sequence\n                max_steps=100,               # Number of steps to train\n                n_freq_downsample=[2, 1, 1]) # Downsampling factors for each stack output\n          ]\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(df=Y_df)\n\nGlobal seed set to 1\nGlobal seed set to 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe performance of Deep Learning models can be very sensitive to the choice of hyperparameters. Tuning the correct hyperparameters is an important step to obtain the best forecasts. The Auto version of these models, AutoLSTM and AutoNHITS, already perform hyperparameter selection automatically.\n\n\n\n\nPredict using the fitted models\nUsing the NeuralForecast.predict method you can obtain the h forecasts after the training data Y_df.\n\nY_hat_df = nf.predict()\n\n\n\n\n\n\n\nThe NeuralForecast.predict method returns a DataFrame with the forecasts for each unique_id, ds, and model.\n\nY_hat_df = Y_hat_df.reset_index()\nY_hat_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nLSTM\nNHITS\n\n\n\n\n0\n1.0\n1961-01-31\n430.170532\n444.642365\n\n\n1\n1.0\n1961-02-28\n438.625183\n434.070618\n\n\n2\n1.0\n1961-03-31\n455.669678\n481.944946\n\n\n3\n1.0\n1961-04-30\n464.299500\n499.688141\n\n\n4\n1.0\n1961-05-31\n499.237671\n512.285706",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Neural Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neuralforecast.html#plot-predictions",
    "href": "Nixtla/nixtla - neuralforecast.html#plot-predictions",
    "title": "Neural Forecast",
    "section": "4. Plot Predictions",
    "text": "4. Plot Predictions\nFinally, we plot the forecasts of both models againts the real values.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df = pd.concat([Y_df, Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes\nplot_df[['y', 'LSTM', 'NHITS']].plot(ax=ax, linewidth=2)\n\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor this guide we are using a simple LSTM model. More recent models, such as RNN, GRU, and DilatedRNN achieve better accuracy than LSTM in most settings. The full list of available models is available here.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Neural Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neuralforecast.html#references",
    "href": "Nixtla/nixtla - neuralforecast.html#references",
    "title": "Neural Forecast",
    "section": "References",
    "text": "References\n\nBoris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio (2020). “N-BEATS: Neural basis expansion analysis for interpretable time series forecasting”. International Conference on Learning Representations.\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). NHITS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Neural Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - mlforecast.html",
    "href": "Nixtla/nixtla - mlforecast.html",
    "title": "Nixtla - ML Forecast",
    "section": "",
    "text": "import pandas as pd\nfrom statsforecast import StatsForecast\n\n\ndf = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/air-passengers.csv', parse_dates=['ds'])\ndf.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nAirPassengers\n1949-01-01\n112\n\n\n1\nAirPassengers\n1949-02-01\n118\n\n\n2\nAirPassengers\n1949-03-01\n132\n\n\n3\nAirPassengers\n1949-04-01\n129\n\n\n4\nAirPassengers\n1949-05-01\n121\n\n\n\n\n\n\n\n\ndf['unique_id'].value_counts()\n\nunique_id\nAirPassengers    144\nName: count, dtype: int64\n\n\n\nStatsForecast.plot(df, engine='plotly')\n\n                                                \n\n\n\nfrom mlforecast import MLForecast\nfrom mlforecast.target_transforms import Differences\nfrom sklearn.linear_model import LinearRegression\n\n\nfcst = MLForecast(\n    models=LinearRegression(),\n    freq='MS',  # our serie has a monthly frequency\n    lags=[12],\n    target_transforms=[Differences([1])],\n)\n\n\nfcst.fit(df)\n\nMLForecast(models=[LinearRegression], freq=&lt;MonthBegin&gt;, lag_features=['lag12'], date_features=[], num_threads=1)\n\n\n\npreds = fcst.predict(12)\npreds\n\n\n\n\n\n\n\n\nunique_id\nds\nLinearRegression\n\n\n\n\n0\nAirPassengers\n1961-01-01\n444.656555\n\n\n1\nAirPassengers\n1961-02-01\n417.470764\n\n\n2\nAirPassengers\n1961-03-01\n446.903076\n\n\n3\nAirPassengers\n1961-04-01\n491.014160\n\n\n4\nAirPassengers\n1961-05-01\n502.622253\n\n\n5\nAirPassengers\n1961-06-01\n568.751465\n\n\n6\nAirPassengers\n1961-07-01\n660.044312\n\n\n7\nAirPassengers\n1961-08-01\n643.343323\n\n\n8\nAirPassengers\n1961-09-01\n540.666748\n\n\n9\nAirPassengers\n1961-10-01\n491.462799\n\n\n10\nAirPassengers\n1961-11-01\n417.095245\n\n\n11\nAirPassengers\n1961-12-01\n461.206299\n\n\n\n\n\n\n\n\nStatsForecast.plot(df, preds, engine='plotly')\n\n                                                \n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - ML Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neural_advanced.html",
    "href": "Nixtla/nixtla - neural_advanced.html",
    "title": "Nixtla - Neural Advanced Forecast",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis Guide assumes basic familiarity with NeuralForecast. For a minimal example visit the Quick Start\nFollow this article for a step to step guide on building a production-ready forecasting pipeline for multiple time series.\nDuring this guide you will gain familiary with the core NueralForecastclass and some relevant methods like NeuralForecast.fit, NeuralForecast.predict, and StatsForecast.cross_validation.\nWe will use a classical benchmarking dataset from the M4 competition. The dataset includes time series from different domains like finance, economy and sales. In this example, we will use a subset of the Hourly dataset.\nWe will model each time series globally Therefore, you will train a set of models for the whole dataset, and then select the best model for each individual time series. NeuralForecast focuses on speed, simplicity, and scalability, which makes it ideal for this task.\nOutline:",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Neural Advanced Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neural_advanced.html#install-libraries",
    "href": "Nixtla/nixtla - neural_advanced.html#install-libraries",
    "title": "Nixtla - Neural Advanced Forecast",
    "section": "1. Install libraries",
    "text": "1. Install libraries\nWe assume you have NeuralForecast already installed. Check this guide for instructions on how to install NeuralForecast.\nAdditionally, we will install s3fs to read from the S3 Filesystem of AWS, statsforecast for plotting, and datasetsforecast for common error metrics like MAE or MASE.\nInstall the necessary packages using pip install statsforecast s3fs datasetsforecast ``\n%%capture ! pip install statsforecast s3fs datasetsforecast\n%%capture ! pip install git+https://github.com/Nixtla/neuralforecast.git@main",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Neural Advanced Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neural_advanced.html#read-the-data",
    "href": "Nixtla/nixtla - neural_advanced.html#read-the-data",
    "title": "Nixtla - Neural Advanced Forecast",
    "section": "2. Read the data",
    "text": "2. Read the data\nWe will use pandas to read the M4 Hourly data set stored in a parquet file for efficiency. You can use ordinary pandas operations to read your data in other formats likes .csv.\nThe input to NeuralForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestampe ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nThis data set already satisfies the requirement.\nDepending on your internet connection, this step should take around 10 seconds.\n\nimport pandas as pd\n\nY_df = pd.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet')\n\n\nY_df.head(), len(Y_df.unique_id.unique())\n\n(  unique_id  ds      y\n 0        H1   1  605.0\n 1        H1   2  586.0\n 2        H1   3  586.0\n 3        H1   4  559.0\n 4        H1   5  511.0,\n 414)\n\n\nThis dataset contains 414 unique series with 900 observations on average. For this example and reproducibility’s sake, we will select only 10 unique IDs. Depending on your processing infrastructure feel free to select more or less series.\n\n\n\n\n\n\nNote\n\n\n\nProcessing time is dependent on the available computing resources. Running this example with the complete dataset takes around 10 minutes in a c5d.24xlarge (96 cores) instance from AWS.\n\n\n\nuids = Y_df['unique_id'].unique()[:10] # Select 10 ids to make the example faster\nY_df = Y_df.query('unique_id in @uids').reset_index(drop=True)",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Neural Advanced Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neural_advanced.html#explore-data-with-the-plot-method-of-statsforecast",
    "href": "Nixtla/nixtla - neural_advanced.html#explore-data-with-the-plot-method-of-statsforecast",
    "title": "Nixtla - Neural Advanced Forecast",
    "section": "3. Explore Data with the plot method of StatsForecast",
    "text": "3. Explore Data with the plot method of StatsForecast\nPlot some series using the plot method from the StatsForecast class. This method prints 8 random series from the dataset and is useful for basic EDA.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a defaul engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(Y_df, engine='matplotlib')\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/statsforecast/core.py:25: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Neural Advanced Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neural_advanced.html#train-multiple-models-for-many-series",
    "href": "Nixtla/nixtla - neural_advanced.html#train-multiple-models-for-many-series",
    "title": "Nixtla - Neural Advanced Forecast",
    "section": "4. Train multiple models for many series",
    "text": "4. Train multiple models for many series\nNeuralForecast can train many models on many time series globally and efficiently.\n\nfrom ray import tune\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.auto import AutoNHITS, AutoLSTM\nfrom neuralforecast.losses.pytorch import MQLoss\n\nEach Auto model contains a default search space that was extensively tested on multiple large-scale datasets. Additionally, users can define specific search spaces tailored for particular datasets and tasks.\nFirst, we create a custom search space for the AutoNHITS and AutoLSTM models. Search spaces are specified with dictionaries, where keys corresponds to the model’s hyperparameter and the value is a Tune function to specify how the hyperparameter will be sampled. For example, use randint to sample integers uniformly, and choice to sample values of a list.\n\nconfig_nhits = {\n    \"input_size\": tune.choice([48, 48*2, 48*3]),              # Length of input window\n    \"start_padding_enabled\": True,\n    \"n_blocks\": 5*[1],                                              # Length of input window\n    \"mlp_units\": 5 * [[64, 64]],                                  # Length of input window\n    \"n_pool_kernel_size\": tune.choice([5*[1], 5*[2], 5*[4],         \n                                      [8, 4, 2, 1, 1]]),            # MaxPooling Kernel size\n    \"n_freq_downsample\": tune.choice([[8, 4, 2, 1, 1],\n                                      [1, 1, 1, 1, 1]]),            # Interpolation expressivity ratios\n    \"learning_rate\": tune.loguniform(1e-4, 1e-2),                   # Initial Learning rate\n    \"scaler_type\": tune.choice([None]),                             # Scaler type\n    \"max_steps\": tune.choice([1000]),                               # Max number of training iterations\n    \"batch_size\": tune.choice([1, 4, 10]),                          # Number of series in batch\n    \"windows_batch_size\": tune.choice([128, 256, 512]),             # Number of windows in batch\n    \"random_seed\": tune.randint(1, 20),                             # Random seed\n}\n\nconfig_lstm = {\n    \"input_size\": tune.choice([48, 48*2, 48*3]),              # Length of input window\n    \"encoder_hidden_size\": tune.choice([64, 128]),            # Hidden size of LSTM cells\n    \"encoder_n_layers\": tune.choice([2,4]),                   # Number of layers in LSTM\n    \"learning_rate\": tune.loguniform(1e-4, 1e-2),             # Initial Learning rate\n    \"scaler_type\": tune.choice(['robust']),                   # Scaler type\n    \"max_steps\": tune.choice([500, 1000]),                    # Max number of training iterations\n    \"batch_size\": tune.choice([1, 4]),                        # Number of series in batch\n    \"random_seed\": tune.randint(1, 20),                       # Random seed\n}\n\nTo instantiate an Auto model you need to define:\n\nh: forecasting horizon.\nloss: training and validation loss from neuralforecast.losses.pytorch.\nconfig: hyperparameter search space. If None, the Auto class will use a pre-defined suggested hyperparameter space.\nsearch_alg: search algorithm (from tune.search), default is random search. Refer to https://docs.ray.io/en/latest/tune/api_docs/suggestion.html for more information on the different search algorithm options.\nnum_samples: number of configurations explored.\n\nIn this example we set horizon h as 48, use the MQLoss distribution loss for training and validation, and use the default search algorithm.\n\nnf = NeuralForecast(\n    models=[\n        AutoNHITS(h=48, config=config_nhits, loss=MQLoss(), num_samples=5),\n        AutoLSTM(h=48, config=config_lstm, loss=MQLoss(), num_samples=2),\n    ],\n    freq='H'\n)\n\n\n\n\n\n\n\nTip\n\n\n\nThe number of samples, num_samples, is a crucial parameter! Larger values will usually produce better results as we explore more configurations in the search space, but it will increase training times. Larger search spaces will usually require more samples. As a general rule, we recommend setting num_samples higher than 20.\n\n\nNext, we use the Neuralforecast class to train the Auto model. In this step, Auto models will automatically perform hyperparameter tuning training multiple models with different hyperparameters, producing the forecasts on the validation set, and evaluating them. The best configuration is selected based on the error on a validation set. Only the best model is stored and used during inference.\n\nnf.fit(df=Y_df)\n\n2023-12-08 16:03:28,784 INFO worker.py:1673 -- Started a local Ray instance.\n2023-12-08 16:03:31,402 INFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n2023-12-08 16:03:31,405 INFO tune.py:586 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n2023-12-08 16:04:31,718 WARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. No trial is running and no new trial has been started within the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 0 CPUs and 1.0 GPUs per trial, but the cluster only has 12.0 CPUs and 0 GPUs available. Stop the tuning and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n2023-12-08 16:05:31,748 WARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. No trial is running and no new trial has been started within the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 0 CPUs and 1.0 GPUs per trial, but the cluster only has 12.0 CPUs and 0 GPUs available. Stop the tuning and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n2023-12-08 16:05:36,474 WARNING tune.py:186 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n2023-12-08 16:05:36,490 WARNING tune.py:1062 -- Experiment has been interrupted, but the most recent state was saved.\nResume experiment with: Tuner.restore(path=\"/home/ben/ray_results/_train_tune_2023-12-08_16-03-25\", trainable=...)\n2023-12-08 16:05:36,495 WARNING experiment_analysis.py:185 -- Failed to fetch metrics for 5 trial(s):\n- _train_tune_7b276_00000: FileNotFoundError('Could not fetch metrics for _train_tune_7b276_00000: both result.json and progress.csv were not found at /home/ben/ray_results/_train_tune_2023-12-08_16-03-25/_train_tune_7b276_00000_0_batch_size=10,input_size=96,learning_rate=0.0002,max_steps=1000,n_freq_downsample=1_1_1_1_1,n_pool_kerne_2023-12-08_16-03-31')\n- _train_tune_7b276_00001: FileNotFoundError('Could not fetch metrics for _train_tune_7b276_00001: both result.json and progress.csv were not found at /home/ben/ray_results/_train_tune_2023-12-08_16-03-25/_train_tune_7b276_00001_1_batch_size=4,input_size=48,learning_rate=0.0045,max_steps=1000,n_freq_downsample=1_1_1_1_1,n_pool_kernel_2023-12-08_16-03-31')\n- _train_tune_7b276_00002: FileNotFoundError('Could not fetch metrics for _train_tune_7b276_00002: both result.json and progress.csv were not found at /home/ben/ray_results/_train_tune_2023-12-08_16-03-25/_train_tune_7b276_00002_2_batch_size=1,input_size=96,learning_rate=0.0032,max_steps=1000,n_freq_downsample=8_4_2_1_1,n_pool_kernel_2023-12-08_16-03-31')\n- _train_tune_7b276_00003: FileNotFoundError('Could not fetch metrics for _train_tune_7b276_00003: both result.json and progress.csv were not found at /home/ben/ray_results/_train_tune_2023-12-08_16-03-25/_train_tune_7b276_00003_3_batch_size=10,input_size=144,learning_rate=0.0004,max_steps=1000,n_freq_downsample=8_4_2_1_1,n_pool_kern_2023-12-08_16-03-31')\n- _train_tune_7b276_00004: FileNotFoundError('Could not fetch metrics for _train_tune_7b276_00004: both result.json and progress.csv were not found at /home/ben/ray_results/_train_tune_2023-12-08_16-03-25/_train_tune_7b276_00004_4_batch_size=4,input_size=144,learning_rate=0.0003,max_steps=1000,n_freq_downsample=8_4_2_1_1,n_pool_kerne_2023-12-08_16-03-31')\n2023-12-08 16:05:36,495 WARNING experiment_analysis.py:575 -- Could not find best trial. Did you pass the correct `metric` parameter?\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 nf.fit(df=Y_df)\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/neuralforecast/core.py:274, in NeuralForecast.fit(self, df, static_df, val_size, sort_df, use_init_models, verbose)\n    271         print(\"WARNING: Deleting previously fitted models.\")\n    273 for model in self.models:\n--&gt; 274     model.fit(self.dataset, val_size=val_size)\n    276 self._fitted = True\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/neuralforecast/common/_base_auto.py:373, in BaseAuto.fit(self, dataset, val_size, test_size, random_seed)\n    360 if self.backend == \"ray\":\n    361     results = self._tune_model(\n    362         cls_model=self.cls_model,\n    363         dataset=dataset,\n   (...)\n    371         config=self.config,\n    372     )\n--&gt; 373     best_config = results.get_best_result().config\n    374 else:\n    375     results = self._optuna_tune_model(\n    376         cls_model=self.cls_model,\n    377         dataset=dataset,\n   (...)\n    383         config=self.config,\n    384     )\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/ray/tune/result_grid.py:162, in ResultGrid.get_best_result(self, metric, mode, scope, filter_nan_and_inf)\n    151     error_msg = (\n    152         \"No best trial found for the given metric: \"\n    153         f\"{metric or self._experiment_analysis.default_metric}. \"\n    154         \"This means that no trial has reported this metric\"\n    155     )\n    156     error_msg += (\n    157         \", or all values reported for this metric are NaN. To not ignore NaN \"\n    158         \"values, you can set the `filter_nan_and_inf` arg to False.\"\n    159         if filter_nan_and_inf\n    160         else \".\"\n    161     )\n--&gt; 162     raise RuntimeError(error_msg)\n    164 return self._trial_to_result(best_trial)\n\nRuntimeError: No best trial found for the given metric: loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.\n\n\n\n(raylet) [2023-12-08 16:11:28,719 E 602035 602035] (raylet) node_manager.cc:3035: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0e206a13d87f7d2f8b01010d06fa0b550b64bbb8773293ab52e0686e, IP: 172.19.100.51) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.19.100.51`\n(raylet) \n(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n(raylet) [2023-12-08 16:21:28,835 E 602035 602035] (raylet) node_manager.cc:3035: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0e206a13d87f7d2f8b01010d06fa0b550b64bbb8773293ab52e0686e, IP: 172.19.100.51) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.19.100.51`\n(raylet) \n(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n\n\nNext, we use the predict method to forecast the next 48 days using the optimal hyperparameters.\n\nfcst_df = nf.predict()\nfcst_df.columns = fcst_df.columns.str.replace('-median', '')\nfcst_df.head()\n\nPredicting DataLoader 0: 100%|██████████| 3/3 [00:00&lt;00:00, 164.16it/s]\nPredicting DataLoader 0: 100%|██████████| 3/3 [00:00&lt;00:00, 13.89it/s]\n\n\n\n\n\n\n\n\n\nds\nAutoNHITS\nAutoNHITS-lo-90\nAutoNHITS-lo-80\nAutoNHITS-hi-80\nAutoNHITS-hi-90\nAutoLSTM\nAutoLSTM-lo-90\nAutoLSTM-lo-80\nAutoLSTM-hi-80\nAutoLSTM-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nH1\n749\n550.545288\n491.368347\n484.838226\n640.832520\n658.631592\n581.597534\n510.460632\n533.967041\n660.153076\n690.976379\n\n\nH1\n750\n549.216736\n491.054932\n484.474243\n639.552002\n657.615967\n530.324402\n440.821899\n472.254272\n622.214539\n653.435913\n\n\nH1\n751\n528.075989\n466.917053\n463.002289\n621.197205\n642.255005\n487.045593\n383.502045\n423.310974\n594.273071\n627.640320\n\n\nH1\n752\n486.842255\n418.012115\n419.017242\n585.653259\n611.903809\n457.408081\n347.901093\n390.807495\n569.789062\n604.200012\n\n\nH1\n753\n452.015930\n371.543884\n379.539215\n558.845154\n590.465942\n441.641418\n333.888611\n374.730621\n557.401978\n595.008484\n\n\n\n\n\n\n\n\nStatsForecast.plot(Y_df, fcst_df, engine='matplotlib', max_insample_length=48 * 3, level=[80, 90])\n\n\n\n\n\n\n\n\nThe StatsForecast.plot allows for further customization. For example, plot the results of the different models and unique ids.\n\n# Plot to unique_ids and some selected models\nStatsForecast.plot(Y_df, fcst_df, models=[\"AutoLSTM\"], unique_ids=[\"H107\", \"H104\"], level=[80, 90], engine='matplotlib')\n\n\n\n\n\n\n\n\n\n# Explore other models \nStatsForecast.plot(Y_df, fcst_df, models=[\"AutoNHITS\"], unique_ids=[\"H10\", \"H105\"], level=[80, 90], engine='matplotlib')",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Neural Advanced Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neural_advanced.html#evaluate-the-models-performance",
    "href": "Nixtla/nixtla - neural_advanced.html#evaluate-the-models-performance",
    "title": "Nixtla - Neural Advanced Forecast",
    "section": "5. Evaluate the model’s performance",
    "text": "5. Evaluate the model’s performance\nIn previous steps, we’ve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model’s predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\n\n\n\n\n\n\nTip\n\n\n\nSetting n_windows=1 mirrors a traditional train-test split with our historical data serving as the training set and the last 48 hours serving as the testing set.\n\n\nThe cross_validation method from the NeuralForecast class takes the following arguments.\n\ndf: training data frame\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows (int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\nfrom neuralforecast.auto import AutoNHITS, AutoLSTM\nconfig_nhits = {\n    \"input_size\": tune.choice([48, 48*2, 48*3]),              # Length of input window\n    \"start_padding_enabled\": True,\n    \"n_blocks\": 5*[1],                                              # Length of input window\n    \"mlp_units\": 5 * [[64, 64]],                                  # Length of input window\n    \"n_pool_kernel_size\": tune.choice([5*[1], 5*[2], 5*[4],         \n                                      [8, 4, 2, 1, 1]]),            # MaxPooling Kernel size\n    \"n_freq_downsample\": tune.choice([[8, 4, 2, 1, 1],\n                                      [1, 1, 1, 1, 1]]),            # Interpolation expressivity ratios\n    \"learning_rate\": tune.loguniform(1e-4, 1e-2),                   # Initial Learning rate\n    \"scaler_type\": tune.choice([None]),                             # Scaler type\n    \"max_steps\": tune.choice([1000]),                               # Max number of training iterations\n    \"batch_size\": tune.choice([1, 4, 10]),                          # Number of series in batch\n    \"windows_batch_size\": tune.choice([128, 256, 512]),             # Number of windows in batch\n    \"random_seed\": tune.randint(1, 20),                             # Random seed\n}\n\nconfig_lstm = {\n    \"input_size\": tune.choice([48, 48*2, 48*3]),              # Length of input window\n    \"encoder_hidden_size\": tune.choice([64, 128]),            # Hidden size of LSTM cells\n    \"encoder_n_layers\": tune.choice([2,4]),                   # Number of layers in LSTM\n    \"learning_rate\": tune.loguniform(1e-4, 1e-2),             # Initial Learning rate\n    \"scaler_type\": tune.choice(['robust']),                   # Scaler type\n    \"max_steps\": tune.choice([500, 1000]),                    # Max number of training iterations\n    \"batch_size\": tune.choice([1, 4]),                        # Number of series in batch\n    \"random_seed\": tune.randint(1, 20),                       # Random seed\n}\nnf = NeuralForecast(\n    models=[\n        AutoNHITS(h=48, config=config_nhits, loss=MQLoss(), num_samples=5),\n        AutoLSTM(h=48, config=config_lstm, loss=MQLoss(), num_samples=2), \n    ],\n    freq='H'\n)\n\n\ncv_df = nf.cross_validation(Y_df, n_windows=2)\n\nGlobal seed set to 4\nGlobal seed set to 19\n\n\nThe cv_df object is a new data frame that includes the following columns:\n\nunique_id: identifies each time series\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncv_df.columns = cv_df.columns.str.replace('-median', '')\n\n\ncv_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\nAutoNHITS\nAutoNHITS-lo-90\nAutoNHITS-lo-80\nAutoNHITS-hi-80\nAutoNHITS-hi-90\nAutoLSTM\nAutoLSTM-lo-90\nAutoLSTM-lo-80\nAutoLSTM-hi-80\nAutoLSTM-hi-90\ny\n\n\n\n\n0\nH1\n700\n699\n646.881714\n601.402893\n626.471008\n672.432617\n683.847778\n633.707031\n365.139832\n407.289246\n871.474976\n925.476196\n684.0\n\n\n1\nH1\n701\n699\n635.608643\n595.042908\n612.889771\n669.565979\n679.472900\n632.455017\n365.303131\n406.472992\n869.484985\n922.926514\n619.0\n\n\n2\nH1\n702\n699\n592.663940\n564.124390\n566.502319\n648.286072\n647.859253\n633.002502\n365.147522\n407.174866\n868.677979\n925.269409\n565.0\n\n\n3\nH1\n703\n699\n543.364563\n516.760742\n517.990234\n603.099182\n601.462280\n633.903503\n364.976746\n408.498779\n869.797180\n925.993164\n532.0\n\n\n4\nH1\n704\n699\n498.051178\n461.069489\n474.206360\n540.752563\n555.169739\n634.015991\n363.384155\n408.305298\n870.154297\n920.329224\n495.0\n\n\n\n\n\n\n\n\nfor cutoff in cv_df['cutoff'].unique():\n    StatsForecast.plot(\n        Y_df, \n        cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']), \n        max_insample_length=48 * 4, \n        unique_ids=['H185'],\n        engine='matplotlib'\n    )\n\nNow, let’s evaluate the models’ performance.\n\nfrom datasetsforecast.losses import mse, mae, rmse\nfrom datasetsforecast.evaluation import accuracy\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can also use Mean Average Percentage Error (MAPE), however for granular forecasts, MAPE values are extremely hard to judge and not useful to assess forecasting quality.\n\n\nCreate the data frame with the results of the evaluation of your cross-validation data frame using a Mean Squared Error metric.\n\nevaluation_df = accuracy(cv_df, [mse, mae, rmse], agg_by=['unique_id'])\nevaluation_df['best_model'] = evaluation_df.drop(columns=['metric', 'unique_id']).idxmin(axis=1)\nevaluation_df.head()\n\n\n\n\n\n\n\n\nmetric\nunique_id\nAutoNHITS\nAutoLSTM\nbest_model\n\n\n\n\n0\nmae\nH1\n38.259457\n131.158150\nAutoNHITS\n\n\n1\nmae\nH10\n14.044900\n32.972164\nAutoNHITS\n\n\n2\nmae\nH100\n254.464978\n281.836064\nAutoNHITS\n\n\n3\nmae\nH101\n257.810841\n148.341771\nAutoLSTM\n\n\n4\nmae\nH102\n176.114826\n472.413350\nAutoNHITS\n\n\n\n\n\n\n\nCreate a summary table with a model column and the number of series where that model performs best.\n\nsummary_df = evaluation_df.groupby(['metric', 'best_model']).size().sort_values().to_frame()\n\nsummary_df = summary_df.reset_index()\nsummary_df.columns = ['metric', 'model', 'nr. of unique_ids']\nsummary_df\n\n\n\n\n\n\n\n\nmetric\nmodel\nnr. of unique_ids\n\n\n\n\n0\nmae\nAutoLSTM\n1\n\n\n1\nmse\nAutoLSTM\n1\n\n\n2\nrmse\nAutoLSTM\n1\n\n\n3\nmae\nAutoNHITS\n9\n\n\n4\nmse\nAutoNHITS\n9\n\n\n5\nrmse\nAutoNHITS\n9\n\n\n\n\n\n\n\n\nsummary_df.query('metric == \"mse\"')\n\n\n\n\n\n\n\n\nmetric\nmodel\nnr. of unique_ids\n\n\n\n\n1\nmse\nAutoLSTM\n1\n\n\n4\nmse\nAutoNHITS\n9\n\n\n\n\n\n\n\nYou can further explore your results by plotting the unique_ids where a specific model wins.\n\nnhits_ids = evaluation_df.query('best_model == \"AutoNHITS\" and metric == \"mse\"')['unique_id'].unique()\n\nStatsForecast.plot(Y_df, fcst_df, unique_ids=nhits_ids, engine='matplotlib')",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Neural Advanced Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neural_advanced.html#select-the-best-model-for-every-unique-series",
    "href": "Nixtla/nixtla - neural_advanced.html#select-the-best-model-for-every-unique-series",
    "title": "Nixtla - Neural Advanced Forecast",
    "section": "6. Select the best model for every unique series",
    "text": "6. Select the best model for every unique series\nDefine a utility function that takes your forecast’s data frame with the predictions and the evaluation data frame and returns a data frame with the best possible forecast for every unique_id.\n\ndef get_best_model_forecast(forecasts_df, evaluation_df, metric):\n    df = forecasts_df.set_index('ds', append=True).stack().to_frame().reset_index(level=2) # Wide to long \n    df.columns = ['model', 'best_model_forecast'] \n    df = df.join(evaluation_df.query('metric == @metric').set_index('unique_id')[['best_model']])\n    df = df.query('model.str.replace(\"-lo-90|-hi-90\", \"\", regex=True) == best_model').copy()\n    df.loc[:, 'model'] = [model.replace(bm, 'best_model') for model, bm in zip(df['model'], df['best_model'])]\n    df = df.drop(columns='best_model').set_index('model', append=True).unstack()\n    df.columns = df.columns.droplevel()\n    df = df.reset_index(level=1)\n    return df\n\nCreate your production-ready data frame with the best forecast for every unique_id.\n\nprod_forecasts_df = get_best_model_forecast(fcst_df, evaluation_df, metric='mse')\n\nprod_forecasts_df.head()\n\n\n\n\n\n\n\nmodel\nds\nbest_model\nbest_model-hi-90\nbest_model-lo-90\n\n\nunique_id\n\n\n\n\n\n\n\n\nH1\n749\n550.545288\n658.631592\n491.368347\n\n\nH1\n750\n549.216736\n657.615967\n491.054932\n\n\nH1\n751\n528.075989\n642.255005\n466.917053\n\n\nH1\n752\n486.842255\n611.903809\n418.012115\n\n\nH1\n753\n452.015930\n590.465942\n371.543884\n\n\n\n\n\n\n\nPlot the results.\n\nStatsForecast.plot(Y_df, prod_forecasts_df, level=[90], engine='matplotlib')",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Nixtla - Neural Advanced Forecast"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neural_save_and_load.html",
    "href": "Nixtla/nixtla - neural_save_and_load.html",
    "title": "Neural Save and Load Models",
    "section": "",
    "text": "Saving and loading trained Deep Learning models has multiple valuable uses. These models are often costly to train; storing a pre-trained model can help reduce costs as it can be loaded and reused to forecast multiple times. Moreover, it enables Transfer learning capabilities, consisting of pre-training a flexible model on a large dataset and using it later on other data with little to no training. It is one of the most outstanding 🚀 achievements in Machine Learning 🧠 and has many practical applications.\nIn this notebook we show an example on how to save and load NeuralForecast models.\nThe two methods to consider are: 1. NeuralForecast.save: Saves models into disk, allows save dataset and config. 2. NeuralForecast.load: Loads models from a given path.\nYou can run these experiments using GPU with Google Colab.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Neural Save and Load Models"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neural_save_and_load.html#installing-neuralforecast",
    "href": "Nixtla/nixtla - neural_save_and_load.html#installing-neuralforecast",
    "title": "Neural Save and Load Models",
    "section": "1. Installing NeuralForecast",
    "text": "1. Installing NeuralForecast\n%%capture !pip install neuralforecast",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Neural Save and Load Models"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neural_save_and_load.html#loading-airpassengers-data",
    "href": "Nixtla/nixtla - neural_save_and_load.html#loading-airpassengers-data",
    "title": "Neural Save and Load Models",
    "section": "2. Loading AirPassengers Data",
    "text": "2. Loading AirPassengers Data\nFor this example we will use the classical AirPassenger Data set. Import the pre-processed AirPassenger from utils.\n\nfrom neuralforecast.utils import AirPassengersDF\n\nY_df = AirPassengersDF\nY_df = Y_df.reset_index(drop=True)\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n1.0\n1949-01-31\n112.0\n\n\n1\n1.0\n1949-02-28\n118.0\n\n\n2\n1.0\n1949-03-31\n132.0\n\n\n3\n1.0\n1949-04-30\n129.0\n\n\n4\n1.0\n1949-05-31\n121.0",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Neural Save and Load Models"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neural_save_and_load.html#model-training",
    "href": "Nixtla/nixtla - neural_save_and_load.html#model-training",
    "title": "Neural Save and Load Models",
    "section": "3. Model Training",
    "text": "3. Model Training\nNext, we instantiate and train three models: NBEATS, NHITS, and AutoMLP. The models with their hyperparameters are defined in the models list.\n\nfrom ray import tune\n\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.auto import AutoMLP\nfrom neuralforecast.models import NBEATS, NHITS\n\n\nhorizon = 12\nmodels = [NBEATS(input_size=2 * horizon, h=horizon, max_steps=50),\n          NHITS(input_size=2 * horizon, h=horizon, max_steps=50),\n          ]\n\nGlobal seed set to 1\nGlobal seed set to 1\n\n\n\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(df=Y_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduce the forecasts with the predict method.\n\nY_hat_df = nf.predict().reset_index()\nY_hat_df.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunique_id\nds\nNBEATS\nNHITS\n\n\n\n\n0\n1.0\n1961-01-31\n446.280823\n445.295135\n\n\n1\n1.0\n1961-02-28\n463.121948\n462.276184\n\n\n2\n1.0\n1961-03-31\n470.079895\n475.442902\n\n\n3\n1.0\n1961-04-30\n492.255310\n503.250916\n\n\n4\n1.0\n1961-05-31\n535.243164\n556.968689\n\n\n\n\n\n\n\nWe plot the forecasts for each model. Note how the two NBEATS models are differentiated with a numerical suffix.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nplot_df = pd.concat([Y_df, Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes\n\nplt.figure(figsize = (12, 3))\nplot_df[['y', 'NBEATS', 'NHITS']].plot(linewidth=2)\n\nplt.title('AirPassengers Forecast', fontsize=10)\nplt.ylabel('Monthly Passengers', fontsize=10)\nplt.xlabel('Timestamp [t]', fontsize=10)\nplt.axvline(x=plot_df.index[-horizon], color='k', linestyle='--', linewidth=2)\nplt.legend(prop={'size': 10})\n\n&lt;Figure size 1200x300 with 0 Axes&gt;",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Neural Save and Load Models"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neural_save_and_load.html#save-models",
    "href": "Nixtla/nixtla - neural_save_and_load.html#save-models",
    "title": "Neural Save and Load Models",
    "section": "4. Save models",
    "text": "4. Save models\nTo save all the trained models use the save method. This method will save both the hyperparameters and the learnable weights (parameters).\nThe save method has the following inputs:\n\npath: directory where models will be saved.\nmodel_index: optional list to specify which models to save. For example, to only save the NHITS model use model_index=[2].\noverwrite: boolean to overwrite existing files in path. When True, the method will only overwrite models with conflicting names.\nsave_dataset: boolean to save Dataset object with the dataset.\n\n\nnf.save(path='./checkpoints/test_run/',\n        model_index=None, \n        overwrite=True,\n        save_dataset=True)\n\nFor each model, two files are created and stored:\n\n[model_name]_[suffix].ckpt: Pytorch Lightning checkpoint file with the model parameters and hyperparameters.\n[model_name]_[suffix].pkl: Dictionary with configuration attributes.\n\nWhere model_name corresponds to the name of the model in lowercase (eg. nhits). We use a numerical suffix to distinguish multiple models of each class. In this example the names will be automlp_0, nbeats_0, and nhits_0.\n\n\n\n\n\n\nImportant\n\n\n\nThe Auto models will be stored as their base model. For example, the AutoMLP trained above is stored as an MLP model, with the best hyparparameters found during tuning.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Neural Save and Load Models"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neural_save_and_load.html#load-models",
    "href": "Nixtla/nixtla - neural_save_and_load.html#load-models",
    "title": "Neural Save and Load Models",
    "section": "5. Load models",
    "text": "5. Load models\nLoad the saved models with the load method, specifying the path, and use the new nf2 object to produce forecasts.\n\nnf2 = NeuralForecast.load(path='./checkpoints/test_run/')\nY_hat_df = nf2.predict().reset_index()\nY_hat_df.head()\n\nGlobal seed set to 1\nGlobal seed set to 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunique_id\nds\nNHITS\nNBEATS\n\n\n\n\n0\n1.0\n1961-01-31\n445.295135\n446.280823\n\n\n1\n1.0\n1961-02-28\n462.276184\n463.121948\n\n\n2\n1.0\n1961-03-31\n475.442902\n470.079895\n\n\n3\n1.0\n1961-04-30\n503.250916\n492.255310\n\n\n4\n1.0\n1961-05-31\n556.968689\n535.243164\n\n\n\n\n\n\n\nFinally, plot the forecasts to confirm they are identical to the original forecasts.\n\nplot_df = pd.concat([Y_df, Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes\n\nplt.figure(figsize = (12, 3))\nplot_df[['y', 'NBEATS', 'NHITS']].plot(linewidth=2)\n\nplt.title('AirPassengers Forecast', fontsize=10)\nplt.ylabel('Monthly Passengers', fontsize=10)\nplt.xlabel('Timestamp [t]', fontsize=10)\nplt.axvline(x=plot_df.index[-horizon], color='k', linestyle='--', linewidth=2)\nplt.legend(prop={'size': 10})\nplt.show()\n\n&lt;Figure size 1200x300 with 0 Axes&gt;",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Neural Save and Load Models"
    ]
  },
  {
    "objectID": "Nixtla/nixtla - neural_save_and_load.html#references",
    "href": "Nixtla/nixtla - neural_save_and_load.html#references",
    "title": "Neural Save and Load Models",
    "section": "References",
    "text": "References\nhttps://pytorch-lightning.readthedocs.io/en/stable/common/checkpointing_basic.html\nOreshkin, B. N., Carpov, D., Chapados, N., & Bengio, Y. (2019). N-BEATS: Neural basis expansion analysis for interpretable time series forecasting. ICLR 2020\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023.",
    "crumbs": [
      "Blog",
      "Nixtla",
      "Neural Save and Load Models"
    ]
  },
  {
    "objectID": "xgboost.html",
    "href": "xgboost.html",
    "title": "XGBoost",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom fastai.imports import *\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nimport zipfile,kaggle\nimport os\n\n\ncolor_pal = sns.color_palette()\nplt.style.use('fivethirtyeight')\n\n\nfrom nbdevAuto.functions import * \nimport nbdevAuto.functions\n\n\nname = 'hourly-energy-consumption'\npath = Path(f'Data/{name}')\nuser = 'robikscube'\nkaggle_dataset_download(user = user, \n                         name = name)\n\n\ndf = pd.read_csv(f'{path}/PJME_hourly.csv')\ndf = df.set_index('Datetime')\ndf.index = pd.to_datetime(df.index)\n\n\ndf.plot(style='.',\n        figsize=(15, 5),\n        color=color_pal[0],\n        title='PJME Energy Use in MW')\nplt.show()\n\n\n\n\n\n\n\n\n\nTrain / Test Split\n\ntrain = df.loc[df.index &lt; '01-01-2015']\ntest = df.loc[df.index &gt;= '01-01-2015']\n\nfig, ax = plt.subplots(figsize=(15, 5))\ntrain.plot(ax=ax, label='Training Set', title='Data Train/Test Split')\ntest.plot(ax=ax, label='Test Set')\nax.axvline('01-01-2015', color='black', ls='--')\nax.legend(['Training Set', 'Test Set'])\nplt.show()\n\n\n\n\n\n\n\n\n\ndf.loc[(df.index &gt; '01-01-2010') & (df.index &lt; '01-08-2010')] \\\n    .plot(figsize=(15, 5), title='Week Of Data')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFeature Creation\n\ndef create_features(df):\n    \"\"\"\n    Create time series features based on time series index.\n    \"\"\"\n    df = df.copy()\n    df['hour'] = df.index.hour\n    df['dayofweek'] = df.index.dayofweek\n    df['quarter'] = df.index.quarter\n    df['month'] = df.index.month\n    df['year'] = df.index.year\n    df['dayofyear'] = df.index.dayofyear\n    df['dayofmonth'] = df.index.day\n    df['weekofyear'] = df.index.isocalendar().week\n    return df\n\ndf = create_features(df)\n\n\n\nVisualize our Feature / Target Relationship\n\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.boxplot(data=df, x='hour', y='PJME_MW')\nax.set_title('MW by Hour')\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.boxplot(data=df, x='month', y='PJME_MW', palette='Blues')\nax.set_title('MW by Month')\nplt.show()\n\n/tmp/ipykernel_109462/958582662.py:3: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(data=df, x='month', y='PJME_MW', palette='Blues')\n\n\n\n\n\n\n\n\n\n\n\nCreate our Model\n\ntrain = create_features(train)\ntest = create_features(test)\n\nFEATURES = ['dayofyear', 'hour', 'dayofweek', 'quarter', 'month', 'year']\nTARGET = 'PJME_MW'\n\nX_train = train[FEATURES]\ny_train = train[TARGET]\n\nX_test = test[FEATURES]\ny_test = test[TARGET]\n\n\nreg = xgb.XGBRegressor(base_score=0.5, booster='gbtree',    \n                       n_estimators=1000,\n                       early_stopping_rounds=50,\n                       objective='reg:linear',\n                       max_depth=3,\n                       learning_rate=0.01)\nreg.fit(X_train, y_train,\n        eval_set=[(X_train, y_train), (X_test, y_test)],\n        verbose=100)\n\n[0] validation_0-rmse:32605.13970   validation_1-rmse:31657.15729\n\n\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [17:58:09] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n\n\n[100]   validation_0-rmse:12584.35462   validation_1-rmse:11747.28803\n[200]   validation_0-rmse:5837.33066    validation_1-rmse:5363.58554\n[300]   validation_0-rmse:3923.28511    validation_1-rmse:4020.48045\n[400]   validation_0-rmse:3447.54638    validation_1-rmse:3860.60088\n[500]   validation_0-rmse:3288.19208    validation_1-rmse:3816.37862\n[600]   validation_0-rmse:3206.55619    validation_1-rmse:3779.04119\n[700]   validation_0-rmse:3153.61368    validation_1-rmse:3754.45684\n[800]   validation_0-rmse:3114.34038    validation_1-rmse:3738.38209\n[900]   validation_0-rmse:3084.39550    validation_1-rmse:3730.01893\n[989]   validation_0-rmse:3059.85847    validation_1-rmse:3727.94591\n\n\nXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=50,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1000, n_jobs=None,\n             num_parallel_tree=None, objective='reg:linear', ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=50,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1000, n_jobs=None,\n             num_parallel_tree=None, objective='reg:linear', ...)\n\n\n\n\nFeature Importance\n\nfi = pd.DataFrame(data=reg.feature_importances_,\n             index=reg.feature_names_in_,\n             columns=['importance'])\nfi.sort_values('importance').plot(kind='barh', title='Feature Importance')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nForecast on Test\n\ntest['prediction'] = reg.predict(X_test)\ndf = df.merge(test[['prediction']], how='left', left_index=True, right_index=True)\nax = df[['PJME_MW']].plot(figsize=(15, 5))\ndf['prediction'].plot(ax=ax, style='.')\nplt.legend(['Truth Data', 'Predictions'])\nax.set_title('Raw Dat and Prediction')\nplt.show()\n\n\n\n\n\n\n\n\n\nax = df.loc[(df.index &gt; '04-01-2018') & (df.index &lt; '04-18-2018')]['PJME_MW'] \\\n    .plot(figsize=(15, 5), title='Week Of Data')\ndf.loc[(df.index &gt; '04-01-2018') & (df.index &lt; '04-18-2018')]['prediction'] \\\n    .plot(style='.')\nplt.legend(['Truth Data','Prediction'])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nScore (RMSE)\n\nscore = np.sqrt(mean_squared_error(test['PJME_MW'], test['prediction']))\nprint(f'RMSE Score on Test set: {score:0.2f}')\n\nRMSE Score on Test set: 3726.80\n\n\n\n\nCalculate Error\n\nLook at the worst and best predicted days\n\n\ntest['error'] = np.abs(test[TARGET] - test['prediction'])\ntest['date'] = test.index.date\ntest.groupby(['date'])['error'].mean().sort_values(ascending=False).head(10)\n\ndate\n2016-08-13    12879.484619\n2016-08-14    12772.887207\n2015-02-20    11186.031494\n2016-09-09    10966.513102\n2016-09-10    10889.102214\n2018-01-06    10642.975830\n2016-08-12    10041.172689\n2015-02-21     9988.168783\n2015-02-16     9900.809326\n2018-01-07     9852.571370\nName: error, dtype: float64\n\n\n\n\nNext Steps\n\nMore robust cross validation\nAdd more features (weather forecast, holidays)\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "XGBoost"
    ]
  },
  {
    "objectID": "FFT/fft_analysis.html",
    "href": "FFT/fft_analysis.html",
    "title": "FFT analysis",
    "section": "",
    "text": "import pywt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(‘&lt;file’)\n\ncycles = 26\nweeks = cycles * 2\nresolution = 7\n\n\nnp.random.seed(0)\nt = np.linspace(0, cycles * np.pi, weeks * resolution)\n\nwaveform = np.random.normal(scale=0.5, size=len(t)) + 0.5 * np.sign(np.sin(0.67 * t))\ny = np.sin(t) + 0.3*np.sin(4.71*t) + waveform\n\n\n# Create a Pandas DataFrame\ndf = pd.DataFrame({'y': y})\ndf['unique_id'] = 1\n\n\nrng = pd.date_range('04/01/2021', periods=df.shape[0], freq='D')\ndf['ds'] = rng\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 364 entries, 0 to 363\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   y          364 non-null    float64       \n 1   unique_id  364 non-null    int64         \n 2   ds         364 non-null    datetime64[ns]\ndtypes: datetime64[ns](1), float64(1), int64(1)\nmemory usage: 8.7 KB\n\n\n\ndf_plot = df.copy()\ndf_plot.rename(columns={'y': 'final'}, inplace=True)\ndf_plot['unique_id'] = 'final'\ndf_plot['first'] = np.sin(t)\ndf_plot['second'] = 0.3*np.sin(5*t)\ndf_plot['noise'] = np.random.normal(scale=0.2, size=len(t))\ndf_plot.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 364 entries, 0 to 363\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   final      364 non-null    float64       \n 1   unique_id  364 non-null    object        \n 2   ds         364 non-null    datetime64[ns]\n 3   first      364 non-null    float64       \n 4   second     364 non-null    float64       \n 5   noise      364 non-null    float64       \ndtypes: datetime64[ns](1), float64(4), object(1)\nmemory usage: 17.2+ KB\n\n\n\nimport altair as alt\n\ndef long_form(df_plot):\n    return df_plot.melt('ds', var_name='unique_id', value_name='price')\n\ndef altair_plot(df_plot): \n    highlight = alt.selection_point(on='mouseover', fields=['unique_id'], nearest=True)\n\n    base = alt.Chart(df_plot).encode(\n        x='ds:T',\n        y='price:Q',\n        color='unique_id:N'\n    )\n\n    points = base.mark_circle().encode(\n        opacity=alt.value(0)\n    ).add_params(\n        highlight\n    ).properties(\n        width=1000\n    )\n\n    lines = base.mark_line().encode(\n        size=alt.condition(~highlight, alt.value(1), alt.value(3))\n    )\n\n    return points + lines\n\ndf_plot = long_form(df_plot)\naltair_plot(df_plot)\n\n\n\n\n\n\n\n\ndef wavelet_transform(data):\n    transformed_data = []\n    for column in data.columns:\n        coeffs = pywt.wavedec(data[column], wavelet='db5', level=5)\n        print(coeffs)\n        transformed_data.extend(coeffs)\n        \n    return transformed_data\n\n\ny = pd.DataFrame(df['y'])\n#y['second'] = waveform\ny.columns\n\nIndex(['y'], dtype='object')\n\n\n\nX_wavelet = wavelet_transform(y)\n\n[array([ 8.56037363,  8.28015354,  8.60255108,  8.33149856,  8.73389978,\n        7.74524132, 10.2828375 ,  2.1141299 , -1.18795588,  0.44417908,\n        1.19875776,  0.03718912, -0.69022103,  0.39696723, -0.92086484,\n        0.09806285, -0.67200673,  0.71120904, -4.05185514, -4.04591833]), array([ 0.01153199, -0.25139026,  0.08876349,  1.32828559,  0.07565752,\n        2.63863482,  4.79537636, -1.24767023, -5.54718816, -3.35555816,\n        2.36944361,  4.04646868,  0.87187567, -1.68265763,  1.33179388,\n       -0.40272217,  0.55708821, -1.64304018,  2.00581003, -2.03032757]), array([-0.08897953, -0.4812266 , -0.50538909, -0.10168253, -2.61150232,\n        2.9547261 , -2.29803229, -0.09763533,  1.47903615, -2.54419368,\n        3.62531102, -2.47650808,  2.5678546 , -1.85454489, -0.19285758,\n        1.37250574, -1.5142747 ,  2.39190732, -3.19870564,  2.72915131,\n       -3.41889537,  1.28809091,  0.52633798, -1.98882932,  3.88974806,\n       -5.33502469,  0.91456006,  0.25339818,  3.80649291, -3.62508319,\n        2.3555905 ]), array([ 0.04489604,  1.21115553,  0.30484212,  0.57478063, -0.10751276,\n       -0.65589434,  1.54087004,  0.69994033,  0.84825601, -0.01101841,\n        0.19981331,  0.33554834, -0.29570558,  1.92260008, -0.8211238 ,\n        0.16218484,  1.10221247, -0.2672678 , -0.39443262, -0.74778196,\n       -0.16734269, -1.55505881, -0.16445345, -1.09477366, -0.2275783 ,\n       -0.32740859,  0.09258987,  1.32728503, -1.59283858,  0.0304181 ,\n       -0.04143611, -0.06468654,  0.42971042,  0.40445104,  1.32661766,\n       -0.68588646,  0.56466584,  0.13326441, -0.17115911,  0.37710454,\n       -0.13755441,  1.21737271, -0.55018683, -0.22723865, -0.66991994,\n        0.08144042, -0.09456927, -0.88110325,  0.89665126, -1.27621373,\n        2.02092779, -0.82140168, -0.28145005]), array([ 0.08743568, -0.52802978,  0.76003102, -0.87477212, -0.2137142 ,\n       -0.08251688,  0.22470448, -0.24951939, -0.41966809,  0.24968052,\n       -0.82965552,  0.11282229,  0.41979293, -1.62165521,  0.63917665,\n       -0.05349559, -0.81191952,  0.47499175,  0.4483338 , -0.51529357,\n       -0.03268174,  0.81509277, -0.72212797,  0.94142773,  0.79246556,\n       -0.66823467, -0.04388327, -0.35251386, -0.96690333,  0.76438364,\n        0.61711505, -0.57417453, -0.24123739,  0.47541282, -1.33055547,\n       -0.04216579,  0.49976061, -0.04719167, -1.45547177,  1.4612047 ,\n       -1.09233401, -0.79218819,  1.4451979 , -0.52280188, -0.51661146,\n        0.07177687, -0.06904092, -1.00303966,  0.54283863, -0.20346968,\n       -0.44556147,  1.01559998,  0.21400751, -1.02416331,  1.15926639,\n       -0.56999735,  0.65632908,  0.4355802 ,  0.48993803, -1.39774935,\n        0.38826329, -0.33822322, -0.40172831,  0.53523861, -0.09876577,\n       -0.53922567, -0.34208739,  0.67681303,  0.09242104, -0.10293512,\n        0.68052398, -1.18565793, -0.6305652 , -0.08625569, -1.3032623 ,\n       -0.4577607 ,  0.47035347,  0.05809923, -0.89829346,  0.82602796,\n       -1.07480685, -0.05684645,  0.80296313,  0.22276104, -0.67154841,\n        0.41031737, -0.54248432, -1.24758935,  0.81923313, -0.76539182,\n       -0.06074858,  0.97558165, -0.34115139, -0.11998139,  0.21528626,\n       -0.60822975,  0.00416824]), array([ 7.82169804e-02, -2.12160997e-02, -1.02848006e-01,  1.11393225e+00,\n       -2.69082443e-01, -1.98484826e-01, -1.71201044e-01,  1.81663044e-01,\n        3.49069262e-01,  4.45396733e-01, -4.88410641e-01,  1.15788613e-01,\n        1.24733049e+00,  4.99264605e-01,  2.69837665e-01, -3.29457397e-01,\n       -1.78566872e-01,  5.62829176e-01,  1.68993607e-02, -4.34187851e-01,\n        3.75604438e-02,  1.78940706e-01, -1.31556512e+00,  1.27625750e-01,\n       -9.41784192e-01, -2.90424465e-01, -2.28643513e-01,  4.58050195e-01,\n       -4.08033755e-01, -9.86955101e-02, -5.20036649e-02, -3.78122379e-04,\n        9.93916246e-01, -6.37854384e-01, -6.25300155e-01,  2.95068950e-01,\n        4.62356890e-01,  6.19277900e-01, -3.45611868e-02,  1.00372393e-02,\n       -5.87213878e-01, -2.39389518e-02,  1.44606100e+00, -6.61582418e-01,\n        1.63701508e-01, -8.32121441e-01, -3.99609990e-01, -7.81021937e-02,\n       -3.17109338e-01, -6.62768834e-01,  8.28543671e-01,  1.12792812e-01,\n       -1.05304001e+00, -2.86248468e-01,  7.85163687e-01, -1.14392546e-01,\n       -3.22477044e-01, -9.26960676e-01,  1.51314348e-01,  3.69942059e-01,\n       -1.91301182e-01,  4.69421895e-01, -7.35694506e-01,  3.20851631e-01,\n       -7.78526682e-01,  3.67410679e-02, -5.64428095e-01, -2.68241703e-01,\n       -2.10079402e-03,  3.51694972e-01, -1.49792951e-01, -2.97840420e-01,\n        4.72250268e-01, -9.50879824e-01, -5.42413251e-01, -1.15215252e-01,\n       -8.46279893e-01,  5.97270295e-01,  4.20475483e-01,  1.18398345e-01,\n       -2.68442902e-01, -5.38487858e-01, -1.44555769e-01, -7.29126413e-01,\n        5.81750516e-01,  1.31050332e-01,  4.13242858e-01, -1.69833616e-01,\n       -5.93391260e-01,  2.54453871e-01, -2.24416337e-01,  5.22312215e-01,\n        9.80914799e-01, -1.45058262e-02, -9.80040422e-01, -7.46969348e-01,\n       -7.80924496e-02,  6.39680098e-01,  1.83881590e-01,  2.19077815e-01,\n       -2.97418275e-01,  4.84247670e-01, -1.16957224e-02,  8.16785915e-01,\n        5.63532338e-03, -9.60587176e-02,  3.83645651e-01,  2.16475782e-02,\n       -1.36875879e-01,  5.94808227e-01,  5.22964942e-03, -2.75391075e-01,\n       -6.37982756e-01,  8.51258601e-03, -3.16836196e-01, -6.85667705e-01,\n        1.08353986e-02, -7.82748435e-01,  4.28636022e-01,  1.03055876e+00,\n       -7.63941608e-01,  3.20021138e-01,  6.56538023e-02,  7.50973676e-01,\n       -5.42819573e-01,  6.88499408e-01, -2.69482031e-01,  4.58058772e-01,\n       -3.65725409e-01, -7.34325562e-02, -7.43466155e-01,  6.62097224e-02,\n       -7.63881741e-01,  7.13215259e-02, -7.17346313e-02,  3.79695244e-01,\n        1.97768780e+00, -6.49823532e-01,  3.26231535e-01, -6.76495937e-01,\n       -1.04392496e+00, -4.81300683e-01, -4.17198698e-01, -4.93617859e-01,\n       -4.36926650e-01, -3.29281763e-01,  1.40802817e+00,  1.33180958e-01,\n       -4.08391392e-01, -2.60750946e-01, -7.00602812e-01, -5.40610047e-01,\n        6.22860442e-01, -2.48715379e-01,  6.80335666e-01, -2.22681683e-01,\n       -5.17525473e-01,  5.61675223e-01, -3.38477480e-01,  1.43883793e-01,\n       -6.74863552e-02,  3.17695822e-01, -7.23998209e-02, -1.88063003e-01,\n       -1.19267630e+00,  2.18871157e-01,  9.27703437e-01, -1.90193785e-01,\n       -1.21027604e+00, -3.24657030e-01, -3.35131838e-01,  3.43673865e-01,\n        2.20526430e-01, -2.57544687e-01,  1.10155401e-01, -1.05068944e-01,\n        8.65358978e-01,  2.03499300e-01, -7.50242361e-02, -3.86384134e-01,\n       -7.66133144e-03,  4.64230598e-01, -1.63102790e-01, -3.32783358e-01,\n        1.55127572e-01,  2.00635763e-01])]\n\n\n\nX_wavelet[0]\n\narray([ 1.69782638e+00,  2.86633130e+00,  2.19476886e+00,  1.17397138e+00,\n        2.01586836e+00,  2.70074412e+00,  1.98287045e+00,  2.53977548e+00,\n        1.91548731e+00,  2.07104557e+00,  1.49206799e+00,  6.24585355e-01,\n        3.63488119e-03, -1.23050798e+00, -2.22516777e+00, -1.45626892e+00,\n       -1.57360056e+00, -6.70425537e-01,  3.60216320e-01,  3.92006830e-01,\n       -6.67448566e-01,  1.20076474e+00,  1.34902772e+00, -8.80108851e-01,\n       -8.48793754e-01,  9.86961240e-01, -1.00889684e+00, -9.96228925e-01,\n       -7.58473590e-01, -1.09936767e+00, -4.74542101e-01,  1.07154093e+00,\n        6.71358965e-01,  1.41619721e+00,  1.77307686e+00,  7.40809286e-02,\n        5.62297202e-02,  1.48342153e-02, -7.92568112e-01, -6.72164798e-01,\n       -1.47818506e+00, -2.98386840e+00, -2.19059661e+00, -1.95613078e+00,\n       -2.01711333e+00, -7.02480981e-04,  2.58409010e+00,  1.03976676e+00,\n        2.38187317e+00,  2.73694743e+00,  1.94806995e+00,  2.13110002e+00,\n        1.41275222e+00,  6.61209183e-01, -6.26994065e-01, -3.83339616e-01,\n       -2.24552771e+00, -1.36530903e+00,  5.20891491e-02, -9.63966984e-01,\n        4.33903511e-01,  8.65625523e-01,  5.39747879e-01,  1.31753836e+00,\n        5.60365478e-01, -3.40131488e-01,  1.80168614e-01,  1.10758938e+00,\n       -6.84122197e-01, -3.34468694e-01, -9.67974799e-01, -5.68087480e-01,\n        5.62314033e-01, -1.94723399e-01,  5.90536838e-01,  3.50383895e+00,\n        1.71759551e+00,  9.24381133e-02,  8.06891772e-01,  1.74700310e-01,\n       -1.42390185e+00, -5.70536402e-01, -2.47344630e+00, -2.75288846e+00,\n       -7.27528289e-01, -2.10474614e+00, -2.16611230e+00,  6.83276964e-01,\n        1.45111868e+00,  2.05263083e+00,  2.10768670e+00,  1.71995702e+00,\n        1.28707724e+00,  1.71366022e+00, -3.45686267e-01, -5.42679676e-01,\n       -5.36517797e-01, -1.64763345e+00, -2.23588848e+00, -1.62481503e+00,\n       -1.88789349e+00, -8.67276586e-01,  1.49540135e+00,  3.48492854e-01,\n        8.75034108e-01,  1.48134970e+00, -3.57533461e-01, -3.50851776e-01,\n        1.21067431e+00,  3.34407247e-01, -7.22731370e-01, -7.83948722e-01,\n       -8.00539078e-03, -6.27089015e-01,  2.58212559e-01, -2.01821012e-01,\n        1.25658523e+00,  2.32748366e+00,  3.63168070e-01,  5.26949190e-01,\n        3.16572282e-01,  6.48425185e-03, -7.71003662e-01, -1.01376207e+00,\n       -2.32236401e+00, -2.44432130e+00, -1.75157248e+00, -2.24439864e+00,\n       -1.59672927e+00,  5.30976701e-01,  1.14611200e+00,  2.04313147e+00,\n        1.41599179e+00,  1.26529130e+00,  1.84280178e+00,  6.76185118e-01,\n        4.80539100e-01,  3.72875688e-01, -4.81506716e-01, -2.27614039e+00,\n       -1.94089445e+00, -1.49267556e+00, -2.35262541e+00,  1.51879060e-01,\n        4.85279258e-02,  6.93119371e-01,  7.50628470e-01,  1.58969214e+00,\n       -2.73848686e-01,  1.46411682e+00,  5.94325381e-01,  1.20018857e-01,\n        1.42775133e-01, -9.01552690e-01, -8.36577719e-01,  1.87829611e-01,\n       -6.83570959e-01,  9.44528652e-02,  1.85610930e+00,  1.72844555e+00,\n        9.30680887e-02,  1.00389764e+00,  2.83265909e-01, -8.17614709e-01,\n       -4.59295934e-01, -2.21982178e+00, -2.08909812e+00, -1.23495410e+00,\n       -1.92722736e+00, -2.09438316e+00, -1.27483495e+00,  2.47151395e-02,\n        8.62875878e-01,  3.14299902e+00,  1.53494389e+00,  1.35202124e+00,\n        1.84182300e+00,  1.45584181e+00,  1.20316826e-01,  3.96518649e-01,\n       -7.89934160e-01, -2.90731899e+00, -2.14249061e+00, -1.76722203e+00,\n       -1.01628309e+00,  1.55622881e-01])\n\n\n\nplt.plot(X_wavelet[0])\nplt.plot(X_wavelet[1])\nplt.plot(X_wavelet[2])\nplt.plot(X_wavelet[3])\n\n\n\n\n\n\n\n\n\nX_wavelet\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[56], line 1\n----&gt; 1 X_wavelet.info()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'info'\n\n\n\n\nsplit_point = int(len(data) * 0.8)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[54], line 1\n----&gt; 1 split_point = int(len(data) * 0.8)\n\nNameError: name 'data' is not defined\n\n\n\n\nX_train, X_data = X_wavelet[:split_point], X_wavelet[split_point:, :]\ny_train, y_test = y.iloc[:split_point], y.iloc[split_point:]\n\n\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "FFT",
      "FFT analysis"
    ]
  },
  {
    "objectID": "xplainable.html",
    "href": "xplainable.html",
    "title": "Xplainable",
    "section": "",
    "text": "import xplainable as xplainable\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import HTML\nfrom pathlib import Path\n\n\nfrom nbdevAuto.functions import * \nimport nbdevAuto.functions\n\n\nname = 'titanic'\npath = Path(f'Data/{name}')\nkaggle_download(name = name)\n\nfile exists\n\n\n\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\nfrom xplainable.core.models import PartitionedClassifier\nfrom xplainable.core.models import XClassifier\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\n# Load your data\n\ntrain, test = train_test_split(df, test_size=0.2)\n\n# Train your model (this will open an embedded gui)\npartitioned_model = PartitionedClassifier(partition_on='partition_column')\n\n# Iterate over the unique values in the partition column\nfor partition in train['partition_column'].unique():\n      # Get the data for the partition\n      part = train[train['partition_column'] == partition]\n      x_train, y_train = part.drop('target', axis=1), part['target']\n\n      # Fit the embedded model\n      model = XClassifier()\n      model.fit(x, y)\n\n      # Add the model to the partitioned model\n      partitioned_model.add_partition(model, partition)\n\n# Prepare the test data\nx_test, y_test = test.drop('target', axis=1), test['target']\n\n# Predict on the partitioned model\ny_pred = partitioned_model.predict(x_test)\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802, in Index.get_loc(self, key, method, tolerance)\n   3801 try:\n-&gt; 3802     return self._engine.get_loc(casted_key)\n   3803 except KeyError as err:\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/pandas/_libs/index.pyx:138, in pandas._libs.index.IndexEngine.get_loc()\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/pandas/_libs/index.pyx:165, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:5745, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:5753, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'partition_column'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nCell In[7], line 10\n      7 partitioned_model = PartitionedClassifier(partition_on='partition_column')\n      9 # Iterate over the unique values in the partition column\n---&gt; 10 for partition in train['partition_column'].unique():\n     11       # Get the data for the partition\n     12       part = train[train['partition_column'] == partition]\n     13       x_train, y_train = part.drop('target', axis=1), part['target']\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/pandas/core/frame.py:3807, in DataFrame.__getitem__(self, key)\n   3805 if self.columns.nlevels &gt; 1:\n   3806     return self._getitem_multilevel(key)\n-&gt; 3807 indexer = self.columns.get_loc(key)\n   3808 if is_integer(indexer):\n   3809     indexer = [indexer]\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804, in Index.get_loc(self, key, method, tolerance)\n   3802     return self._engine.get_loc(casted_key)\n   3803 except KeyError as err:\n-&gt; 3804     raise KeyError(key) from err\n   3805 except TypeError:\n   3806     # If we have a listlike key, _check_indexing_error will raise\n   3807     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3808     #  the TypeError.\n   3809     self._check_indexing_error(key)\n\nKeyError: 'partition_column'\n\n\n\n\npp = xp.Preprocessor()\npp.preprocess(df)\n\n\ndf = pd.read_csv('preprocessed_1695571534.csv')\ndf.head()\n\n\n# Train a model\nmodel = xp.classifier(df)\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Xplainable"
    ]
  },
  {
    "objectID": "darts.html",
    "href": "darts.html",
    "title": "Darts",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy.testing as testing\nfrom darts import TimeSeries\nfrom darts.datasets import AirPassengersDataset\nseries = AirPassengersDataset().load()\nseries.plot()\nseries1, series2 = series.split_before(0.75)\nseries1.plot()\nseries2.plot()\nseries1, series2 = series[:-36], series[-36:]\nseries1.plot()\nseries2.plot()\nseries_noise = TimeSeries.from_times_and_values(\n    series.time_index, np.random.randn(len(series))\n)\n(series / 2 + 20 * series_noise - 10).plot()\n(series / 50).stack(series_noise).plot()\nseries.map(np.log).plot()\nseries.map(lambda ts, x: x / ts.days_in_month).plot()\n(series / 20).add_datetime_attribute(\"month\").plot()\n(series / 200).add_holidays(\"US\").plot()\nseries.diff().plot()\nfrom darts.utils.missing_values import fill_missing_values\nvalues = np.arange(50, step=0.5)\nvalues[10:30] = np.nan\nvalues[60:95] = np.nan\nseries_ = TimeSeries.from_values(values)\n\n(series_ - 10).plot(label=\"with missing values (shifted below)\")\nfill_missing_values(series_).plot(label=\"without missing values\")\ntrain, val = series.split_before(pd.Timestamp(\"19580101\"))\ntrain.plot(label=\"training\")\nval.plot(label=\"validation\")",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#training-forecasting-models-and-making-predictions",
    "href": "darts.html#training-forecasting-models-and-making-predictions",
    "title": "Darts",
    "section": "Training forecasting models and making predictions",
    "text": "Training forecasting models and making predictions\n\nfrom darts.models import NaiveSeasonal\n\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/neptune/internal/backends/hosted_client.py:51: NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n  from neptune.version import version as neptune_client_version\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/pytorch_lightning/loggers/neptune.py:51: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n  import neptune.new as neptune\n\n\n\nnaive_model = NaiveSeasonal(K=1)\nnaive_model.fit(train)\nnaive_forecast = naive_model.predict(36)\n\nseries.plot(label=\"actual\")\nnaive_forecast.plot(label=\"naive forecast (K=1)\")",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#inspect-seasonality",
    "href": "darts.html#inspect-seasonality",
    "title": "Darts",
    "section": "Inspect Seasonality",
    "text": "Inspect Seasonality\n\nfrom darts.utils.statistics import plot_acf, check_seasonality\n\n\nplot_acf(train, m=12, alpha=0.05)\n\n\n\n\n\n\n\n\n\nfor m in range(2, 25):\n    is_seasonal, period = check_seasonality(train, m=m, alpha=0.05)\n    if is_seasonal:\n        print(\"There is seasonality of order {}.\".format(period))\n\nThere is seasonality of order 12.\n\n\n\nseasonal_model = NaiveSeasonal(K=12)\nseasonal_model.fit(train)\nseasonal_forecast = seasonal_model.predict(36)\n\nseries.plot(label=\"actual\")\nseasonal_forecast.plot(label=\"naive forecast (K=12)\")\n\n\n\n\n\n\n\n\n\nfrom darts.models import NaiveDrift\n\n\ndrift_model = NaiveDrift()\ndrift_model.fit(train)\ndrift_forecast = drift_model.predict(36)\n\ncombined_forecast = drift_forecast + seasonal_forecast - train.last_value()\n\nseries.plot()\ncombined_forecast.plot(label=\"combined\")\ndrift_forecast.plot(label=\"drift\")\n\n\n\n\n\n\n\n\n\nfrom darts.metrics import mape\n\n\nprint(\n    \"Mean absolute percentage error for the combined naive drift + seasonal: {:.2f}%.\".format(\n        mape(series, combined_forecast)\n    )\n)\n\nMean absolute percentage error for the combined naive drift + seasonal: 5.66%.\n\n\n\nfrom darts.models import ExponentialSmoothing, TBATS, AutoARIMA, Theta\n\n\ndef eval_model(model):\n    model.fit(train)\n    forecast = model.predict(len(val))\n    print(\"model {} obtains MAPE: {:.2f}%\".format(model, mape(val, forecast)))\n    forecast.plot(label = str(model))\n\nseries[-40:].plot(label=\"Orginal\")\neval_model(ExponentialSmoothing())\neval_model(TBATS())\neval_model(AutoARIMA())\neval_model(Theta())\n\nmodel ExponentialSmoothing() obtains MAPE: 5.11%\nmodel TBATS() obtains MAPE: 5.87%\nmodel AutoARIMA() obtains MAPE: 11.65%\nmodel Theta() obtains MAPE: 8.15%",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#searching-for-hyper-parameters-with-the-theta-method",
    "href": "darts.html#searching-for-hyper-parameters-with-the-theta-method",
    "title": "Darts",
    "section": "Searching for hyper-parameters with the Theta method¶",
    "text": "Searching for hyper-parameters with the Theta method¶\n\n# Search for the best theta parameter, by trying 50 different values\nthetas = 2 - np.linspace(-10, 10, 50)\n\nbest_mape = float(\"inf\")\nbest_theta = 0\n\nfor theta in thetas:\n    model = Theta(theta)\n    model.fit(train)\n    pred_theta = model.predict(len(val))\n    res = mape(val, pred_theta)\n\n    if res &lt; best_mape:\n        best_mape = res\n        best_theta = theta\n\n\nbest_theta_model = Theta(best_theta)\nbest_theta_model.fit(train)\npred_best_theta = best_theta_model.predict(len(val))\n\nprint(\n    \"The MAPE is: {:.2f}, with theta = {}.\".format(\n        mape(val, pred_best_theta), best_theta\n    )\n)\n\nThe MAPE is: 4.40, with theta = -3.5102040816326543.\n\n\n\ntrain.plot(label=\"train\")\nval.plot(label=\"true\")\npred_best_theta.plot(label=\"prediction\")",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#backtesting-simulate-historical-forecasting",
    "href": "darts.html#backtesting-simulate-historical-forecasting",
    "title": "Darts",
    "section": "Backtesting: simulate historical forecasting",
    "text": "Backtesting: simulate historical forecasting\n\nhistorical_fcast_theta = best_theta_model.historical_forecasts(\n    series, start=0.6, forecast_horizon=3, verbose=True\n)\n\nseries.plot(label=\"data\")\nhistorical_fcast_theta.plot(label=\"backtest 3-months ahead forecast (Theta)\")\nprint(\"MAPE = {:.2f}%\".format(mape(historical_fcast_theta, series)))\n\n\n\n\nMAPE = 7.70%\n\n\n\n\n\n\n\n\n\n\nbest_theta_model = Theta(best_theta)\n\nraw_errors = best_theta_model.backtest(\n    series, start=0.6, forecast_horizon=3, metric=mape, reduction=None, verbose=True\n)\n\nfrom darts.utils.statistics import plot_hist\n\nplot_hist(\n    raw_errors,\n    bins=np.arange(0, max(raw_errors), 1),\n    title=\"Individual backtest error scores (histogram)\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\naverage_error = best_theta_model.backtest(\n    series,\n    start=0.6,\n    forecast_horizon=3,\n    metric=mape,\n    reduction=np.mean,  # this is actually the default\n    verbose=True,\n)\n\nprint(\"Average error (MAPE) over all historical forecasts: %.2f\" % average_error)\n\n\n\n\nAverage error (MAPE) over all historical forecasts: 6.36\n\n\n\nfrom darts.utils.statistics import plot_residuals_analysis\n\n\nplot_residuals_analysis(best_theta_model.residuals(series))\n\n\n\n\n\n\n\n\n\nmodel_es = ExponentialSmoothing(seasonal_periods=12)\nhistorical_fcast_es = model_es.historical_forecasts(\n    series, start=0.6, forecast_horizon=3, verbose=True\n)\n\nseries.plot(label=\"data\")\nhistorical_fcast_es.plot(label=\"backtest 3-months ahead forecast (Exp. Smoothing)\")\nprint(\"MAPE = {:.2f}%\".format(mape(historical_fcast_es, series)))\n\n\n\n\nMAPE = 4.45%\n\n\n\n\n\n\n\n\n\n\nplot_residuals_analysis(model_es.residuals(series))",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#machine-learning-and-global-models",
    "href": "darts.html#machine-learning-and-global-models",
    "title": "Darts",
    "section": "Machine learning and global models",
    "text": "Machine learning and global models\nRegressionModel can wrap around any sklearn-compatible regression model to produce forecasts (it has its own section below).\nRNNModel is a flexible RNN implementation, which can be used like DeepAR.\nNBEATSModel implements the N-BEATS model.\nTFTModel implements the Temporal Fusion Transformer model.\nTCNModel implements temporal convolutional networks.\n\nfrom darts.datasets import AirPassengersDataset, MonthlyMilkDataset\n\n\nseries_air = AirPassengersDataset().load().astype(np.float32)\nseries_milk = MonthlyMilkDataset().load().astype(np.float32)\n\n# set aside last 36 months of each series as validation set:\ntrain_air, val_air = series_air[:-36], series_air[-36:]\ntrain_milk, val_milk = series_milk[:-36], series_milk[-36:]\n\ntrain_air.plot()\nval_air.plot()\ntrain_milk.plot()\nval_milk.plot()\n\n\n\n\n\n\n\n\n\nfrom darts.dataprocessing.transformers import Scaler\n\n\nscaler = Scaler()\ntrain_air_scaled, train_milk_scaled = scaler.fit_transform([train_air, train_milk])\n\ntrain_air_scaled.plot()\ntrain_milk_scaled.plot()",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#using-deep-learning-example-with-n-beats",
    "href": "darts.html#using-deep-learning-example-with-n-beats",
    "title": "Darts",
    "section": "Using deep learning: example with N-BEATS",
    "text": "Using deep learning: example with N-BEATS\n\nfrom darts.models import NBEATSModel\n\n\nmodel = NBEATSModel(input_chunk_length=24, output_chunk_length=12, random_state=42)\n\nmodel.fit([train_air_scaled, train_milk_scaled], epochs=50, verbose=True);\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nYou are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name          | Type             | Params\n---------------------------------------------------\n0 | criterion     | MSELoss          | 0     \n1 | train_metrics | MetricCollection | 0     \n2 | val_metrics   | MetricCollection | 0     \n3 | stacks        | ModuleList       | 6.2 M \n---------------------------------------------------\n6.2 M     Trainable params\n1.4 K     Non-trainable params\n6.2 M     Total params\n24.787    Total estimated model params size (MB)\n\n\n\n\n\n`Trainer.fit` stopped: `max_epochs=50` reached.\n\n\n\npred_air = model.predict(series=train_air_scaled, n=36)\npred_milk = model.predict(series=train_milk_scaled, n=36)\n\n# scale back:\npred_air, pred_milk = scaler.inverse_transform([pred_air, pred_milk])\n\nplt.figure(figsize=(10, 6))\nseries_air.plot(label=\"actual (air)\")\nseries_milk.plot(label=\"actual (milk)\")\npred_air.plot(label=\"forecast (air)\")\npred_milk.plot(label=\"forecast (milk)\")\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#covariates-using-external-data",
    "href": "darts.html#covariates-using-external-data",
    "title": "Darts",
    "section": "Covariates: using external data",
    "text": "Covariates: using external data\npast_covariates are series not necessarily known ahead of the forecast time. Those can for instance represent things that have to be measured and are not known upfront. Models do not use the future values of past_covariates when making forecasts.\nfuture_covariates are series which are known in advance, up to the forecast horizon. This can represent things such as calendar information, holidays, weather forecasts, etc. Models that accept future_covariates will look at the future values (up to the forecast horizon) when making forecasts.\n\nfrom darts import concatenate\nfrom darts.utils.timeseries_generation import datetime_attribute_timeseries as dt_attr\n\n\nair_covs = concatenate(\n    [\n        dt_attr(series_air.time_index, \"month\", dtype=np.float32) / 12,\n        (dt_attr(series_air.time_index, \"year\", dtype=np.float32) - 1948) / 12,\n    ],\n    axis=\"component\",\n)\n\nmilk_covs = concatenate(\n    [\n        dt_attr(series_milk.time_index, \"month\", dtype=np.float32) / 12,\n        (dt_attr(series_milk.time_index, \"year\", dtype=np.float32) - 1962) / 13,\n    ],\n    axis=\"component\",\n)\n\nair_covs.plot()\nplt.title(\n    \"one multivariate time series of 2 dimensions, containing covariates for the air series:\"\n);\n\n\n\n\n\n\n\n\n\nmodel = NBEATSModel(input_chunk_length=24, output_chunk_length=12, random_state=42)\n\nmodel.fit(\n    [train_air_scaled, train_milk_scaled],\n    past_covariates=[air_covs, milk_covs],\n    epochs=50,\n    verbose=True,\n);\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name          | Type             | Params\n---------------------------------------------------\n0 | criterion     | MSELoss          | 0     \n1 | train_metrics | MetricCollection | 0     \n2 | val_metrics   | MetricCollection | 0     \n3 | stacks        | ModuleList       | 6.6 M \n---------------------------------------------------\n6.6 M     Trainable params\n1.7 K     Non-trainable params\n6.6 M     Total params\n26.314    Total estimated model params size (MB)\n\n\n\n\n\n`Trainer.fit` stopped: `max_epochs=50` reached.\n\n\n\npred_air = model.predict(series=train_air_scaled, past_covariates=air_covs, n=36)\npred_milk = model.predict(series=train_milk_scaled, past_covariates=milk_covs, n=36)\n\n# scale back:\npred_air, pred_milk = scaler.inverse_transform([pred_air, pred_milk])\n\nplt.figure(figsize=(10, 6))\nseries_air.plot(label=\"actual (air)\")\nseries_milk.plot(label=\"actual (milk)\")\npred_air.plot(label=\"forecast (air)\")\npred_milk.plot(label=\"forecast (milk)\")\n\n`predict()` was called with `n &gt; output_chunk_length`: using auto-regression to forecast the values after `output_chunk_length` points. The model will access `(n - output_chunk_length)` future values of your `past_covariates` (relative to the first predicted time step). To hide this warning, set `show_warnings=False`.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\n`predict()` was called with `n &gt; output_chunk_length`: using auto-regression to forecast the values after `output_chunk_length` points. The model will access `(n - output_chunk_length)` future values of your `past_covariates` (relative to the first predicted time step). To hide this warning, set `show_warnings=False`.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#encoders-using-covariates-for-free",
    "href": "darts.html#encoders-using-covariates-for-free",
    "title": "Darts",
    "section": "Encoders: using covariates for free",
    "text": "Encoders: using covariates for free\n\ndef extract_year(idx):\n    \"\"\"Extract the year each time index entry and normalized it.\"\"\"\n    return (idx.year - 1950) / 50\n\n\nencoders = {\n    \"cyclic\": {\"future\": [\"month\"]},\n    \"datetime_attribute\": {\"future\": [\"hour\", \"dayofweek\"]},\n    \"position\": {\"past\": [\"absolute\"], \"future\": [\"relative\"]},\n    \"custom\": {\"past\": [extract_year]},\n    \"transformer\": Scaler(),\n}\n\n\nencoders = {\"datetime_attribute\": {\"past\": [\"month\", \"year\"]}, \"transformer\": Scaler()}\n\n\nmodel = NBEATSModel(\n    input_chunk_length=24,\n    output_chunk_length=12,\n    add_encoders=encoders,\n    random_state=42,\n)\n\nmodel.fit([train_air_scaled, train_milk_scaled], epochs=50, verbose=True);\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name          | Type             | Params\n---------------------------------------------------\n0 | criterion     | MSELoss          | 0     \n1 | train_metrics | MetricCollection | 0     \n2 | val_metrics   | MetricCollection | 0     \n3 | stacks        | ModuleList       | 6.6 M \n---------------------------------------------------\n6.6 M     Trainable params\n1.7 K     Non-trainable params\n6.6 M     Total params\n26.314    Total estimated model params size (MB)\n\n\n\n\n\n`Trainer.fit` stopped: `max_epochs=50` reached.\n\n\n\npred_air = model.predict(series=train_air_scaled, n=36)\n\n# scale back:\npred_air = scaler.inverse_transform(pred_air)\n\nplt.figure(figsize=(10, 6))\nseries_air.plot(label=\"actual (air)\")\npred_air.plot(label=\"forecast (air)\")\n\n`predict()` was called with `n &gt; output_chunk_length`: using auto-regression to forecast the values after `output_chunk_length` points. The model will access `(n - output_chunk_length)` future values of your `past_covariates` (relative to the first predicted time step). To hide this warning, set `show_warnings=False`.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#regression-forecasting-models",
    "href": "darts.html#regression-forecasting-models",
    "title": "Darts",
    "section": "Regression forecasting models",
    "text": "Regression forecasting models\n\nfrom darts.models import RegressionModel\nfrom sklearn.linear_model import BayesianRidge\n\n\nmodel = RegressionModel(lags=72, lags_future_covariates=[-6, 0], model=BayesianRidge())\n\nmodel.fit(\n    [train_air_scaled, train_milk_scaled], future_covariates=[air_covs, milk_covs]\n);\n\n\npred_air, pred_milk = model.predict(\n    series=[train_air_scaled, train_milk_scaled],\n    future_covariates=[air_covs, milk_covs],\n    n=36,\n)\n\n# scale back:\npred_air, pred_milk = scaler.inverse_transform([pred_air, pred_milk])\n\nplt.figure(figsize=(10, 6))\nseries_air.plot(label=\"actual (air)\")\nseries_milk.plot(label=\"actual (milk)\")\npred_air.plot(label=\"forecast (air)\")\npred_milk.plot(label=\"forecast (milk)\")\n\n\n\n\n\n\n\n\n\nmape([series_air, series_milk], [pred_air, pred_milk])\n\n[3.417016565799713, 5.283146351575851]\n\n\n\nmape([series_air, series_milk], [pred_air, pred_milk], inter_reduction=np.mean)\n\n4.350081458687782\n\n\n\nbayes_ridge_model = RegressionModel(\n    lags=72, lags_future_covariates=[0], model=BayesianRidge()\n)\n\nbacktest = bayes_ridge_model.historical_forecasts(\n    series_air, future_covariates=air_covs, start=0.6, forecast_horizon=3, verbose=True\n)\n\nprint(\"MAPE = %.2f\" % (mape(backtest, series_air)))\nseries_air.plot()\nbacktest.plot()\n\n`enable_optimization=True` is ignored because `retrain` is not `False` or `0`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n`enable_optimization=True` is ignored because `forecast_horizon &gt; model.output_chunk_length`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n\n\n\n\n\nMAPE = 3.66",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#probabilistic-forecasts",
    "href": "darts.html#probabilistic-forecasts",
    "title": "Darts",
    "section": "Probabilistic forecasts",
    "text": "Probabilistic forecasts\n\nmodel_es = ExponentialSmoothing()\nmodel_es.fit(train)\nprobabilistic_forecast = model_es.predict(len(val), num_samples=500)\n\nseries.plot(label=\"actual\")\nprobabilistic_forecast.plot(label=\"probabilistic forecast\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#with-neural-networks",
    "href": "darts.html#with-neural-networks",
    "title": "Darts",
    "section": "With neural networks",
    "text": "With neural networks\n\nfrom darts.models import TCNModel\nfrom darts.utils.likelihood_models import LaplaceLikelihood\n\n\nmodel = TCNModel(\n    input_chunk_length=24,\n    output_chunk_length=12,\n    random_state=42,\n    likelihood=LaplaceLikelihood(),\n)\n\nmodel.fit(train_air_scaled, epochs=400, verbose=True);\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | criterion     | MSELoss           | 0     \n1 | train_metrics | MetricCollection  | 0     \n2 | val_metrics   | MetricCollection  | 0     \n3 | dropout       | MonteCarloDropout | 0     \n4 | res_blocks    | ModuleList        | 166   \n----------------------------------------------------\n166       Trainable params\n0         Non-trainable params\n166       Total params\n0.001     Total estimated model params size (MB)\n\n\n\n\n\n`Trainer.fit` stopped: `max_epochs=400` reached.\n\n\n\npred = model.predict(n=36, num_samples=500)\n\n# scale back:\npred = scaler.inverse_transform(pred)\n\nseries_air.plot()\npred.plot()\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel = TCNModel(\n    input_chunk_length=24,\n    output_chunk_length=12,\n    random_state=42,\n    likelihood=LaplaceLikelihood(prior_b=0.1),\n)\n\nmodel.fit(train_air_scaled, epochs=400, verbose=True);\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | criterion     | MSELoss           | 0     \n1 | train_metrics | MetricCollection  | 0     \n2 | val_metrics   | MetricCollection  | 0     \n3 | dropout       | MonteCarloDropout | 0     \n4 | res_blocks    | ModuleList        | 166   \n----------------------------------------------------\n166       Trainable params\n0         Non-trainable params\n166       Total params\n0.001     Total estimated model params size (MB)\n\n\n\n\n\n`Trainer.fit` stopped: `max_epochs=400` reached.\n\n\n\npred = model.predict(n=36, num_samples=500)\n\n# scale back:\npred = scaler.inverse_transform(pred)\n\nseries_air.plot()\npred.plot()\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\n\n\n\n\n\n\n\n\npred.plot(low_quantile=0.01, high_quantile=0.99, label=\"1-99th percentiles\")\npred.plot(low_quantile=0.2, high_quantile=0.8, label=\"20-80th percentiles\")",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#types-of-distributions",
    "href": "darts.html#types-of-distributions",
    "title": "Darts",
    "section": "Types of distributions",
    "text": "Types of distributions\nThe likelihood has to be compatible with the domain of your time series’ values. For instance PoissonLikelihood can be used on discrete positive values, ExponentialLikelihood can be used on real positive values, and BetaLikelihood on real values in (0,1)\nIt is also possible to use QuantileRegression to apply a quantile loss and fit some desired quantiles directly.",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#evaluating-probabilistic-forecasts",
    "href": "darts.html#evaluating-probabilistic-forecasts",
    "title": "Darts",
    "section": "Evaluating Probabilistic Forecasts",
    "text": "Evaluating Probabilistic Forecasts\n\nfrom darts.metrics import rho_risk\n\n\nprint(\"MAPE of median forecast: %.2f\" % mape(series_air, pred))\nfor rho in [0.05, 0.1, 0.5, 0.9, 0.95]:\n    rr = rho_risk(series_air, pred, rho=rho)\n    print(\"rho-risk at quantile %.2f: %.2f\" % (rho, rr))\n\nMAPE of median forecast: 11.67\nrho-risk at quantile 0.05: 0.14\nrho-risk at quantile 0.10: 0.14\nrho-risk at quantile 0.50: 0.12\nrho-risk at quantile 0.90: 0.03\nrho-risk at quantile 0.95: 0.02",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#using-quantile-loss",
    "href": "darts.html#using-quantile-loss",
    "title": "Darts",
    "section": "Using Quantile Loss",
    "text": "Using Quantile Loss\n\nfrom darts.utils.likelihood_models import QuantileRegression\n\n\nmodel = TCNModel(\n    input_chunk_length=24,\n    output_chunk_length=12,\n    random_state=42,\n    likelihood=QuantileRegression([0.05, 0.1, 0.5, 0.9, 0.95]),\n)\n\nmodel.fit(train_air_scaled, epochs=400, verbose=True);\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | criterion     | MSELoss           | 0     \n1 | train_metrics | MetricCollection  | 0     \n2 | val_metrics   | MetricCollection  | 0     \n3 | dropout       | MonteCarloDropout | 0     \n4 | res_blocks    | ModuleList        | 208   \n----------------------------------------------------\n208       Trainable params\n0         Non-trainable params\n208       Total params\n0.001     Total estimated model params size (MB)\n\n\n\n\n\n`Trainer.fit` stopped: `max_epochs=400` reached.\n\n\n\npred = model.predict(n=36, num_samples=500)\n\n# scale back:\npred = scaler.inverse_transform(pred)\n\nseries_air.plot()\npred.plot()\n\nprint(\"MAPE of median forecast: %.2f\" % mape(series_air, pred))\nfor rho in [0.05, 0.1, 0.5, 0.9, 0.95]:\n    rr = rho_risk(series_air, pred, rho=rho)\n    print(\"rho-risk at quantile %.2f: %.2f\" % (rho, rr))\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\nMAPE of median forecast: 5.11\nrho-risk at quantile 0.05: 0.00\nrho-risk at quantile 0.10: 0.00\nrho-risk at quantile 0.50: 0.02\nrho-risk at quantile 0.90: 0.01\nrho-risk at quantile 0.95: 0.01",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#ensembling-models",
    "href": "darts.html#ensembling-models",
    "title": "Darts",
    "section": "Ensembling models",
    "text": "Ensembling models\nEnsembling is about combining the forecasts produced by several models, in order to obtain a final - and hopefully better forecast.\nFor instance, in our example of a less naive model above, we manually combined a naive seasonal model with a naive drift model. Here, we will show how models forecasts can be automatically combined, naively using a NaiveEnsembleModel, or learned using RegressionEnsembleModel.\nIt is of course also possible to use past and/or future_covariates with ensemble model but they will be passed only to the models supporting them when calling fit() and predict().",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#naive-ensembling",
    "href": "darts.html#naive-ensembling",
    "title": "Darts",
    "section": "Naive Ensembling",
    "text": "Naive Ensembling\n\nfrom darts.models import NaiveEnsembleModel\n\n\nmodels = [NaiveDrift(), NaiveSeasonal(12)]\n\nensemble_model = NaiveEnsembleModel(forecasting_models=models)\n\nbacktest = ensemble_model.historical_forecasts(\n    series_air, start=0.6, forecast_horizon=3, verbose=True\n)\n\nprint(\"MAPE = %.2f\" % (mape(backtest, series_air)))\nseries_air.plot()\nbacktest.plot()\n\n\n\n\nMAPE = 11.88",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#learned-ensembling",
    "href": "darts.html#learned-ensembling",
    "title": "Darts",
    "section": "Learned Ensembling",
    "text": "Learned Ensembling\nAs expected in this case, the naive ensemble doesn’t give great results (although in some cases it could!)\nWe can sometimes do better if we see the ensembling as a supervised regression problem: given a set of forecasts (features), find a model that combines them in order to minimise errors on the target. This is what the RegressionEnsembleModel does. It accepts three parameters:\nforecasting_models is a list of forecasting models whose predictions we want to ensemble.\n\nregression_train_n_points is the number of time steps to use for fitting the “ensemble regression” model (i.e., the inner model that combines the forecasts).\n\nregression_model is, optionally, a sklearn-compatible regression model or a Darts RegressionModel to be used for the ensemble regression. If not specified, a linear regression is used. Using a sklearn model is easy out-of-the-box, but using a RegressionModel allows to potentially take arbitrary lags of the individual forecasts as inputs of the regression model.\nOnce these elements are in place, a RegressionEnsembleModel can be used like a regular forecasting model:\n\nfrom darts.models import RegressionEnsembleModel\n\n\nmodels = [NaiveDrift(), NaiveSeasonal(12)]\n\nensemble_model = RegressionEnsembleModel(\n    forecasting_models=models, regression_train_n_points=12\n)\n\nbacktest = ensemble_model.historical_forecasts(\n    series_air, start=0.6, forecast_horizon=3, verbose=True\n)\n\nprint(\"MAPE = %.2f\" % (mape(backtest, series_air)))\nseries_air.plot()\nbacktest.plot()\n\n\n\n\nMAPE = 4.85\n\n\n\n\n\n\n\n\n\n\nensemble_model.fit(series_air)\nensemble_model.regression_model.model.coef_#|eval: false\n\narray([0.0136882, 1.0980103], dtype=float32)\n\n\n\nfrom darts.models import LinearRegressionModel\n\n\nquantiles = [0.25, 0.5, 0.75]\n\nmodels = [NaiveDrift(), NaiveSeasonal(12)]\n\nregression_model = LinearRegressionModel(\n    quantiles=quantiles,\n    lags_future_covariates=[0],\n    likelihood=\"quantile\",\n    fit_intercept=False,\n)\n\nensemble_model = RegressionEnsembleModel(\n    forecasting_models=models,\n    regression_train_n_points=12,\n    regression_model=regression_model,\n)\n\nbacktest = ensemble_model.historical_forecasts(\n    series_air, start=0.6, forecast_horizon=3, num_samples=500, verbose=True\n)\n\nprint(\"MAPE = %.2f\" % (mape(backtest, series_air)))\nseries_air.plot()\nbacktest.plot()\n\n\n\n\nMAPE = 4.79",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#fitting-a-kalman-filter",
    "href": "darts.html#fitting-a-kalman-filter",
    "title": "Darts",
    "section": "Fitting a Kalman Filter",
    "text": "Fitting a Kalman Filter\n\nfrom darts.models import KalmanFilter\n\n\nkf = KalmanFilter(dim_x=3)\nkf.fit(train_air_scaled)\nfiltered_series = kf.filter(train_air_scaled, num_samples=100)\n\ntrain_air_scaled.plot()\nfiltered_series.plot(#|eval: false\n)",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "darts.html#inferring-missing-values-with-gaussian-processes",
    "href": "darts.html#inferring-missing-values-with-gaussian-processes",
    "title": "Darts",
    "section": "Inferring missing values with Gaussian Processes",
    "text": "Inferring missing values with Gaussian Processes\n\nfrom darts.models import GaussianProcessFilter\nfrom sklearn.gaussian_process.kernels import RBF\n\n\n# create a series with holes:\nvalues = train_air_scaled.values()\nvalues[20:22] = np.nan\nvalues[28:32] = np.nan\nvalues[55:59] = np.nan\nvalues[72:80] = np.nan\nseries_holes = TimeSeries.from_times_and_values(train_air_scaled.time_index, values)\nseries_holes.plot()\n\nkernel = RBF()\n\ngpf = GaussianProcessFilter(kernel=kernel, alpha=0.1, normalize_y=True)\nfiltered_series = gpf.filter(series_holes, num_samples=100)\n\nfiltered_series.plot()",
    "crumbs": [
      "Blog",
      "Darts"
    ]
  },
  {
    "objectID": "hyperparameter optimization.html",
    "href": "hyperparameter optimization.html",
    "title": "Darts - Hyper-parameters Optimization for Electricity Load Forecasting",
    "section": "",
    "text": "The following cell can take a few minutes to execute. It’ll download about 250 MB of data from the Internet. We specify multivariate=False, so we get a list of 370 univariate TimeSeries. We could also have specified multivariate=True to obtain one multivariate TimeSeries containing 370 components.\nWe keep the last 80 days for each series, and cast all of them to float32:\nWe have 370 univariate TimeSeries, each with a frequency of 15 minutes. In what follows, we will be training a single global model on all of them.\nFirst, we create our training set. We set aside the last 14 days as test set, and the 14 days before that as validation set (which will be used for hyperparameter optimization).\nNote that the val and test sets below only contain the 14-days “forecast evaluation” parts of the series. Throughout the notebook, we’ll evaluate how accurate some 14-days forecasts are over val (or test). However, to produce these 14-days forecasts, our models will consume a certain lookback window in_len worth of time stamps. For this reason, below we will also create validation sets that include these extra in_len points (as in_len is a hyper-parameter itself, we create these longer validation sets dynamically); it will be mainly useful for early-stopping.\n\n# Split in train/val/test\nval_len = 14 * DAY_DURATION  # 14 days\n\ntrain = [s[: -(2 * val_len)] for s in all_series_fp32]\nval = [s[-(2 * val_len) : -val_len] for s in all_series_fp32]\ntest = [s[-val_len:] for s in all_series_fp32]\n\n# Scale so that the largest value is 1.\n# This way of scaling perserves the sMAPE\nscaler = Scaler(scaler=MaxAbsScaler())\ntrain = scaler.fit_transform(train)\nval = scaler.transform(val)\ntest = scaler.transform(test)\n\nLet’s plot a few of our series:\n\nfor i in [10, 50, 100, 150, 250, 350]:\n    plt.figure(figsize=(15, 5))\n    train[i].plot(label=\"{}\".format(i, lw=1))",
    "crumbs": [
      "Blog",
      "Darts - Hyper-parameters Optimization for Electricity Load Forecasting"
    ]
  },
  {
    "objectID": "hyperparameter optimization.html#data-preparation",
    "href": "hyperparameter optimization.html#data-preparation",
    "title": "Darts - Hyper-parameters Optimization for Electricity Load Forecasting",
    "section": "",
    "text": "The following cell can take a few minutes to execute. It’ll download about 250 MB of data from the Internet. We specify multivariate=False, so we get a list of 370 univariate TimeSeries. We could also have specified multivariate=True to obtain one multivariate TimeSeries containing 370 components.\nWe keep the last 80 days for each series, and cast all of them to float32:\nWe have 370 univariate TimeSeries, each with a frequency of 15 minutes. In what follows, we will be training a single global model on all of them.\nFirst, we create our training set. We set aside the last 14 days as test set, and the 14 days before that as validation set (which will be used for hyperparameter optimization).\nNote that the val and test sets below only contain the 14-days “forecast evaluation” parts of the series. Throughout the notebook, we’ll evaluate how accurate some 14-days forecasts are over val (or test). However, to produce these 14-days forecasts, our models will consume a certain lookback window in_len worth of time stamps. For this reason, below we will also create validation sets that include these extra in_len points (as in_len is a hyper-parameter itself, we create these longer validation sets dynamically); it will be mainly useful for early-stopping.\n\n# Split in train/val/test\nval_len = 14 * DAY_DURATION  # 14 days\n\ntrain = [s[: -(2 * val_len)] for s in all_series_fp32]\nval = [s[-(2 * val_len) : -val_len] for s in all_series_fp32]\ntest = [s[-val_len:] for s in all_series_fp32]\n\n# Scale so that the largest value is 1.\n# This way of scaling perserves the sMAPE\nscaler = Scaler(scaler=MaxAbsScaler())\ntrain = scaler.fit_transform(train)\nval = scaler.transform(val)\ntest = scaler.transform(test)\n\nLet’s plot a few of our series:\n\nfor i in [10, 50, 100, 150, 250, 350]:\n    plt.figure(figsize=(15, 5))\n    train[i].plot(label=\"{}\".format(i, lw=1))",
    "crumbs": [
      "Blog",
      "Darts - Hyper-parameters Optimization for Electricity Load Forecasting"
    ]
  },
  {
    "objectID": "hyperparameter optimization.html#build-a-simple-linear-model",
    "href": "hyperparameter optimization.html#build-a-simple-linear-model",
    "title": "Darts - Hyper-parameters Optimization for Electricity Load Forecasting",
    "section": "Build a Simple Linear Model",
    "text": "Build a Simple Linear Model\nWe start off without any hyperparameter optimization, and try a simple linear regression model first. It will serve as a baseline. In this model, we use a lookback window of 1 week.\nNote: doing significantly better than linear regression is often not trivial! We recommend to always consider at least one such reasonably simple baseline first, before jumping to more complex models.\nLinearRegressionModel wraps around sklearn.linear_model.LinearRegression, which may take a significant amount of processing and memory. Running this cell takes a couple of minutes, and we recommend skipping it unless you have at least 20GB of RAM on your system.\n\n#lr_model = LinearRegressionModel(lags=7 * DAY_DURATION)\n#lr_model.fit(train);\n\nLet’s see how this model is doing:\nlr_preds = lr_model.predict(series=train, n=val_len) eval_model(lr_preds, “linear regression”)\nThis model is already doing quite well out of the box! Let’s see now if we can do better using deep learning.",
    "crumbs": [
      "Blog",
      "Darts - Hyper-parameters Optimization for Electricity Load Forecasting"
    ]
  },
  {
    "objectID": "hyperparameter optimization.html#build-a-simple-tcn-model",
    "href": "hyperparameter optimization.html#build-a-simple-tcn-model",
    "title": "Darts - Hyper-parameters Optimization for Electricity Load Forecasting",
    "section": "Build a Simple TCN Model",
    "text": "Build a Simple TCN Model\nWe now build a TCN model with some simple choice of hyperparameters, but without any hyperparameter optimization.\n\n\"\"\" We write a function to build and fit a TCN Model, which we will re-use later.\n\"\"\"\n\n#|eval: false\n\ndef build_fit_tcn_model(\n    in_len,\n    out_len,\n    kernel_size,\n    num_filters,\n    weight_norm,\n    dilation_base,\n    dropout,\n    lr,\n    include_dayofweek,\n    likelihood=None,\n    callbacks=None,\n):\n\n    # reproducibility\n    torch.manual_seed(42)\n\n    # some fixed parameters that will be the same for all models\n    BATCH_SIZE = 512\n    MAX_N_EPOCHS = 4\n    NR_EPOCHS_VAL_PERIOD = 1\n    MAX_SAMPLES_PER_TS = 100\n\n    # throughout training we'll monitor the validation loss for early stopping\n    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.001, patience=3, verbose=True)\n    if callbacks is None:\n        callbacks = [early_stopper]\n    else:\n        callbacks = [early_stopper] + callbacks\n\n    # detect if a GPU is available\n    if torch.cuda.is_available():\n        pl_trainer_kwargs = {\n            \"accelerator\": \"gpu\",\n            \"callbacks\": callbacks,\n        }\n        num_workers = 4\n    else:\n        pl_trainer_kwargs = {\"callbacks\": callbacks}\n        num_workers = 0\n\n    # optionally also add the day of the week (cyclically encoded) as a past covariate\n    encoders = {\"cyclic\": {\"past\": [\"dayofweek\"]}} if include_dayofweek else None\n\n    # build the TCN model\n    model = TCNModel(\n        input_chunk_length=in_len,\n        output_chunk_length=out_len,\n        batch_size=BATCH_SIZE,\n        n_epochs=MAX_N_EPOCHS,\n        nr_epochs_val_period=NR_EPOCHS_VAL_PERIOD,\n        kernel_size=kernel_size,\n        num_filters=num_filters,\n        weight_norm=weight_norm,\n        dilation_base=dilation_base,\n        dropout=dropout,\n        optimizer_kwargs={\"lr\": lr},\n        add_encoders=encoders,\n        likelihood=likelihood,\n        pl_trainer_kwargs=pl_trainer_kwargs,\n        model_name=\"tcn_model\",\n        force_reset=True,\n        save_checkpoints=True,\n    )\n\n    # when validating during training, we can use a slightly longer validation\n    # set which also contains the first input_chunk_length time steps\n    model_val_set = scaler.transform(\n        [s[-((2 * val_len) + in_len) : -val_len] for s in all_series_fp32]\n    )\n\n    # train the model\n    model.fit(\n        series=train,\n        val_series=model_val_set,\n        max_samples_per_ts=MAX_SAMPLES_PER_TS,\n        num_loader_workers=num_workers,\n    )\n\n    # reload best model over course of training\n    model = TCNModel.load_from_checkpoint(\"tcn_model\")\n\n    return model\n\n\nmodel = build_fit_tcn_model(\n    in_len=7 * DAY_DURATION,\n    out_len=6 * DAY_DURATION,\n    kernel_size=5,\n    num_filters=5,\n    weight_norm=False,\n    dilation_base=2,\n    dropout=0.2,\n    lr=1e-3,\n    include_dayofweek=True,\n)\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nYou are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | criterion     | MSELoss           | 0     \n1 | train_metrics | MetricCollection  | 0     \n2 | val_metrics   | MetricCollection  | 0     \n3 | dropout       | MonteCarloDropout | 0     \n4 | res_blocks    | ModuleList        | 1.7 K \n----------------------------------------------------\n1.7 K     Trainable params\n0         Non-trainable params\n1.7 K     Total params\n0.007     Total estimated model params size (MB)\n\n\n\n\n\n\n\n\n\n\n\nMetric val_loss improved. New best score: 0.030\n\n\n\n\n\nMetric val_loss improved by 0.012 &gt;= min_delta = 0.001. New best score: 0.018\n\n\n\n\n\nMetric val_loss improved by 0.004 &gt;= min_delta = 0.001. New best score: 0.014\n\n\n\n\n\nMetric val_loss improved by 0.002 &gt;= min_delta = 0.001. New best score: 0.012\n`Trainer.fit` stopped: `max_epochs=4` reached.\n\n\n\npreds = model.predict(series=train, n=val_len)\neval_model(preds, \"First TCN model\")\n\n`predict()` was called with `n &gt; output_chunk_length`: using auto-regression to forecast the values after `output_chunk_length` points. The model will access `(n - output_chunk_length)` future values of your `past_covariates` (relative to the first predicted time step). To hide this warning, set `show_warnings=False`.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\nFirst TCN model sMAPE: 20.30 +- 21.44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbove, we built a first TCN model without any hyper-parameter search, and got an sMAPE of about 17%. Although this model looks like a good start (performing quite well on some of the series), it’s not as good as the simple linear regression.\nWe can certainly do better, because there are many parameters that we have fixed but could have a large impact on performance, such as:\n\nThe architecture of the network (number of filters, dilation size, kernel size, etc…)\nThe learning rate\nWhether to use weight normalization and/or the dropout rate\nThe length of the lookback and lookforward windows\nWhether to add calendar covariates, such as day-of-the-week\n…\n\n\nOne Option: using gridsearch()\nOne way to try and optimize these hyper-parameters is to try all combinations (assuming we have discretized our parameters). Darts offers a gridsearch() method to do just that. The advantage is that it is very simple to use. However, it also has severe drawbacks:\n\nIt takes exponential time in the number of hyper-parameters: grid-searching over any non-trivial number of hyperparameters thus quickly becomes intractable.\nGridsearch is naive: it does not attempt to pay attention to regions of the hyperparameter space that are more promising than others. It is limited points in the pre-defined grid.\nFinally, for simplicity reasons the Darts gridsearch() method is (at least at the time of writing) limited to working on one time series only.\n\nFor all these reasons, for any serious hyperparameter search, we need better techniques than grid-search. Fortunately, there are some great tools out there to help us.",
    "crumbs": [
      "Blog",
      "Darts - Hyper-parameters Optimization for Electricity Load Forecasting"
    ]
  },
  {
    "objectID": "hyperparameter optimization.html#using-optuna",
    "href": "hyperparameter optimization.html#using-optuna",
    "title": "Darts - Hyper-parameters Optimization for Electricity Load Forecasting",
    "section": "Using Optuna",
    "text": "Using Optuna\nOptuna is a very nice open-source library for hyperparameter optimization. It’s based on ideas such as Bayesian optimization, which balance exploration (of the hyperparameter space) with exploitation (namely, exploring more the parts of the space that look more promising). It can also use pruning in order to stop unpromising experiments early.\nIt’s very easy to make it work: Optuna will take care of suggesting (sampling) hyper-parameters for us, and more or less all we need to do is to compute the objective value for a set of hyperparameters. In our case it consists in using these hyperparameters to build a model, train it, and report the obtained validation accuracy. We also setup a PyTorch Lightning pruning callback in order to early-stop unpromising experiments. All of this is done in the objective() function below.\n\ndef objective(trial):\n    callback = [PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")]\n\n    # set input_chunk_length, between 5 and 14 days\n    days_in = trial.suggest_int(\"days_in\", 5, 14)\n    in_len = days_in * DAY_DURATION\n\n    # set out_len, between 1 and 13 days (it has to be strictly shorter than in_len).\n    days_out = trial.suggest_int(\"days_out\", 1, days_in - 1)\n    out_len = days_out * DAY_DURATION\n\n    # Other hyperparameters\n    kernel_size = trial.suggest_int(\"kernel_size\", 5, 25)\n    num_filters = trial.suggest_int(\"num_filters\", 5, 25)\n    weight_norm = trial.suggest_categorical(\"weight_norm\", [False, True])\n    dilation_base = trial.suggest_int(\"dilation_base\", 2, 4)\n    dropout = trial.suggest_float(\"dropout\", 0.0, 0.4)\n    lr = trial.suggest_float(\"lr\", 5e-5, 1e-3, log=True)\n    include_dayofweek = trial.suggest_categorical(\"dayofweek\", [False, True])\n\n    # build and train the TCN model with these hyper-parameters:\n    model = build_fit_tcn_model(\n        in_len=in_len,\n        out_len=out_len,\n        kernel_size=kernel_size,\n        num_filters=num_filters,\n        weight_norm=weight_norm,\n        dilation_base=dilation_base,\n        dropout=dropout,\n        lr=lr,\n        include_dayofweek=include_dayofweek,\n        callbacks=callback,\n    )\n\n    # Evaluate how good it is on the validation set\n    preds = model.predict(series=train, n=val_len)\n    smapes = smape(val, preds, n_jobs=-1, verbose=True)\n    smape_val = np.mean(smapes)\n\n    return smape_val if smape_val != np.nan else float(\"inf\")\n\nNow that we have specified our objective, all we need to do is create an Optuna study, and run the optimization. We can either ask Optuna to run for a specified period of time (as we do here), or a certain number of trials. Let’s run the optimization for a couple of hours:\n\ndef print_callback(study, trial):\n    print(f\"Current value: {trial.value}, Current params: {trial.params}\")\n    print(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\")\n\n\nstudy = optuna.create_study(direction=\"minimize\")\n\nstudy.optimize(objective, n_trials=2, callbacks=[print_callback])\n\n# We could also have used a command as follows to limit the number of trials instead:\n# study.optimize(objective, n_trials=100, callbacks=[print_callback])\n\n# Finally, print the best value and best hyperparameters:\nprint(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\")\n\n[I 2023-11-27 18:21:17,748] A new study created in memory with name: no-name-17402fda-b494-4157-8b6a-5445e5542a2d\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | criterion     | MSELoss           | 0     \n1 | train_metrics | MetricCollection  | 0     \n2 | val_metrics   | MetricCollection  | 0     \n3 | dropout       | MonteCarloDropout | 0     \n4 | res_blocks    | ModuleList        | 49.5 K\n----------------------------------------------------\n49.5 K    Trainable params\n0         Non-trainable params\n49.5 K    Total params\n0.198     Total estimated model params size (MB)\n\n\n\n\n\n\n\n\n\n\n\nMetric val_loss improved. New best score: 0.044\n\n\n\n\n\nMetric val_loss improved by 0.016 &gt;= min_delta = 0.001. New best score: 0.028\n\n\n\n\n\nMetric val_loss improved by 0.006 &gt;= min_delta = 0.001. New best score: 0.022\n\n\n\n\n\nMetric val_loss improved by 0.003 &gt;= min_delta = 0.001. New best score: 0.019\n`Trainer.fit` stopped: `max_epochs=4` reached.\n`predict()` was called with `n &gt; output_chunk_length`: using auto-regression to forecast the values after `output_chunk_length` points. The model will access `(n - output_chunk_length)` future values of your `past_covariates` (relative to the first predicted time step). To hide this warning, set `show_warnings=False`.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\n\n\n\n[I 2023-11-27 18:22:17,658] Trial 0 finished with value: 22.625341993328686 and parameters: {'days_in': 9, 'days_out': 5, 'kernel_size': 12, 'num_filters': 20, 'weight_norm': True, 'dilation_base': 2, 'dropout': 0.29362403159128114, 'lr': 7.707227410795801e-05, 'dayofweek': True}. Best is trial 0 with value: 22.625341993328686.\n\n\nCurrent value: 22.625341993328686, Current params: {'days_in': 9, 'days_out': 5, 'kernel_size': 12, 'num_filters': 20, 'weight_norm': True, 'dilation_base': 2, 'dropout': 0.29362403159128114, 'lr': 7.707227410795801e-05, 'dayofweek': True}\nBest value: 22.625341993328686, Best params: {'days_in': 9, 'days_out': 5, 'kernel_size': 12, 'num_filters': 20, 'weight_norm': True, 'dilation_base': 2, 'dropout': 0.29362403159128114, 'lr': 7.707227410795801e-05, 'dayofweek': True}\n\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | criterion     | MSELoss           | 0     \n1 | train_metrics | MetricCollection  | 0     \n2 | val_metrics   | MetricCollection  | 0     \n3 | dropout       | MonteCarloDropout | 0     \n4 | res_blocks    | ModuleList        | 10.6 K\n----------------------------------------------------\n10.6 K    Trainable params\n0         Non-trainable params\n10.6 K    Total params\n0.042     Total estimated model params size (MB)\n\n\n\n\n\n\n\n\n\n\n\nMetric val_loss improved. New best score: 0.096\n\n\n\n\n\nMetric val_loss improved by 0.038 &gt;= min_delta = 0.001. New best score: 0.058\n\n\n\n\n\nMetric val_loss improved by 0.010 &gt;= min_delta = 0.001. New best score: 0.047\n\n\n\n\n\nMetric val_loss improved by 0.005 &gt;= min_delta = 0.001. New best score: 0.042\n`Trainer.fit` stopped: `max_epochs=4` reached.\n`predict()` was called with `n &gt; output_chunk_length`: using auto-regression to forecast the values after `output_chunk_length` points. The model will access `(n - output_chunk_length)` future values of your `past_covariates` (relative to the first predicted time step). To hide this warning, set `show_warnings=False`.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\n\n\n\n[I 2023-11-27 18:23:01,207] Trial 1 finished with value: 30.121115812578715 and parameters: {'days_in': 12, 'days_out': 6, 'kernel_size': 20, 'num_filters': 9, 'weight_norm': True, 'dilation_base': 3, 'dropout': 0.2818098055929793, 'lr': 0.00013484175968074997, 'dayofweek': True}. Best is trial 0 with value: 22.625341993328686.\n\n\nCurrent value: 30.121115812578715, Current params: {'days_in': 12, 'days_out': 6, 'kernel_size': 20, 'num_filters': 9, 'weight_norm': True, 'dilation_base': 3, 'dropout': 0.2818098055929793, 'lr': 0.00013484175968074997, 'dayofweek': True}\nBest value: 22.625341993328686, Best params: {'days_in': 9, 'days_out': 5, 'kernel_size': 12, 'num_filters': 20, 'weight_norm': True, 'dilation_base': 2, 'dropout': 0.29362403159128114, 'lr': 7.707227410795801e-05, 'dayofweek': True}\nBest value: 22.625341993328686, Best params: {'days_in': 9, 'days_out': 5, 'kernel_size': 12, 'num_filters': 20, 'weight_norm': True, 'dilation_base': 2, 'dropout': 0.29362403159128114, 'lr': 7.707227410795801e-05, 'dayofweek': True}\n\n\nNote: If we wanted to optimize further, we could still call study.optimize() more times to resume where we left off\nThere’s a lot more that you can do with Optuna. We refer to the documentation for more information. For instance, it’s possible to obtain useful insights into the optimization process, by visualising the objective value history (over trials), the objective value as a function of some of the hyperparameters, or the overall importance of some of the hyperparameters.\n\nplot_optimization_history(study)\n\n                                                \n\n\n\nplot_contour(study, params=[\"lr\", \"num_filters\"])\n\n                                                \n\n\n\nplot_param_importances(study)",
    "crumbs": [
      "Blog",
      "Darts - Hyper-parameters Optimization for Electricity Load Forecasting"
    ]
  },
  {
    "objectID": "hyperparameter optimization.html#picking-up-the-best-model",
    "href": "hyperparameter optimization.html#picking-up-the-best-model",
    "title": "Darts - Hyper-parameters Optimization for Electricity Load Forecasting",
    "section": "Picking up the best model",
    "text": "Picking up the best model\nAfter running the hyperparameter optimization for a couple of hours on a GPU, we got:\nBest value: 14.720555851487694, Best params: {'days_in': 14, 'days_out': 6, 'kernel_size': 19, 'num_filters': 19, 'weight_norm': True, 'dilation_base': 4, 'dropout': 0.07718156729165897, 'lr': 0.0008841998396117885, 'dayofweek': False}\nWe can now take these hyperparameters and train the “best” model again. This time, we will directly try to fit a probabilistic model (using a Gaussian likelihood). Note that this actually changes the loss, so we’re hoping that our hyperparameters are not too sensitive in that regard.\n\nbest_model = build_fit_tcn_model(\n    in_len=14 * DAY_DURATION,\n    out_len=6 * DAY_DURATION,\n    kernel_size=19,\n    num_filters=19,\n    weight_norm=True,\n    dilation_base=4,\n    dropout=0.0772,\n    lr=0.0008842,\n    likelihood=GaussianLikelihood(),\n    include_dayofweek=False,\n)\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | criterion     | MSELoss           | 0     \n1 | train_metrics | MetricCollection  | 0     \n2 | val_metrics   | MetricCollection  | 0     \n3 | dropout       | MonteCarloDropout | 0     \n4 | res_blocks    | ModuleList        | 42.6 K\n----------------------------------------------------\n42.6 K    Trainable params\n0         Non-trainable params\n42.6 K    Total params\n0.170     Total estimated model params size (MB)\n\n\n\n\n\n\n\n\n\n\n\nMetric val_loss improved. New best score: -0.819\n\n\n\n\n\nMetric val_loss improved by 0.043 &gt;= min_delta = 0.001. New best score: -0.862\n\n\n\n\n\nMetric val_loss improved by 0.051 &gt;= min_delta = 0.001. New best score: -0.913\n\n\n\n\n\nMetric val_loss improved by 0.038 &gt;= min_delta = 0.001. New best score: -0.951\n`Trainer.fit` stopped: `max_epochs=4` reached.\n\n\nLet’s now look at the accuracy of stochastic forecasts, with 100 samples:\n\nbest_preds = best_model.predict(\n    series=train, n=val_len, num_samples=100, mc_dropout=True\n)\neval_model(best_preds, \"best model, probabilistic\")\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\nbest model, probabilistic sMAPE: 16.03 +- 19.80\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe accuracy seems really good, and this model does not suffer from some of the same issues that our initial linear regression and early TCN had (look for instance in the failure modes it used to have on series 150).\nLet’s now also see how it performs on the test set:\n\ntrain_val_set = scaler.transform([s[:-val_len] for s in all_series_fp32])\n\nbest_preds_test = best_model.predict(\n    series=train_val_set, n=val_len, num_samples=100, mc_dropout=True\n)\n\neval_model(\n    best_preds_test,\n    \"best model, probabilistic, on test set\",\n    train_set=train_val_set,\n    val_set=test,\n)\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\nbest model, probabilistic, on test set sMAPE: 19.95 +- 21.62\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe performance is not as good on the test set, but on closer inspection this seems to be due to the Christmas time, during which some of the clients (unsurprisingly) changed their consumption. Besides the Christmas time, the quality of the forecasts seems roughly on par with what we had during the validation set, which is a good indication that we probably did not overfit our hyperparameter optimization to the validation set.\nTo improve this model further, it might be a good idea to consider using indicator variables capturing public holidays (which we haven’t done here).\nAs a last experiment, let’s see how our linear regression model performs on the test set:",
    "crumbs": [
      "Blog",
      "Darts - Hyper-parameters Optimization for Electricity Load Forecasting"
    ]
  },
  {
    "objectID": "hyperparameter optimization.html#conclusions",
    "href": "hyperparameter optimization.html#conclusions",
    "title": "Darts - Hyper-parameters Optimization for Electricity Load Forecasting",
    "section": "Conclusions",
    "text": "Conclusions\nWe have seen in this notebook that Optuna can be seamlessly used to optimize the hyper-parameters of Darts’ models. In fact, there’s nothing really particular to Darts when it comes to hyperparameter optimization: Optuna and other libraries can be used as they would with other frameworks. The only thing to be aware of are the PyTorch Ligthning integrations, which are available through Darts.\n\nSide conclusion: shall we go with linear regression or TCN to forecast electricty consumption?\nBoth approaches have pros and cons.\nPros of linear regression:\n\nSimplicity\nDoes not require scaling\nSpeed\nDoes not require a GPU\nOften provides good performance out-of-the-box, without requiring tuning\n\nCons of linear regression:\n\nCan require a significant amount of memory (when used as a global model as here), although there are ways around that (e.g., SGD-based).\nIn our setting, it’s impractical to train a stochastic version of the LinearRegression model, as this would incur too large a computational complexity.\n\nTCN, pros:\n\nPotentially more tuneable and powerful\nTypically lower memory requirements thanks to SGD\nVery rich support to capture stochasticity in different ways, without requiring significantly more computation\nVery fast bulk inference over many time series once the model is trained - especially if using a GPU\n\nTCN, cons:\n\nMore hyperparameters, which can take longer to tune and offer more risks of overfitting. It also means that the model is harder to industrialize and maintain\nWill often require a GPU",
    "crumbs": [
      "Blog",
      "Darts - Hyper-parameters Optimization for Electricity Load Forecasting"
    ]
  }
]